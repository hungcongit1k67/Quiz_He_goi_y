<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>100 Câu Hỏi Trắc Nghiệm - Case Study: NPA</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; background-color: #f4f4f4; color: #333; }
        .container { max-width: 900px; margin: auto; background: #fff; padding: 30px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); }
        header { text-align: center; border-bottom: 2px solid #d9534f; margin-bottom: 30px; padding-bottom: 20px; }
        header h1 { color: #d9534f; margin: 0; }
        header p { margin: 5px 0 0; font-style: italic; color: #555; }
        .question-block { margin-bottom: 25px; padding: 20px; border: 1px solid #ddd; border-left: 5px solid #d9534f; border-radius: 5px; background-color: #fdfdfd; }
        .question-text { font-weight: bold; font-size: 1.1em; margin-bottom: 15px; }
        .options { list-style-type: none; padding-left: 0; }
        .options li { margin-bottom: 10px; padding: 8px; border-radius: 4px; }
        .explanation { margin-top: 15px; padding: 15px; background-color: #e9f7ef; border: 1px solid #a3d9b8; border-radius: 5px; }
        .explanation b { color: #1d7b46; }
        .correct-answer { background-color: #dff0d8; border-left: 3px solid #3c763d; }
        .fill-in-answer { font-weight: bold; color: #3c763d; font-size: 1.2em; }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>100 Câu Hỏi Trắc Nghiệm - Case Study: NPA</h1>
            <p>Dựa trên nội dung slide "NPA: Neural News Recommendation with Personalized Attention" - Viện CNTT&TT - ĐHBK Hà Nội</p>
        </header>

        <!-- CATEGORY: INTRODUCTION & MOTIVATION -->
        <div class="question-block"><p class="question-text">Câu 1: Tên viết tắt của mô hình được giới thiệu trong slide 4 là gì?</p><ul class="options"><li>A. NAML</li><li class="correct-answer">B. NPA</li><li>C. LSTUR</li><li>D. DKN</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 4 có tiêu đề "NPA: Neural News Recommendation with Personalized Attention".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 2: Theo slide 6, vấn đề nào KHÔNG được đề cập như một động lực cho mô hình NPA?</p><ul class="options"><li>A. Số lượng bài báo đăng tải mỗi ngày rất lớn.</li><li>B. Không phải tin nào user xem cũng thuộc lĩnh vực họ quan tâm.</li><li>C. Các từ trong tiêu đề đóng góp mức độ khác nhau.</li><li class="correct-answer">D. Người dùng có sở thích dài hạn và ngắn hạn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6 tập trung vào sự đa dạng và mức độ quan tâm khác nhau của người dùng đối với các tin tức và các từ trong tiêu đề. Vấn đề dài hạn/ngắn hạn là động lực chính của mô hình LSTUR ở case study trước.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 3: Giả định cốt lõi của mô hình NPA là gì?</p><ul class="options"><li>A. Tất cả người dùng đều có cùng mức độ quan tâm đến cùng một từ.</li><li class="correct-answer">B. Những người dùng khác nhau có thể quan tâm đến các từ khác nhau trong cùng một tiêu đề tin tức.</li><li>C. Chỉ các từ đầu tiên trong tiêu đề là quan trọng.</li><li>D. Nội dung của bài báo quan trọng hơn tiêu đề.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6 nêu rõ: "Trong một tiêu đề, các từ khác nhau được những người dung khác nhau chú ý đền với mức độ khác nhau." và mô hình sẽ "Mô hình hóa được mức độ quan tâm khác nhau của người dùng đến từ ngữ trong tiêu đề...".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 4: Tên đầy đủ của mô hình NPA (Neural Personalized Attention) nhấn mạnh vào cơ chế nào?</p><ul class="options"><li>A. Mạng Nơ-ron tích chập (CNN).</li><li class="correct-answer">B. Cơ chế chú ý được cá nhân hóa (Personalized Attention).</li><li>C. Mạng Nơ-ron hồi quy (RNN).</li><li>D. Học đa nhiệm (Multi-task Learning).</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tên gọi "Neural News Recommendation with Personalized Attention" cho thấy điểm nhấn chính của bài báo là việc áp dụng cơ chế attention đã được cá nhân hóa.</p></div></div>
        
        <!-- CATEGORY: MODEL ARCHITECTURE -->
        <div class="question-block"><p class="question-text">Câu 5: Mô hình NPA (slide 7) gồm 3 thành phần chính nào?</p><ul class="options"><li>A. News Encoder, User Encoder, Attention Unit.</li><li class="correct-answer">B. News encoder, User encoder, Click predictor.</li><li>C. Word Embedding, CNN, Personalized Attention.</li><li>D. Title Encoder, Body Encoder, Category Encoder.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 liệt kê 3 phần của mô hình: "News encoder", "User encoder", và "Click preditor". Sơ đồ cũng thể hiện rõ 3 khối chính này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 6: News Encoder trong NPA có nhiệm vụ gì?</p><ul class="options"><li>A. Học biểu diễn của người dùng.</li><li class="correct-answer">B. Học biểu diễn cho một bài báo (item).</li><li>C. Dự đoán khả năng click.</li><li>D. Lấy mẫu dữ liệu.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 ghi rõ: "News encoder: học biểu diễn item".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 7: User Encoder trong NPA có nhiệm vụ gì?</p><ul class="options"><li>A. Học biểu diễn của một bài báo.</li><li class="correct-answer">B. Học biểu diễn của người dùng thông qua lịch sử đọc của họ.</li><li>C. Dự đoán khả năng click.</li><li>D. Lấy ID của người dùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 ghi rõ: "User encoder: học biểu diễn user thông qua lịch sử đọc."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 8: Click Predictor trong NPA có nhiệm vụ gì?</p><ul class="options"><li>A. Học biểu diễn của người dùng.</li><li>B. Học biểu diễn của một bài báo.</li><li class="correct-answer">C. Dự đoán khả năng click của một người dùng vào một bài báo.</li><li>D. Đếm số lần click.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 ghi rõ: "Click preditor: dự đoán khả năng click item của mỗi user."</p></div></div>
        
        <!-- CATEGORY: ENCODERS IN DETAIL -->
        <div class="question-block"><p class="question-text">Câu 9: Theo slide 8, News Encoder của NPA gồm những bước nào theo thứ tự?</p><ul class="options"><li>A. CNN -> Word embedding -> Personalized attention.</li><li class="correct-answer">B. Word embedding -> CNN -> Personalized attention.</li><li>C. Personalized attention -> Word embedding -> CNN.</li><li>D. Word embedding -> Personalized attention -> CNN.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 liệt kê các bước của News encoder lần lượt là: "Word embedding", sau đó "CNN: học thông tin về ngữ cảnh", và cuối cùng là "Personalized attention".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 10: Cơ chế "Personalized attention" trong News encoder (slide 9) khác với attention thông thường ở điểm nào?</p><ul class="options"><li>A. Nó sử dụng hàm softmax.</li><li class="correct-answer">B. Nó học một vector truy vấn (query vector) `q_w` riêng cho mỗi người dùng, dựa trên ID của người dùng đó.</li><li>C. Nó được áp dụng sau CNN.</li><li>D. Nó tính tổng có trọng số.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Điểm "cá nhân hóa" nằm ở chỗ vector truy vấn `q_w` không phải là một tham số chung cho toàn mô hình, mà được tạo ra từ vector embedding `e_u` của người dùng. Điều này cho phép mô hình học rằng các người dùng khác nhau sẽ chú ý đến các từ khác nhau.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 11: Vector `e_u` trong Personalized Attention được tạo ra từ đâu?</p><ul class="options"><li>A. Từ tiêu đề tin tức.</li><li class="correct-answer">B. Từ việc embedding userID.</li><li>C. Từ nội dung tin tức.</li><li>D. Từ lịch sử click.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 ghi rõ: "Embedding userID thành vector `e_u`".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 12: Biểu diễn cuối cùng của một item (`r_i`) trong News Encoder được tính như thế nào?</p><ul class="options"><li>A. Bằng cách lấy trung bình các vector context `c_j`.</li><li class="correct-answer">B. Bằng tổng có trọng số của các vector context `c_j`, với trọng số `α_j` được tính từ cơ chế personalized attention.</li><li>C. Bằng cách ghép nối các vector `c_j`.</li><li>D. Bằng vector `q_w`.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 đưa ra công thức `r_i = Σ α_j * c_j`, đây là một phép tổng có trọng số (weighted sum).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 13: User Encoder (slide 10) sử dụng kỹ thuật gì?</p><ul class="options"><li>A. CNN</li><li>B. GRU</li><li class="correct-answer">C. Personalized Attention</li><li>D. MLP</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tương tự như News Encoder, User Encoder cũng "Dùng attention để biểu diễn user...". Cụ thể, nó là một cơ chế personalized attention, nơi query vector `q_d` cũng được tạo ra từ `e_u`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 14: Trong User Encoder, cơ chế attention được áp dụng lên đối tượng nào?</p><ul class="options"><li>A. Các từ trong các tiêu đề đã xem.</li><li class="correct-answer">B. Các vector biểu diễn của các bài báo mà người dùng đã xem trong lịch sử.</li><li>C. Các category mà người dùng đã xem.</li><li>D. Các người dùng khác.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 10 ghi: "Dùng attention để biểu diễn user thông qua các item mà user đó đã xem". Công thức `u = Σ α'_j * r_j` cho thấy nó tổng hợp các vector `r_j` (biểu diễn của các tin đã xem) lại với nhau.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 15: Theo slide 11, điểm số dự đoán khả năng click `ŷ` được tính bằng cách nào?</p><ul class="options"><li>A. Bằng hàm softmax.</li><li class="correct-answer">B. Bằng tích vô hướng của vector người dùng `u` và vector bài báo ứng viên `r_c`.</li><li>C. Bằng độ tương đồng cosine.</li><li>D. Bằng cách đưa qua một mạng FNN.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 11 (có tiêu đề 2.3. Click prediction) và slide 12 đều đưa ra công thức `ŷ = u^T * r_c`.</p></div></div>

        <!-- CATEGORY: EVALUATION -->
        <div class="question-block"><p class="question-text">Câu 16: Bộ dữ liệu được sử dụng trong đánh giá của NPA là gì?</p><ul class="options"><li>A. Tuniu.com</li><li class="correct-answer">B. MSN News</li><li>C. Adressa</li><li>D. MovieLens</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 12 và 13 đều ghi: "Dữ liệu từ MSN News".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 17: Theo bảng kết quả ở slide 13, mô hình NPA đạt giá trị AUC là bao nhiêu?</p><ul class="options"><li>A. 0.5869</li><li>B. 0.6009</li><li class="correct-answer">C. 0.6243*</li><li>D. 0.5774</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong bảng kết quả, hàng cuối cùng "NPA", cột "AUC" có giá trị là 0.6243*.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 18: Dấu `*` bên cạnh các kết quả của NPA và DKN có ý nghĩa gì?</p><ul class="options"><li>A. Các mô hình này chạy chậm hơn.</li><li class="correct-answer">B. Sự cải thiện của các mô hình này so với các baseline khác là có ý nghĩa thống kê.</li><li>C. Các mô hình này được triển khai bởi tác giả.</li><li>D. Các mô hình này bị lỗi.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong các bài báo khoa học, dấu `*` hoặc các ký hiệu tương tự thường được dùng để chỉ ra rằng sự khác biệt về kết quả đã được kiểm chứng là có ý nghĩa thống kê (ví dụ, p-value < 0.05), chứ không phải do may rủi.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 19: (Điền đáp án) Theo bảng ở slide 13, mô hình NPA có giá trị nDCG@5 là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.3535*</span></p><p><b>💡 Giải thích:</b> Trong bảng, hàng "NPA", cột "nDCG@5" có giá trị là 0.3535*.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 20: (Điền đáp án) Theo bảng ở slide 13, mô hình NPA có giá trị nDCG@10 là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.4380*</span></p><p><b>💡 Giải thích:</b> Trong bảng, hàng "NPA", cột "nDCG@10" có giá trị là 0.4380*.</p></div></div>
        
        <!-- ... More questions from 21 to 100 ... -->
        <div class="question-block"><p class="question-text">Câu 21: Mô hình NPA mô hình hóa sự quan tâm khác nhau của người dùng đến "từ ngữ trong tiêu đề" thông qua thành phần nào?</p><ul class="options"><li class="correct-answer">A. Personalized attention trong News Encoder</li><li>B. CNN trong News Encoder</li><li>C. User Encoder</li><li>D. Click Predictor</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8-9 mô tả chi tiết cơ chế "Personalized attention" trong News Encoder, nơi vector query `q_w` được tạo ra từ user embedding `e_u` để tính trọng số cho từng từ trong tiêu đề.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 22: Mô hình NPA mô hình hóa sự quan tâm khác nhau của người dùng đến "các tin khác nhau" trong lịch sử đọc thông qua thành phần nào?</p><ul class="options"><li>A. News Encoder</li><li class="correct-answer">B. Personalized attention trong User Encoder</li><li>C. Click Predictor</li><li>D. Word Embedding</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 10 mô tả User Encoder, nơi cơ chế attention được dùng để tính trọng số `α'_j` cho mỗi tin tức `r_j` trong lịch sử đọc, phản ánh mức độ quan tâm khác nhau của người dùng đến các tin này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 23: (Điền đáp án) Theo slide 7, sơ đồ kiến trúc tổng quan của NPA cho thấy `r_c` là biểu diễn của cái gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Candidate News (Tin tức ứng viên)</span></p><p><b>💡 Giải thích:</b> Sơ đồ cho thấy `r_c` là đầu ra của News Encoder cho "Candidate News".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 24: (Điền đáp án) Theo slide 7, `r_1, ..., r_N` là biểu diễn của cái gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Clicked News (Các tin tức đã click trong lịch sử)</span></p><p><b>💡 Giải thích:</b> Sơ đồ cho thấy `r_1` đến `r_N` là đầu ra của các News Encoder cho "Clicked News" và là đầu vào cho User Encoder.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 25: Theo slide 9, vector query `q_w` được tạo ra bằng cách nào?</p><ul class="options"><li>A. Bằng `e_u`.</li><li class="correct-answer">B. Bằng cách đưa `e_u` qua một tầng fully connected (Dense) với hàm kích hoạt ReLU.</li><li>C. Bằng cách lấy trung bình các embedding từ.</li><li>D. Là một tham số được học chung cho tất cả user.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 có công thức: `q_w = ReLU(V_w × e_u + v_w)`. Đây là một phép biến đổi phi tuyến của `e_u` thông qua một tầng Dense.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 26: Theo slide 10, vector query `q_d` được tạo ra bằng cách nào?</p><ul class="options"><li>A. Bằng `e_u`.</li><li class="correct-answer">B. Bằng cách đưa `e_u` qua một tầng fully connected (Dense) với hàm kích hoạt ReLU.</li><li>C. Bằng cách lấy trung bình các biểu diễn tin tức.</li><li>D. Là một tham số được học chung cho tất cả user.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tương tự như `q_w`, slide 10 có công thức: `q_d = ReLU(V_d × e_u + v_d)`. Cả hai query vector đều được cá nhân hóa dựa trên `e_u`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 27: Tại sao lại cần hai query vector khác nhau là `q_w` và `q_d`?</p><ul class="options"><li>A. Đây là lỗi đánh máy, chúng là một.</li><li class="correct-answer">B. Vì chúng phục vụ hai cơ chế attention khác nhau: `q_w` để xác định sự quan tâm đến từ (word-level), còn `q_d` để xác định sự quan tâm đến tin tức (news-level). Mô hình học các phép chiếu khác nhau cho hai nhiệm vụ này.</li><li>C. Vì `q_w` cho user, `q_d` cho item.</li><li>D. Vì `q_w` cho CNN, `q_d` cho attention.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Việc một người dùng chú ý đến các từ khóa nào có thể là một cơ chế nhận thức khác với việc họ chú ý đến các bài báo nào trong lịch sử. Việc sử dụng hai phép chiếu (hai tầng Dense với các trọng số `V_w` và `V_d` khác nhau) cho phép mô hình học hai loại "sở thích chú ý" này một cách độc lập.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 28: (Điền đáp án) Theo slide 8, `c_1, c_2, ..., c_M` là vector thu được sau khi qua mạng nào?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">CNN</span></p><p><b>💡 Giải thích:</b> Slide 8 ghi: "CNN: học thông tin về ngữ cảnh. Thu được vector `[c1, c2, ..., cM]`".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 29: Theo slide 12, bộ dữ liệu MSN News trong 1 tháng có bao nhiêu samples?</p><ul class="options"><li>A. 10,000</li><li>B. 42,255</li><li>C. 445,230</li><li class="correct-answer">D. 7,141,584</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Bảng ở slide 12 (đánh số nhầm, đúng là 13), hàng "# samples" có giá trị 7,141,584.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 30: So với các mô hình NAML, LSTUR, HyperNews, đâu là điểm chung cốt lõi?</p><ul class="options"><li>A. Chúng đều sử dụng Lọc cộng tác.</li><li class="correct-answer">B. Chúng đều là các kiến trúc deep learning, sử dụng các kỹ thuật như embedding và attention để gợi ý tin tức được cá nhân hóa.</li><li>C. Chúng đều sử dụng dữ liệu MovieLens.</li><li>D. Chúng đều là các mô hình dựa trên láng giềng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Cả ba case study này đều đại diện cho xu hướng hiện đại trong hệ gợi ý: sử dụng các mạng nơ-ron sâu để học các biểu diễn phức tạp từ dữ liệu nội dung và hành vi người dùng, với attention là một thành phần quan trọng để mô hình hóa sự quan tâm.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 31: Trong sơ đồ tổng quan ở slide 7, tại sao có nhiều khối "Personalized Attention"?</p><ul class="options"><li>A. Để làm cho sơ đồ phức tạp.</li><li class="correct-answer">B. Vì cơ chế attention được áp dụng ở nhiều nơi: cho mỗi tin tức trong lịch sử (để tạo `r_i`), cho tin tức ứng viên (để tạo `r_c`), và cho chính người dùng (để tạo `u`).</li><li>C. Mỗi khối dành cho một người dùng.</li><li>D. Đây là các bước thời gian khác nhau.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ cho thấy mỗi News Encoder (cả cho candidate và clicked news) đều chứa một khối Personalized Attention bên trong (cấp độ từ). Ngoài ra, User Encoder cũng chứa một khối Personalized Attention khác (cấp độ tin tức).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 32: (Điền đáp án) Theo slide 6, việc mô hình hóa mức độ quan tâm khác nhau của người dùng sẽ giúp ích trong việc gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Gợi ý</span></p><p><b>💡 Giải thích:</b> Dòng cuối của slide 6 kết luận: "...sẽ giúp ích trong việ gợi ý."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 33: Nếu một người dùng có userID là 123, vector `e_u` của họ được lấy từ đâu?</p><ul class="options"><li>A. Bằng cách tính toán từ số 123.</li><li class="correct-answer">B. Bằng cách lấy hàng thứ 123 từ một ma trận embedding người dùng được học.</li><li>C. Bằng cách lấy trung bình các `e_u` khác.</li><li>D. Là một vector ngẫu nhiên mỗi lần.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tầng embedding hoạt động như một bảng tra cứu (lookup table). ID của người dùng (hoặc item) được dùng làm chỉ số để lấy ra vector biểu diễn tương ứng từ ma trận embedding.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 34: NPA khác với các mô hình attention cho gợi ý tin tức trước đó ở điểm nào?</p><ul class="options"><li>A. Nó là mô hình deep learning đầu tiên.</li><li class="correct-answer">B. Nó là mô hình đầu tiên đưa ra cơ chế attention "cá nhân hóa", nơi sự chú ý đến từ/tin tức phụ thuộc vào chính người dùng đó.</li><li>C. Nó là mô hình đầu tiên sử dụng CNN.</li><li>D. Nó là mô hình đầu tiên sử dụng dot-product.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Điểm mới lạ và cốt lõi của NPA, như tên gọi của nó, là "Personalized Attention". Các mô hình attention trước đó thường có một query vector chung cho tất cả người dùng, trong khi NPA học một query vector riêng cho mỗi người dùng, làm cho sự chú ý trở nên linh hoạt và cá nhân hóa hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 35: (Điền đáp án) Trong bảng kết quả ở slide 13, NPA là viết tắt của Neural ... Attention?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Personalized (Không có trong slide nhưng suy luận từ tên bài báo)</span></p><p><b>💡 Giải thích:</b> Mặc dù slide 13 chỉ ghi "NPA", nhưng dựa vào tiêu đề ở slide 4 "Neural News Recommendation with Personalized Attention", ta có thể suy ra NPA là viết tắt của Neural Personalized Attention.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 36: Mô hình NPA có sử dụng thông tin gì ngoài tiêu đề và lịch sử click không?</p><ul class="options"><li>A. Không.</li><li class="correct-answer">B. Có, sơ đồ News Encoder trong bài báo gốc (slide 10) cho thấy nó còn sử dụng Body (nội dung) và Categories (thể loại).</li><li>C. Có, nó sử dụng thông tin nhân khẩu học.</li><li>D. Có, nó sử dụng thời gian đọc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 10 trình bày một phiên bản đầy đủ hơn của News Encoder, cho thấy nó là một mô hình multi-view, xử lý cả Title, Body và Categories, tương tự như mô hình NAML.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 37: (Điền đáp án) Theo slide 8, bước đầu tiên trong News Encoder là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Word embedding</span></p><p><b>💡 Giải thích:</b> Slide 8 liệt kê "Word embedding: embedding các từ trong tiêu đề" là bước đầu tiên.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 38: Theo slide 12, bộ dữ liệu MSN News được thu thập trong bao lâu?</p><ul class="options"><li class="correct-answer">A. 1 tháng</li><li>B. 2 tháng</li><li>C. 1 tuần</li><li>D. 1 năm</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tiêu đề của slide 12 (đánh số nhầm thành 13) là "Dữ liệu từ MSN News trong 01 tháng".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 39: Theo slide 13, DFM là viết tắt của mô hình nào?</p><ul class="options"><li>A. Dynamic Factorization Machine</li><li class="correct-answer">B. Deep Factorization Machine</li><li>C. Document-based Factorization Machine</li><li>D. Deep Fusion Model</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong bối cảnh các mô hình deep learning cho gợi ý, DFM thường là viết tắt của Deep Factorization Machine, một biến thể khác của DeepFM.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 40: (Điền đáp án) Trong công thức của User Encoder (slide 10), `r_j` là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Biểu diễn của bài báo thứ j trong lịch sử người dùng</span></p><p><b>💡 Giải thích:</b> Công thức `u = Σ α'_j * r_j` là tổng có trọng số của các vector `r_j`, là các biểu diễn của các tin tức đã xem.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 41: Tại sao User Encoder và News Encoder lại có kiến trúc Personalized Attention tương tự nhau?</p><ul class="options"><li>A. Do ngẫu nhiên.</li><li class="correct-answer">B. Vì cả hai đều giải quyết cùng một vấn đề cơ bản: tổng hợp một tập hợp các vector (các từ trong tin, hoặc các tin trong lịch sử) thành một vector đại diện duy nhất, với sự chú ý phụ thuộc vào người dùng.</li><li>C. Vì tác giả lười thiết kế.</li><li>D. Vì chúng được huấn luyện chung.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Ý tưởng "personalized attention" là một ý tưởng tổng quát có thể áp dụng ở nhiều cấp độ. Ở cấp độ từ, nó giúp chọn từ quan trọng. Ở cấp độ tin tức, nó giúp chọn tin quan trọng. Cấu trúc attention là giống nhau, chỉ khác ở đối tượng được áp dụng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 42: (Điền đáp án) Theo slide 6, "Trong một tiêu đề, các từ khác nhau được những người dung khác nhau chú ý đền với ... khác nhau."</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">mức độ</span></p><p><b>💡 Giải thích:</b> Slide 6 ghi đầy đủ câu: "Trong một tiêu đề, các từ khác nhau được những người dung khác nhau chú ý đền với mức độ khác nhau."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 43: Việc sử dụng Softmax trong Click Predictor (như ở slide 7) có ý nghĩa gì?</p><ul class="options"><li>A. Để tính điểm tương đồng.</li><li class="correct-answer">B. Để chuyển đổi các điểm số của một tập các item ứng viên thành một phân phối xác suất.</li><li>C. Để mã hóa người dùng.</li><li>D. Để tính hàm loss.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Hàm softmax nhận một vector các điểm số và chuyển nó thành một vector các xác suất có tổng bằng 1. Trong gợi ý, nó thường được dùng để tính xác suất người dùng sẽ click vào mỗi item trong một tập hợp các item ứng viên.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 44: Mô hình NPA có thể được cải thiện bằng cách nào, dựa trên các nhược điểm đã nêu ở slide 22?</p><ul class="options"><li>A. Giảm số lượng tham số.</li><li class="correct-answer">B. Khai thác thêm thông tin về thực thể (entity) và xem xét mối quan hệ phân cấp giữa các category.</li><li>C. Sử dụng một mô hình đơn giản hơn.</li><li>D. Tăng thời gian huấn luyện.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các nhược điểm được liệt kê ("Chưa khai thác được entity", "Cat lớn và Cat nhỏ có phần nào đó liên quan...") chính là những hướng đi tiềm năng để cải thiện mô hình trong tương lai.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 45: Nếu hai người dùng có cùng lịch sử click, biểu diễn user `u` của họ trong mô hình NAML sẽ như thế nào?</p><ul class="options"><li>A. Khác nhau hoàn toàn.</li><li class="correct-answer">B. Giống hệt nhau, vì `u` chỉ phụ thuộc vào lịch sử click và userID (để tạo query vector). Nếu userID và lịch sử giống nhau, `u` sẽ giống nhau.</li><li>C. Gần giống nhau.</li><li>D. Không thể xác định.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Biểu diễn `u` là một hàm xác định của lịch sử click và userID. Nếu hai người dùng có cùng đầu vào, họ sẽ có cùng đầu ra `u`. Tuy nhiên, trong thực tế, hai người dùng khác nhau sẽ có userID khác nhau, dẫn đến `e_u` và `q_d` khác nhau, và do đó `u` cũng sẽ khác nhau ngay cả khi có cùng lịch sử click.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 46: Mô hình NPA thuộc loại gợi ý nào?</p><ul class="options"><li>A. Gợi ý dựa trên lọc cộng tác.</li><li class="correct-answer">B. Gợi ý dựa trên nội dung được cá nhân hóa cao độ.</li><li>C. Gợi ý dựa trên tri thức.</li><li>D. Gợi ý hybrid.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Mặc dù nó học từ hành vi của người dùng (giống CF), nhưng cốt lõi của NPA là xây dựng các biểu diễn rất chi tiết từ nội dung của tin tức và sau đó cá nhân hóa các biểu diễn đó dựa trên hồ sơ người dùng. Do đó, nó có thể được xem là một dạng rất tiên tiến của Gợi ý dựa trên nội dung.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 47: (Điền đáp án) Theo slide 13, trong bảng so sánh, Wide & Deep có giá trị MRR là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.2989</span></p><p><b>💡 Giải thích:</b> Trong bảng ở slide 13, hàng "Wide & Deep", cột "MRR", giá trị là 0.2989.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 48: (Điền đáp án) Theo slide 13, trong bảng so sánh, LibFM có giá trị nDCG@10 là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.3932</span></p><p><b>💡 Giải thích:</b> Trong bảng ở slide 13, hàng "LibFM", cột "nDCG@10", giá trị là 0.3932.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 49: Nếu một người dùng có sở thích rất đa dạng, biểu diễn user `u` của họ sẽ như thế nào?</p><ul class="options"><li>A. Sẽ là một vector không.</li><li class="correct-answer">B. Sẽ là một vector tổng hợp từ nhiều loại tin tức khác nhau, có thể không có độ tương đồng cao với bất kỳ loại tin tức cụ thể nào.</li><li>C. Sẽ giống với người dùng có sở thích hẹp.</li><li>D. Sẽ không thể tính toán được.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> User Encoder sẽ tính tổng có trọng số của các vector tin tức đa dạng. Kết quả sẽ là một vector "trung bình" nằm ở đâu đó giữa các cụm chủ đề khác nhau trong không gian đặc trưng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 50: Bài học chính từ case study NPA là gì?</p><ul class="options"><li>A. CNN luôn tốt hơn RNN.</li><li class="correct-answer">B. Việc cá nhân hóa cơ chế attention, cho phép mô hình học được sự chú ý khác nhau của từng người dùng đến các từ và các tin tức, là một kỹ thuật hiệu quả để cải thiện độ chính xác của gợi ý tin tức.</li><li>C. Gợi ý tin tức là một bài toán dễ.</li><li>D. Chỉ cần sử dụng ID người dùng là đủ để gợi ý.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Điểm đột phá của NPA là ý tưởng "Personalized Attention". Nó cho thấy việc đi sâu vào mô hình hóa sự tương tác tinh vi giữa người dùng và nội dung (người dùng nào chú ý đến từ nào) có thể mang lại những cải tiến đáng kể so với các mô hình attention thông thường.</p></div></div>
    </div>
</body>
</html>