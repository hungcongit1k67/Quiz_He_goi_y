<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>100 CÃ¢u Há»i Tráº¯c Nghiá»‡m - Case Study: Long- and Short-term News Recommendation</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; background-color: #f4f4f4; color: #333; }
        .container { max-width: 900px; margin: auto; background: #fff; padding: 30px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); }
        header { text-align: center; border-bottom: 2px solid #d9534f; margin-bottom: 30px; padding-bottom: 20px; }
        header h1 { color: #d9534f; margin: 0; }
        header p { margin: 5px 0 0; font-style: italic; color: #555; }
        .question-block { margin-bottom: 25px; padding: 20px; border: 1px solid #ddd; border-left: 5px solid #d9534f; border-radius: 5px; background-color: #fdfdfd; }
        .question-text { font-weight: bold; font-size: 1.1em; margin-bottom: 15px; }
        .options { list-style-type: none; padding-left: 0; }
        .options li { margin-bottom: 10px; padding: 8px; border-radius: 4px; }
        .explanation { margin-top: 15px; padding: 15px; background-color: #e9f7ef; border: 1px solid #a3d9b8; border-radius: 5px; }
        .explanation b { color: #1d7b46; }
        .correct-answer { background-color: #dff0d8; border-left: 3px solid #3c763d; }
        .fill-in-answer { font-weight: bold; color: #3c763d; font-size: 1.2em; }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>100 CÃ¢u Há»i Tráº¯c Nghiá»‡m - Case Study: Long- and Short-term News Recommendation</h1>
            <p>Dá»±a trÃªn ná»™i dung slide "PhÃ¢n tÃ­ch má»™t sá»‘ há»‡ gá»£i Ã½ cá»¥ thá»ƒ" - Viá»‡n CNTT&TT - ÄHBK HÃ  Ná»™i</p>
        </header>

        <!-- CATEGORY: INTRODUCTION & MOTIVATION -->
        <div class="question-block"><p class="question-text">CÃ¢u 1: Váº¥n Ä‘á» chÃ­nh mÃ  bÃ i bÃ¡o "Neural News Recommendation with Long- and Short-term User Representations" giáº£i quyáº¿t lÃ  gÃ¬?</p><ul class="options"><li class="correct-answer">A. NgÆ°á»i dÃ¹ng cÃ³ cáº£ sá»Ÿ thÃ­ch dÃ i háº¡n (á»•n Ä‘á»‹nh) vÃ  ngáº¯n háº¡n (thay Ä‘á»•i theo thá»i gian), vÃ  cáº§n khai thÃ¡c cáº£ hai.</li><li>B. Gá»£i Ã½ tin tá»©c lÃ  má»™t bÃ i toÃ¡n khÃ³ vÃ¬ cÃ³ quÃ¡ nhiá»u tin má»›i má»—i ngÃ y.</li><li>C. CÃ¡c mÃ´ hÃ¬nh hiá»‡n táº¡i khÃ´ng thá»ƒ xá»­ lÃ½ Ä‘Æ°á»£c vÄƒn báº£n.</li><li>D. Cáº§n dá»± Ä‘oÃ¡n thá»i gian ngÆ°á»i dÃ¹ng Ä‘á»c tin tá»©c.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 5 nÃªu rÃµ: "User cÃ³ sá»Ÿ thÃ­ch dÃ i háº¡n vÃ  ngáº¯n háº¡n. => Cáº§n pháº£i khai thÃ¡c cáº£ 2." vÃ  "paper tiáº¿p cáº­n theo hÆ°á»›ng biá»ƒu diá»…n user theo long-term vÃ  short-term".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 2: Sá»Ÿ thÃ­ch ngáº¯n háº¡n (short-term) cá»§a ngÆ°á»i dÃ¹ng thá»ƒ hiá»‡n Ä‘iá»u gÃ¬?</p><ul class="options"><li>A. CÃ¡c chá»§ Ä‘á» ngÆ°á»i dÃ¹ng luÃ´n luÃ´n quan tÃ¢m.</li><li class="correct-answer">B. Sá»± quan tÃ¢m táº¡m thá»i, thÆ°á»ng liÃªn quan Ä‘áº¿n cÃ¡c tin tá»©c vá»«a Ä‘á»c gáº§n Ä‘Ã¢y.</li><li>C. ThÃ´ng tin nhÃ¢n kháº©u há»c cá»§a ngÆ°á»i dÃ¹ng.</li><li>D. Sá»Ÿ thÃ­ch cá»§a báº¡n bÃ¨ ngÆ°á»i dÃ¹ng.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Sá»Ÿ thÃ­ch ngáº¯n háº¡n pháº£n Ã¡nh sá»± quan tÃ¢m tá»©c thá»i. VÃ­ dá»¥, sau khi Ä‘á»c má»™t tin vá» "Rami Malek Wins the 2019 Oscar" (slide 5), ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ quan tÃ¢m Ä‘áº¿n cÃ¡c tin tá»©c liÃªn quan Ä‘áº¿n giáº£i Oscar ngay sau Ä‘Ã³.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 3: Sá»Ÿ thÃ­ch dÃ i háº¡n (long-term) cá»§a ngÆ°á»i dÃ¹ng thá»ƒ hiá»‡n Ä‘iá»u gÃ¬?</p><ul class="options"><li class="correct-answer">A. CÃ¡c chá»§ Ä‘á» cá»‘t lÃµi, á»•n Ä‘á»‹nh mÃ  ngÆ°á»i dÃ¹ng quan tÃ¢m, vÃ­ dá»¥ nhÆ° má»™t ngÆ°á»i luÃ´n thÃ­ch Ä‘á»c tin vá» Ä‘á»™i bÃ³ng rá»• "Golden State Warriors".</li><li>B. Sá»± quan tÃ¢m Ä‘áº¿n má»™t tin tá»©c vá»«a má»›i xáº£y ra.</li><li>C. Thá»i gian trong ngÃ y mÃ  ngÆ°á»i dÃ¹ng hay Ä‘á»c tin.</li><li>D. Loáº¡i thiáº¿t bá»‹ ngÆ°á»i dÃ¹ng sá»­ dá»¥ng.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Sá»Ÿ thÃ­ch dÃ i háº¡n lÃ  nhá»¯ng má»‘i quan tÃ¢m tÆ°Æ¡ng Ä‘á»‘i khÃ´ng thay Ä‘á»•i cá»§a ngÆ°á»i dÃ¹ng qua thá»i gian, vÃ­ dá»¥ nhÆ° hÃ¢m má»™ má»™t Ä‘á»™i thá»ƒ thao, quan tÃ¢m Ä‘áº¿n má»™t lÄ©nh vá»±c chÃ­nh trá»‹, hoáº·c theo dÃµi má»™t máº£ng cÃ´ng nghá»‡ cá»¥ thá»ƒ.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 4: MÃ´ hÃ¬nh Ä‘Æ°á»£c Ä‘á» xuáº¥t trong bÃ i bÃ¡o bao gá»“m hai thÃ nh pháº§n chÃ­nh nÃ o?</p><ul class="options"><li>A. Title encoder vÃ  Topic encoder.</li><li class="correct-answer">B. News encoder vÃ  User encoder.</li><li>C. Long-term encoder vÃ  Short-term encoder.</li><li>D. CNN vÃ  GRU.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 6 liá»‡t kÃª 2 thÃ nh pháº§n chÃ­nh cá»§a Model lÃ  "1. News encoder" vÃ  "2. User encoder".</p></div></div>
        
        <!-- CATEGORY: NEWS ENCODER -->
        <div class="question-block"><p class="question-text">CÃ¢u 5: News encoder Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ há»c cÃ¡c thÃ´ng tin gÃ¬ tá»« má»™t bÃ i bÃ¡o?</p><ul class="options"><li>A. Chá»‰ Title.</li><li>B. Chá»‰ Topic.</li><li class="correct-answer">C. Title, topic vÃ  subtopic categories.</li><li>D. ToÃ n bá»™ ná»™i dung vÄƒn báº£n.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 7 nÃªu rÃµ News encoder "ÄÆ°á»£c dÃ¹ng Ä‘á»ƒ há»c title, topic, subtopic categories."</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 6: News encoder Ä‘Æ°á»£c chia lÃ m 2 pháº§n nÃ o?</p><ul class="options"><li>A. CNN vÃ  Attention.</li><li>B. Long-term vÃ  Short-term.</li><li class="correct-answer">C. Title encoder vÃ  Topic encoder.</li><li>D. Word embedding vÃ  Context embedding.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 7 ghi rÃµ News encoder "Chia lÃ m 2 pháº§n: â—‹ Title encoder â—‹ Topic encoder".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 7: Trong Title encoder, sau khi embedding cÃ¡c tá»«, mÃ´ hÃ¬nh sá»­ dá»¥ng máº¡ng nÃ o Ä‘á»ƒ trÃ­ch xuáº¥t context?</p><ul class="options"><li>A. GRU</li><li class="correct-answer">B. CNN</li><li>C. FNN (Feed-forward Neural Network)</li><li>D. Autoencoder</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 8 mÃ´ táº£: "Sau khi embedding, Ä‘Æ°a qua máº¡ng CNN Ä‘á»ƒ trÃ­ch xuáº¥t context".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 8: Trong Title encoder, sau khi cÃ³ cÃ¡c vector context `c_i` tá»« CNN, ká»¹ thuáº­t nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ láº¥y tá»•ng cÃ³ trá»ng sá»‘ vÃ  táº¡o ra vector Ä‘áº¡i diá»‡n cho title `e_t`?</p><ul class="options"><li>A. Max-pooling</li><li>B. Average-pooling</li><li class="correct-answer">C. Attention</li><li>D. GRU</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 8 trÃ¬nh bÃ y cÃ¡c cÃ´ng thá»©c tÃ­nh trá»ng sá»‘ attention `Î±_i` vÃ  sau Ä‘Ã³ tÃ­nh tá»•ng cÃ³ trá»ng sá»‘ `e_t = Î£ Î±_i * c_i`. Äiá»u nÃ y cho tháº¥y cÆ¡ cháº¿ attention Ä‘Æ°á»£c sá»­ dá»¥ng.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 9: Topic encoder xá»­ lÃ½ nhá»¯ng thÃ´ng tin gÃ¬?</p><ul class="options"><li>A. Chá»‰ title.</li><li class="correct-answer">B. Topic vÃ  subtopic categories.</li><li>C. Chá»‰ ná»™i dung.</li><li>D. Chá»‰ cÃ¡c tá»« khÃ³a.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 9 nÃªu rÃµ Topic encoder xá»­ lÃ½ "topic vÃ  subtopic categories".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 10: Vector biá»ƒu diá»…n cuá»‘i cÃ¹ng cá»§a má»™t bÃ i bÃ¡o (`e` trong sÆ¡ Ä‘á»“ slide 10) Ä‘Æ°á»£c táº¡o ra nhÆ° tháº¿ nÃ o?</p><ul class="options"><li>A. Chá»‰ báº±ng vector cá»§a title (`e_t`).</li><li>B. Chá»‰ báº±ng vector cá»§a topic (`e_v`).</li><li class="correct-answer">C. Báº±ng cÃ¡ch ghÃ©p ná»‘i (concatenate) vector cá»§a title (`e_t`) vÃ  vector cá»§a topic (`e_v`).</li><li>D. Báº±ng cÃ¡ch láº¥y trung bÃ¬nh cá»§a `e_t` vÃ  `e_v`.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> SÆ¡ Ä‘á»“ trÃªn slide 10 cho tháº¥y `e_t` vÃ  `e_v` Ä‘Æ°á»£c Ä‘Æ°a vÃ o má»™t khá»‘i cÃ³ kÃ½ hiá»‡u `âŠ•` (thÆ°á»ng cÃ³ nghÄ©a lÃ  concat hoáº·c cá»™ng) Ä‘á»ƒ táº¡o ra vector cuá»‘i cÃ¹ng `e`.</p></div></div>
        
        <!-- CATEGORY: USER ENCODER -->
        <div class="question-block"><p class="question-text">CÃ¢u 11: User encoder Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ náº¯m báº¯t 2 loáº¡i sá»Ÿ thÃ­ch nÃ o?</p><ul class="options"><li>A. TÃ­ch cá»±c vÃ  tiÃªu cá»±c.</li><li>B. XÃ£ há»™i vÃ  cÃ¡ nhÃ¢n.</li><li class="correct-answer">C. Ngáº¯n háº¡n (Short-term) vÃ  DÃ i háº¡n (Long-term).</li><li>D. RÃµ rÃ ng vÃ  tiá»m áº©n.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 11 liá»‡t kÃª 2 thÃ nh pháº§n cá»§a User encoder lÃ  "â€¢ Short-term" vÃ  "â€¢ Long-term".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 12: MÃ´ hÃ¬nh nÃ o Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ biá»ƒu diá»…n sá»Ÿ thÃ­ch ngáº¯n háº¡n (short-term) cá»§a ngÆ°á»i dÃ¹ng?</p><ul class="options"><li>A. CNN</li><li class="correct-answer">B. GRU (Gated Recurrent Unit)</li><li>C. FNN</li><li>D. Attention</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 12 ghi rÃµ: "Sá»­ dá»¥ng GRU Ä‘á»ƒ biá»ƒu diá»…n short term". GRU lÃ  má»™t loáº¡i máº¡ng nÆ¡-ron há»“i quy, ráº¥t phÃ¹ há»£p Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»± nhÆ° lá»‹ch sá»­ click cá»§a ngÆ°á»i dÃ¹ng.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 13: Äáº§u vÃ o cá»§a GRU á»Ÿ má»—i bÆ°á»›c thá»i gian `t` lÃ  gÃ¬?</p><ul class="options"><li>A. ID cá»§a bÃ i bÃ¡o thá»© `t`.</li><li class="correct-answer">B. Vector biá»ƒu diá»…n `e_t` cá»§a bÃ i bÃ¡o thá»© `t` (tá»« News Encoder) vÃ  tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³ `h_{t-1}`.</li><li>C. Chá»‰ tráº¡ng thÃ¡i áº©n `h_{t-1}`.</li><li>D. Chá»‰ vector `e_t`.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> CÃ¡c cÃ´ng thá»©c trÃªn slide 12 (`r_t = Ïƒ(W_r[h_{t-1}, e_t])`,...) cho tháº¥y GRU nháº­n cáº£ tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³ vÃ  Ä‘áº§u vÃ o hiá»‡n táº¡i (`e_t`) Ä‘á»ƒ tÃ­nh toÃ¡n tráº¡ng thÃ¡i áº©n má»›i.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 14: Biá»ƒu diá»…n sá»Ÿ thÃ­ch dÃ i háº¡n (long-term) Ä‘Æ°á»£c há»c nhÆ° tháº¿ nÃ o?</p><ul class="options"><li>A. ÄÆ°á»£c tÃ­nh toÃ¡n tá»« cÃ¡c thuá»™c tÃ­nh nhÃ¢n kháº©u há»c.</li><li class="correct-answer">B. ÄÆ°á»£c biá»ƒu diá»…n báº±ng má»™t vector embedding riÃªng cho má»—i user, Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn vÃ  há»c trong quÃ¡ trÃ¬nh train.</li><li>C. Báº±ng cÃ¡ch láº¥y trung bÃ¬nh táº¥t cáº£ cÃ¡c bÃ i bÃ¡o ngÆ°á»i dÃ¹ng Ä‘Ã£ Ä‘á»c.</li><li>D. NÃ³ lÃ  má»™t giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 13 mÃ´ táº£ biá»ƒu diá»…n Long-term: "ÄÆ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn" vÃ  "ÄÆ°á»£c há»c trong quÃ¡ trÃ¬nh train." Äiá»u nÃ y cÃ³ nghÄ©a lÃ  má»—i user cÃ³ má»™t vector `u` riÃªng, vÃ  vector nÃ y Ä‘Æ°á»£c cáº­p nháº­t cÃ¹ng vá»›i cÃ¡c tham sá»‘ khÃ¡c cá»§a mÃ´ hÃ¬nh.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 15: MÃ´ hÃ¬nh "LSTUR-ini" (slide 14a) káº¿t há»£p sá»Ÿ thÃ­ch dÃ i háº¡n vÃ  ngáº¯n háº¡n nhÆ° tháº¿ nÃ o?</p><ul class="options"><li>A. Báº±ng cÃ¡ch cá»™ng chÃºng láº¡i vá»›i nhau.</li><li class="correct-answer">B. Biá»ƒu diá»…n dÃ i háº¡n `u` Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ khá»Ÿi táº¡o tráº¡ng thÃ¡i áº©n ban Ä‘áº§u cá»§a GRU.</li><li>C. Biá»ƒu diá»…n dÃ i háº¡n Ä‘Æ°á»£c ghÃ©p ná»‘i vá»›i Ä‘áº§u ra cá»§a GRU.</li><li>D. ChÃºng khÃ´ng Ä‘Æ°á»£c káº¿t há»£p.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> SÆ¡ Ä‘á»“ (a) trÃªn slide 14 cho tháº¥y vector `u_i` tá»« "User Embedding" (biá»ƒu diá»…n dÃ i háº¡n) Ä‘Æ°á»£c Ä‘Æ°a vÃ o vá»‹ trÃ­ Ä‘áº§u tiÃªn cá»§a chuá»—i GRU, Ä‘Ã³ng vai trÃ² lÃ  tráº¡ng thÃ¡i áº©n ban Ä‘áº§u. TÃªn gá»i "-ini" (initialization) cÅ©ng gá»£i Ã½ Ä‘iá»u nÃ y.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 16: MÃ´ hÃ¬nh "LSTUR-con" (slide 14b) káº¿t há»£p sá»Ÿ thÃ­ch dÃ i háº¡n vÃ  ngáº¯n háº¡n nhÆ° tháº¿ nÃ o?</p><ul class="options"><li>A. Báº±ng cÃ¡ch cá»™ng chÃºng láº¡i vá»›i nhau.</li><li>B. Biá»ƒu diá»…n dÃ i háº¡n Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ khá»Ÿi táº¡o tráº¡ng thÃ¡i áº©n ban Ä‘áº§u cá»§a GRU.</li><li class="correct-answer">C. Biá»ƒu diá»…n dÃ i háº¡n `u_s` Ä‘Æ°á»£c ghÃ©p ná»‘i (Concatenation) vá»›i biá»ƒu diá»…n ngáº¯n háº¡n `u_t` (Ä‘áº§u ra cuá»‘i cÃ¹ng cá»§a GRU).</li><li>D. ChÃºng khÃ´ng Ä‘Æ°á»£c káº¿t há»£p.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> SÆ¡ Ä‘á»“ (b) trÃªn slide 14 cho tháº¥y `u_s` (long-term) vÃ  `u_t` (short-term, tá»« GRU) Ä‘Æ°á»£c Ä‘Æ°a vÃ o má»™t khá»‘i "Concatenation". TÃªn gá»i "-con" (concatenation) cÅ©ng gá»£i Ã½ Ä‘iá»u nÃ y.</p></div></div>

        <!-- CATEGORY: TRAINING & EXPERIMENTS -->
        <div class="question-block"><p class="question-text">CÃ¢u 17: Äá»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a user `u` vÃ  item á»©ng viÃªn `c_x` Ä‘Æ°á»£c tÃ­nh báº±ng phÃ©p toÃ¡n gÃ¬?</p><ul class="options"><li>A. Cosine similarity</li><li class="correct-answer">B. TÃ­ch vÃ´ hÆ°á»›ng (dot-product)</li><li>C. Khoáº£ng cÃ¡ch Euclidean</li><li>D. TÃ­ch Hadamard</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 15 Ä‘Æ°a ra cÃ´ng thá»©c `s(u, c_x) = u^T * e_x`. ÄÃ¢y lÃ  phÃ©p toÃ¡n tÃ­ch vÃ´ hÆ°á»›ng. LÃ½ do Ä‘Æ°á»£c nÃªu lÃ  "nháº±m giáº£m thá»i gian tÃ­nh toÃ¡n".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 18: HÃ m má»¥c tiÃªu cá»§a mÃ´ hÃ¬nh (slide 15) dá»±a trÃªn ká»¹ thuáº­t nÃ o?</p><ul class="options"><li>A. Tá»‘i Ä‘a hÃ³a sai sá»‘ bÃ¬nh phÆ°Æ¡ng.</li><li class="correct-answer">B. Tá»‘i thiá»ƒu hÃ³a negative log-likelihood (sá»­ dá»¥ng negative sampling).</li><li>C. Tá»‘i Ä‘a hÃ³a Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng.</li><li>D. Tá»‘i thiá»ƒu hÃ³a sá»‘ lÆ°á»£ng tham sá»‘.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> HÃ m má»¥c tiÃªu cÃ³ dáº¡ng `-Î£ log(...)`. ÄÃ¢y lÃ  dáº¡ng chuáº©n cá»§a viá»‡c tá»‘i thiá»ƒu hÃ³a negative log-likelihood. Máº«u sá»‘ cÃ³ dáº¡ng `exp(...) + Î£ exp(...)` lÃ  Ä‘áº·c trÆ°ng cá»§a hÃ m softmax, thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng trong cÃ¡c mÃ´ hÃ¬nh cÃ³ negative sampling.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 19: "Negative sampling" Ä‘Æ°á»£c sá»­ dá»¥ng trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n cÃ³ nghÄ©a lÃ  gÃ¬?</p><ul class="options"><li>A. Chá»‰ há»c tá»« cÃ¡c vÃ­ dá»¥ ngÆ°á»i dÃ¹ng khÃ´ng thÃ­ch.</li><li class="correct-answer">B. Vá»›i má»—i vÃ­ dá»¥ dÆ°Æ¡ng (positive, ngÆ°á»i dÃ¹ng Ä‘Ã£ click), há»‡ thá»‘ng láº¥y máº«u má»™t sá»‘ K vÃ­ dá»¥ Ã¢m (negative, ngÆ°á»i dÃ¹ng chÆ°a click) Ä‘á»ƒ giÃºp mÃ´ hÃ¬nh há»c cÃ¡ch phÃ¢n biá»‡t.</li><li>C. Láº¥y máº«u ngáº«u nhiÃªn tá»« toÃ n bá»™ dá»¯ liá»‡u.</li><li>D. Loáº¡i bá» cÃ¡c máº«u Ã¢m.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> CÃ´ng thá»©c trÃªn slide 15 cÃ³ táº­p `P` (positive) vÃ  `K` (negative). Ká»¹ thuáº­t nÃ y giÃºp viá»‡c huáº¥n luyá»‡n hiá»‡u quáº£ hÆ¡n nhiá»u so vá»›i viá»‡c tÃ­nh toÃ¡n hÃ m softmax trÃªn toÃ n bá»™ cÃ¡c item.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 20: Ká»¹ thuáº­t "mask" `u_l = M * W_u[u]` Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» gÃ¬?</p><ul class="options"><li>A. Äá»ƒ giáº£m thá»i gian tÃ­nh toÃ¡n.</li><li class="correct-answer">B. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» khÃ´ng pháº£i user nÃ o cÅ©ng cÃ³ sá»Ÿ thÃ­ch dÃ i háº¡n rÃµ rÃ ng (hoáº·c lÃ  ngÆ°á»i dÃ¹ng má»›i).</li><li>C. Äá»ƒ tÄƒng sá»‘ lÆ°á»£ng tham sá»‘.</li><li>D. Äá»ƒ xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 15 ghi: "KhÃ´ng pháº£i user nÃ o cÅ©ng cÃ³ long-term => thÃªm mask". `M` lÃ  má»™t biáº¿n Bernoulli, cÃ³ thá»ƒ báº±ng 0 vá»›i má»™t xÃ¡c suáº¥t `p`. Náº¿u `M=0`, biá»ƒu diá»…n dÃ i háº¡n cá»§a ngÆ°á»i dÃ¹ng Ä‘Ã³ sáº½ bá»‹ vÃ´ hiá»‡u hÃ³a, vÃ  mÃ´ hÃ¬nh chá»‰ dá»±a vÃ o sá»Ÿ thÃ­ch ngáº¯n háº¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 21: (Äiá»n Ä‘Ã¡p Ã¡n) Bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng trong thá»±c nghiá»‡m cá»§a bÃ i bÃ¡o lÃ  gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">MSN</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 16 cÃ³ tiÃªu Ä‘á» "Bá»™ dá»¯ liá»‡u MSN".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 22: (Äiá»n Ä‘Ã¡p Ã¡n) Theo báº£ng thá»‘ng kÃª dá»¯ liá»‡u (slide 16), sá»‘ lÆ°á»£ng máº«u Ã¢m (negative samples) nhiá»u hÆ¡n sá»‘ lÆ°á»£ng máº«u dÆ°Æ¡ng (positive samples) khoáº£ng bao nhiÃªu láº§n?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Khoáº£ng 18-19 láº§n</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Sá»‘ máº«u Ã¢m lÃ  9,224,537 vÃ  sá»‘ máº«u dÆ°Æ¡ng lÃ  492,185. Tá»· lá»‡ `9,224,537 / 492,185 â‰ˆ 18.74`. GiÃ¡ trá»‹ `NP ratio` trong báº£ng cÅ©ng lÃ  18.74.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 23: Theo báº£ng káº¿t quáº£ á»Ÿ slide 17, mÃ´ hÃ¬nh nÃ o cho káº¿t quáº£ AUC cao nháº¥t?</p><ul class="options"><li>A. LSTUR-con</li><li class="correct-answer">B. LSTUR-ini</li><li>C. GRU</li><li>D. CNN</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong báº£ng á»Ÿ slide 17, hÃ ng "LSTUR-ini" cÃ³ giÃ¡ trá»‹ AUC cao nháº¥t lÃ  63.56 Â± 0.42.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 24: Theo báº£ng káº¿t quáº£ á»Ÿ slide 17, mÃ´ hÃ¬nh nÃ o lÃ  baseline yáº¿u nháº¥t?</p><ul class="options"><li class="correct-answer">A. LibFM</li><li>B. DeepFM</li><li>C. GRU</li><li>D. LSTUR-ini</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong báº£ng, hÃ ng "LibFM" cÃ³ cÃ¡c giÃ¡ trá»‹ AUC, MRR, nDCG@5, nDCG@10 Ä‘á»u tháº¥p nháº¥t so vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 25: Theo Figure 4 (slide 18), viá»‡c káº¿t há»£p cáº£ sá»Ÿ thÃ­ch dÃ i háº¡n (LTUR) vÃ  ngáº¯n háº¡n (STUR) mang láº¡i hiá»‡u quáº£ nhÆ° tháº¿ nÃ o so vá»›i chá»‰ dÃ¹ng má»™t trong hai?</p><ul class="options"><li>A. KhÃ´ng cÃ³ sá»± khÃ¡c biá»‡t.</li><li class="correct-answer">B. CÃ¡c mÃ´ hÃ¬nh káº¿t há»£p (LSTUR-con, LSTUR-ini) cho káº¿t quáº£ tá»‘t hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i chá»‰ dÃ¹ng LTUR hoáº·c STUR.</li><li>C. Chá»‰ dÃ¹ng LTUR lÃ  tá»‘t nháº¥t.</li><li>D. Chá»‰ dÃ¹ng STUR lÃ  tá»‘t nháº¥t.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Biá»ƒu Ä‘á»“ trong Figure 4 cho tháº¥y cÃ¡c cá»™t mÃ u xanh lÃ¡ (LSTUR-con, LSTUR-ini) cao hÆ¡n háº³n cÃ¡c cá»™t mÃ u vÃ ng (LTUR) vÃ  xanh dÆ°Æ¡ng (STUR) á»Ÿ cáº£ hai Ä‘á»™ Ä‘o AUC vÃ  nDCG@10, chá»©ng minh hiá»‡u quáº£ cá»§a viá»‡c káº¿t há»£p.</p></div></div>
        
        <!-- ... More questions from 26 to 100 ... -->
        <div class="question-block"><p class="question-text">CÃ¢u 26: Theo Figure 5 (slide 18), phÆ°Æ¡ng phÃ¡p nÃ o há»c biá»ƒu diá»…n ngáº¯n háº¡n hiá»‡u quáº£ nháº¥t?</p><ul class="options"><li>A. Average</li><li>B. Attention</li><li>C. LSTM</li><li class="correct-answer">D. GRU</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong Figure 5, cá»™t mÃ u xanh lÃ¡ Ä‘áº­m (GRU) lÃ  cá»™t cao nháº¥t á»Ÿ cáº£ AUC vÃ  nDCG@10, cho tháº¥y GRU lÃ  lá»±a chá»n tá»‘t nháº¥t Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a sá»Ÿ thÃ­ch ngáº¯n háº¡n trong cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c so sÃ¡nh.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 27: Theo Figure 6 (slide 19), viá»‡c thÃªm cÆ¡ cháº¿ Attention vÃ o cÃ¡c mÃ´ hÃ¬nh há»c biá»ƒu diá»…n title (LSTM+Att, CNN+Att) cÃ³ tÃ¡c dá»¥ng gÃ¬?</p><ul class="options"><li>A. LÃ m giáº£m hiá»‡u suáº¥t.</li><li class="correct-answer">B. Cáº£i thiá»‡n hiá»‡u suáº¥t (tÄƒng AUC vÃ  nDCG).</li><li>C. KhÃ´ng thay Ä‘á»•i hiá»‡u suáº¥t.</li><li>D. LÃ m cho mÃ´ hÃ¬nh cháº¡y cháº­m hÆ¡n.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong cáº£ hai biá»ƒu Ä‘á»“ (a) vÃ  (b), cá»™t LSTM+Att cao hÆ¡n LSTM, vÃ  cá»™t CNN+Att cao hÆ¡n CNN. Äiá»u nÃ y cho tháº¥y viá»‡c thÃªm Attention Ä‘á»ƒ chá»n lá»c cÃ¡c tá»« quan trá»ng trong title lÃ  má»™t cáº£i tiáº¿n hiá»‡u quáº£.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 28: Theo Figure 7 (slide 20), viá»‡c thÃªm thÃ´ng tin Topic vÃ  Subtopic vÃ o News Encoder cÃ³ hiá»‡u quáº£ khÃ´ng?</p><ul class="options"><li class="correct-answer">A. CÃ³, viá»‡c thÃªm cáº£ hai (+Both) cho káº¿t quáº£ tá»‘t nháº¥t.</li><li>B. KhÃ´ng, viá»‡c khÃ´ng dÃ¹ng thÃ´ng tin nÃ y (None) láº¡i tá»‘t hÆ¡n.</li><li>C. Chá»‰ thÃªm Topic lÃ  tá»‘t.</li><li>D. Chá»‰ thÃªm Subtopic lÃ  tá»‘t.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong cáº£ hai biá»ƒu Ä‘á»“, cá»™t mÃ u xanh lÃ¡ Ä‘áº­m (+Both) lÃ  cá»™t cao nháº¥t, cho tháº¥y viá»‡c káº¿t há»£p thÃ´ng tin tá»« cáº£ Topic vÃ  Subtopic giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t gá»£i Ã½.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 29: Biá»ƒu Ä‘á»“ á»Ÿ slide 21 phÃ¢n tÃ­ch áº£nh hÆ°á»Ÿng cá»§a tham sá»‘ nÃ o?</p><ul class="options"><li>A. KÃ­ch thÆ°á»›c embedding.</li><li>B. Sá»‘ lÆ°á»£ng táº§ng GRU.</li><li class="correct-answer">C. XÃ¡c suáº¥t mask (Mask probability p).</li><li>D. Tá»‘c Ä‘á»™ há»c.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trá»¥c hoÃ nh cá»§a cáº£ hai biá»ƒu Ä‘á»“ trÃªn slide 21 Ä‘á»u Ä‘Æ°á»£c gÃ¡n nhÃ£n lÃ  "Mask probability p".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 30: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 22, má»™t trong nhá»¯ng Æ°u Ä‘iá»ƒm cá»§a mÃ´ hÃ¬nh lÃ  "ThÃªm attention weight Ä‘á»ƒ...". HoÃ n thÃ nh cÃ¢u nÃ y.</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Ä‘Ã¡nh trá»ng sá»‘ cho context quan trá»ng.</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 22, má»¥c "Æ¯u Ä‘iá»ƒm", cÃ³ ghi: "ThÃªm attention weight Ä‘á»ƒ Ä‘Ã¡nh trá»ng sá»‘ cho context quan trá»ng."</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 31: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 22, má»™t trong nhá»¯ng nhÆ°á»£c Ä‘iá»ƒm cá»§a mÃ´ hÃ¬nh lÃ  chÆ°a khai thÃ¡c Ä‘Æ°á»£c loáº¡i thÃ´ng tin nÃ o?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Entity (thá»±c thá»ƒ)</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 22, má»¥c "NhÆ°á»£c Ä‘iá»ƒm", ghi: "ChÆ°a khai thÃ¡c Ä‘Æ°á»£c entity".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 32: GRU lÃ  má»™t biáº¿n thá»ƒ cá»§a máº¡ng nÆ¡-ron nÃ o?</p><ul class="options"><li class="correct-answer">A. RNN (Recurrent Neural Network)</li><li>B. CNN (Convolutional Neural Network)</li><li>C. FNN (Feed-forward Neural Network)</li><li>D. Transformer</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> GRU vÃ  LSTM lÃ  hai biáº¿n thá»ƒ phá»• biáº¿n nháº¥t cá»§a RNN, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» vanishing/exploding gradient vÃ  náº¯m báº¯t cÃ¡c phá»¥ thuá»™c dÃ i háº¡n tá»‘t hÆ¡n RNN truyá»n thá»‘ng.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 33: Trong cÃ´ng thá»©c cá»§a Title Encoder (slide 8), `C Ã— W[i-M:i+M]` lÃ  má»™t phÃ©p toÃ¡n Ä‘áº·c trÆ°ng cá»§a máº¡ng nÃ o?</p><ul class="options"><li>A. RNN</li><li class="correct-answer">B. CNN</li><li>C. Attention</li><li>D. FNN</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> ÄÃ¢y lÃ  phÃ©p toÃ¡n tÃ­ch cháº­p (convolution) má»™t chiá»u. `W[i-M:i+M]` lÃ  má»™t cá»­a sá»• (window) cÃ¡c vector embedding cá»§a tá»«, vÃ  `C` lÃ  ma tráº­n bá»™ lá»c (filter). PhÃ©p toÃ¡n nÃ y lÃ  cá»‘t lÃµi cá»§a máº¡ng CNN khi Ã¡p dá»¥ng cho dá»¯ liá»‡u tuáº§n tá»±.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 34: HÃ m kÃ­ch hoáº¡t `tanh` Ä‘Æ°á»£c sá»­ dá»¥ng á»Ÿ Ä‘Ã¢u trong mÃ´ hÃ¬nh?</p><ul class="options"><li>A. á» táº§ng cuá»‘i cÃ¹ng cá»§a mÃ´ hÃ¬nh.</li><li class="correct-answer">B. Trong cÆ¡ cháº¿ attention Ä‘á»ƒ tÃ­nh Ä‘iá»ƒm attention thÃ´ (`a_i`).</li><li>C. Äá»ƒ khá»Ÿi táº¡o embedding.</li><li>D. Trong hÃ m loss.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 8 trÃ¬nh bÃ y cÃ´ng thá»©c `a_i = tanh(v Ã— c_i + v_b)`. HÃ m `tanh` Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ Ä‘Æ°a ra má»™t Ä‘iá»ƒm sá»‘ trÆ°á»›c khi qua hÃ m softmax Ä‘á»ƒ tÃ­nh trá»ng sá»‘ attention.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 35: Viá»‡c concat cÃ¡c vector topic vÃ  subtopic (slide 9) cÃ³ Ã½ nghÄ©a gÃ¬?</p><ul class="options"><li>A. Äá»ƒ giáº£m sá»‘ chiá»u.</li><li class="correct-answer">B. Äá»ƒ káº¿t há»£p thÃ´ng tin tá»« cáº£ hai cáº¥p Ä‘á»™ category (chá»§ Ä‘á» chÃ­nh vÃ  chá»§ Ä‘á» phá»¥) thÃ nh má»™t biá»ƒu diá»…n duy nháº¥t.</li><li>C. Äá»ƒ tÃ­nh toÃ¡n attention.</li><li>D. Äá»ƒ so sÃ¡nh chÃºng vá»›i nhau.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Má»™t bÃ i bÃ¡o cÃ³ thá»ƒ thuá»™c má»™t topic lá»›n (vÃ­ dá»¥: Thá»ƒ thao) vÃ  má»™t subtopic nhá» hÆ¡n (vÃ­ dá»¥: BÃ³ng rá»•). Viá»‡c káº¿t há»£p cáº£ hai embedding nÃ y giÃºp mÃ´ hÃ¬nh cÃ³ Ä‘Æ°á»£c má»™t biá»ƒu diá»…n danh má»¥c chi tiáº¿t vÃ  Ä‘áº§y Ä‘á»§ hÆ¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 36: Dá»±a trÃªn sÆ¡ Ä‘á»“ á»Ÿ slide 14, mÃ´ hÃ¬nh LSTUR-ini vÃ  LSTUR-con khÃ¡c nhau á»Ÿ Ä‘Ã¢u?</p><ul class="options"><li>A. á» News Encoder.</li><li class="correct-answer">B. á» cÃ¡ch biá»ƒu diá»…n dÃ i háº¡n (user embedding) Ä‘Æ°á»£c tÃ­ch há»£p vÃ o mÃ´ hÃ¬nh.</li><li>C. á» loáº¡i RNN Ä‘Æ°á»£c sá»­ dá»¥ng.</li><li>D. á» hÃ m loss.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Cáº£ hai mÃ´ hÃ¬nh Ä‘á»u cÃ³ cÃ¹ng cáº¥u trÃºc News Encoder vÃ  GRU. Äiá»ƒm khÃ¡c biá»‡t duy nháº¥t Ä‘Æ°á»£c minh há»a lÃ  LSTUR-ini sá»­ dá»¥ng user embedding Ä‘á»ƒ khá»Ÿi táº¡o GRU, trong khi LSTUR-con ghÃ©p ná»‘i user embedding vá»›i Ä‘áº§u ra cá»§a GRU.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 37: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 15, `P` vÃ  `K` trong hÃ m má»¥c tiÃªu láº§n lÆ°á»£t lÃ  táº­p positive vÃ  táº­p gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Negative</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 15 Ä‘á»‹nh nghÄ©a "â—‹ P : táº­p positive" vÃ  "â—‹ K : táº­p negative".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 38: Theo báº£ng káº¿t quáº£ á»Ÿ slide 17, mÃ´ hÃ¬nh LSTUR-ini vÃ  LSTUR-con, mÃ´ hÃ¬nh nÃ o cho káº¿t quáº£ tá»‘t hÆ¡n má»™t cÃ¡ch nháº¥t quÃ¡n?</p><ul class="options"><li>A. LSTUR-con</li><li class="correct-answer">B. LSTUR-ini</li><li>C. ChÃºng tÆ°Æ¡ng Ä‘Æ°Æ¡ng nhau.</li><li>D. TÃ¹y thuá»™c vÃ o Ä‘á»™ Ä‘o.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> So sÃ¡nh hai hÃ ng cuá»‘i cÃ¹ng trong báº£ng, LSTUR-ini (63.56, 30.98, 33.45, 41.37) cÃ³ táº¥t cáº£ cÃ¡c chá»‰ sá»‘ Ä‘á»u cao hÆ¡n má»™t chÃºt so vá»›i LSTUR-con (63.47, 30.94, 33.43, 41.34). Äiá»u nÃ y cho tháº¥y chiáº¿n lÆ°á»£c khá»Ÿi táº¡o GRU báº±ng biá»ƒu diá»…n dÃ i háº¡n hiá»‡u quáº£ hÆ¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 39: Theo Figure 6 (slide 19), mÃ´ hÃ¬nh nÃ o há»c biá»ƒu diá»…n title tá»‘t nháº¥t?</p><ul class="options"><li>A. LSTM</li><li>B. LSTM+Att</li><li>C. CNN</li><li class="correct-answer">D. CNN+Att</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong cáº£ hai biá»ƒu Ä‘á»“ (a) vÃ  (b), cá»™t mÃ u xanh lÃ¡ Ä‘áº­m (CNN+Att) lÃ  cá»™t cao nháº¥t, cho tháº¥y CNN káº¿t há»£p vá»›i Attention lÃ  phÆ°Æ¡ng phÃ¡p hiá»‡u quáº£ nháº¥t Ä‘á»ƒ mÃ£ hÃ³a tiÃªu Ä‘á» trong cÃ¡c lá»±a chá»n Ä‘Æ°á»£c so sÃ¡nh.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 40: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 22, má»™t nhÆ°á»£c Ä‘iá»ƒm cá»§a mÃ´ hÃ¬nh lÃ  viá»‡c há»c cÃ¹ng lÃºc cho item vÃ  user cÃ³ thá»ƒ dáº«n Ä‘áº¿n Ä‘iá»u gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Train lÃ¢u</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 22, má»¥c "NhÆ°á»£c Ä‘iá»ƒm", ghi: "Viá»‡c há»c cÃ¹ng 1 lÃºc cho viá»‡c biá»ƒu diá»…n item vÃ  user cÃ³ thá»ƒ dáº«n Ä‘áº¿n train lÃ¢u."</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 41: "Cat lá»›n vÃ  Cat nhá» sáº½ cÃ³ pháº§n nÃ o Ä‘Ã³ liÃªn quan Ä‘áº¿n nhau" (slide 22) lÃ  má»™t nháº­n xÃ©t vá» má»‘i quan há»‡ giá»¯a cÃ¡i gÃ¬?</p><ul class="options"><li>A. Title vÃ  Content.</li><li class="correct-answer">B. Topic vÃ  Subtopic.</li><li>C. User vÃ  Item.</li><li>D. Long-term vÃ  Short-term.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> "Cat" lÃ  viáº¿t táº¯t cá»§a Category. "Cat lá»›n" (vÃ­ dá»¥: Thá»ƒ thao) vÃ  "Cat nhá»" (vÃ­ dá»¥: BÃ³ng Ä‘Ã¡) rÃµ rÃ ng cÃ³ liÃªn quan. NhÆ°á»£c Ä‘iá»ƒm Ä‘Æ°á»£c nÃªu lÃ  viá»‡c há»c biá»ƒu diá»…n riÃªng ráº½ cho chÃºng cÃ³ thá»ƒ lÃ m máº¥t Ä‘i thÃ´ng tin vá» má»‘i quan há»‡ phÃ¢n cáº¥p nÃ y.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 42: Theo slide 21, khi xÃ¡c suáº¥t mask `p` tÄƒng lÃªn, hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh cÃ³ xu hÆ°á»›ng thay Ä‘á»•i nhÆ° tháº¿ nÃ o?</p><ul class="options"><li>A. LuÃ´n tÄƒng.</li><li>B. LuÃ´n giáº£m.</li><li class="correct-answer">C. TÄƒng lÃªn Ä‘áº¿n má»™t Ä‘iá»ƒm tá»‘i Æ°u rá»“i sau Ä‘Ã³ giáº£m xuá»‘ng.</li><li>D. KhÃ´ng thay Ä‘á»•i.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> CÃ¡c Ä‘Æ°á»ng cong trong cáº£ hai biá»ƒu Ä‘á»“ trÃªn slide 21 Ä‘á»u cho tháº¥y hiá»‡u suáº¥t (AUC, MRR, nDCG) tÄƒng khi `p` Ä‘i tá»« 0.0 Ä‘áº¿n khoáº£ng 0.5-0.6, sau Ä‘Ã³ báº¯t Ä‘áº§u giáº£m dáº§n khi `p` tiáº¿n Ä‘áº¿n 0.9. Äiá»u nÃ y cho tháº¥y cÃ³ má»™t má»©c Ä‘á»™ "che" biá»ƒu diá»…n dÃ i háº¡n tá»‘i Æ°u.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 43: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 16, NP ratio lÃ  viáº¿t táº¯t cá»§a cÃ¡i gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Tá»· lá»‡ máº«u Ã‚m/DÆ°Æ¡ng (Negative/Positive ratio)</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> NP ratio lÃ  tá»· lá»‡ giá»¯a sá»‘ lÆ°á»£ng máº«u Ã¢m (# of negative samples) vÃ  sá»‘ lÆ°á»£ng máº«u dÆ°Æ¡ng (# of positive samples).</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 44: MÃ´ hÃ¬nh LSTUR-ini vÃ  LSTUR-con lÃ  cÃ¡c biáº¿n thá»ƒ cá»§a mÃ´ hÃ¬nh nÃ o?</p><ul class="options"><li>A. HyperNews</li><li class="correct-answer">B. LSTUR (Long- and Short-term User Representation)</li><li>C. LibFM</li><li>D. DKN</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> TÃªn gá»i LSTUR-ini vÃ  LSTUR-con cho tháº¥y chÃºng lÃ  hai phiÃªn báº£n/chiáº¿n lÆ°á»£c cá»¥ thá»ƒ cá»§a má»™t mÃ´ hÃ¬nh chung gá»i lÃ  LSTUR, Ä‘Æ°á»£c Ä‘á» xuáº¥t trong bÃ i bÃ¡o nÃ y.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 45: Náº¿u má»™t user lÃ  ngÆ°á»i dÃ¹ng má»›i, biá»ƒu diá»…n dÃ i háº¡n `u` cá»§a há» sáº½ nhÆ° tháº¿ nÃ o?</p><ul class="options"><li>A. Sáº½ Ä‘Æ°á»£c tÃ­nh tá»« ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tá»±.</li><li class="correct-answer">B. Sáº½ chá»‰ lÃ  má»™t vector Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn vÃ  chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n.</li><li>C. Sáº½ báº±ng vector khÃ´ng.</li><li>D. Há»‡ thá»‘ng khÃ´ng thá»ƒ xá»­ lÃ½.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> VÃ¬ biá»ƒu diá»…n dÃ i háº¡n Ä‘Æ°á»£c há»c tá»« lá»‹ch sá»­, ngÆ°á»i dÃ¹ng má»›i sáº½ chÆ°a cÃ³ dá»¯ liá»‡u Ä‘á»ƒ há»c. Há» sáº½ báº¯t Ä‘áº§u vá»›i má»™t vector embedding ngáº«u nhiÃªn. Ká»¹ thuáº­t mask (slide 15) cÅ©ng cÃ³ thá»ƒ giÃºp xá»­ lÃ½ trÆ°á»ng há»£p nÃ y báº±ng cÃ¡ch vÃ´ hiá»‡u hÃ³a áº£nh hÆ°á»Ÿng cá»§a biá»ƒu diá»…n dÃ i háº¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 46: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 16, NP ratio lÃ  viáº¿t táº¯t cá»§a cÃ¡i gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Tá»· lá»‡ máº«u Ã‚m/DÆ°Æ¡ng (Negative/Positive ratio)</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> NP ratio lÃ  tá»· lá»‡ giá»¯a sá»‘ lÆ°á»£ng máº«u Ã¢m (# of negative samples) vÃ  sá»‘ lÆ°á»£ng máº«u dÆ°Æ¡ng (# of positive samples).</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 47: MÃ´ hÃ¬nh LSTUR-ini vÃ  LSTUR-con lÃ  cÃ¡c biáº¿n thá»ƒ cá»§a mÃ´ hÃ¬nh nÃ o?</p><ul class="options"><li>A. HyperNews</li><li class="correct-answer">B. LSTUR (Long- and Short-term User Representation)</li><li>C. LibFM</li><li>D. DKN</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> TÃªn gá»i LSTUR-ini vÃ  LSTUR-con cho tháº¥y chÃºng lÃ  hai phiÃªn báº£n/chiáº¿n lÆ°á»£c cá»¥ thá»ƒ cá»§a má»™t mÃ´ hÃ¬nh chung gá»i lÃ  LSTUR, Ä‘Æ°á»£c Ä‘á» xuáº¥t trong bÃ i bÃ¡o nÃ y.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 48: Náº¿u má»™t user lÃ  ngÆ°á»i dÃ¹ng má»›i, biá»ƒu diá»…n dÃ i háº¡n `u` cá»§a há» sáº½ nhÆ° tháº¿ nÃ o?</p><ul class="options"><li>A. Sáº½ Ä‘Æ°á»£c tÃ­nh tá»« ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tá»±.</li><li class="correct-answer">B. Sáº½ chá»‰ lÃ  má»™t vector Ä‘Æ°á»£c khá»Ÿi táº¡o ngáº«u nhiÃªn vÃ  chÆ°a Ä‘Æ°á»£c huáº¥n luyá»‡n.</li><li>C. Sáº½ báº±ng vector khÃ´ng.</li><li>D. Há»‡ thá»‘ng khÃ´ng thá»ƒ xá»­ lÃ½.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> VÃ¬ biá»ƒu diá»…n dÃ i háº¡n Ä‘Æ°á»£c há»c tá»« lá»‹ch sá»­, ngÆ°á»i dÃ¹ng má»›i sáº½ chÆ°a cÃ³ dá»¯ liá»‡u Ä‘á»ƒ há»c. Há» sáº½ báº¯t Ä‘áº§u vá»›i má»™t vector embedding ngáº«u nhiÃªn. Ká»¹ thuáº­t mask (slide 15) cÅ©ng cÃ³ thá»ƒ giÃºp xá»­ lÃ½ trÆ°á»ng há»£p nÃ y báº±ng cÃ¡ch vÃ´ hiá»‡u hÃ³a áº£nh hÆ°á»Ÿng cá»§a biá»ƒu diá»…n dÃ i háº¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 49: Táº¡i sao mÃ´ hÃ¬nh láº¡i sá»­ dá»¥ng cáº£ LDA vÃ  Doc2vec Ä‘á»ƒ biá»ƒu diá»…n ná»™i dung?</p><ul class="options"><li>A. VÃ¬ má»™t trong hai cÃ³ thá»ƒ bá»‹ lá»—i.</li><li class="correct-answer">B. Äá»ƒ táº­n dá»¥ng Ä‘iá»ƒm máº¡nh cá»§a cáº£ hai phÆ°Æ¡ng phÃ¡p; LDA náº¯m báº¯t tá»‘t cÃ¡c chá»§ Ä‘á» (topic), trong khi Doc2vec náº¯m báº¯t tá»‘t ngá»¯ nghÄ©a á»Ÿ cáº¥p Ä‘á»™ cÃ¢u/Ä‘oáº¡n vÄƒn.</li><li>C. VÃ¬ tÃ¡c giáº£ khÃ´ng quyáº¿t Ä‘á»‹nh Ä‘Æ°á»£c nÃªn dÃ¹ng cÃ¡i nÃ o.</li><li>D. Äá»ƒ tÄƒng sá»‘ chiá»u cá»§a vector.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> ÄÃ¢y lÃ  má»™t ká»¹ thuáº­t ensemble á»Ÿ má»©c Ä‘áº·c trÆ°ng. Báº±ng cÃ¡ch káº¿t há»£p cÃ¡c biá»ƒu diá»…n tá»« cÃ¡c mÃ´ hÃ¬nh khÃ¡c nhau, má»—i mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Ã³ng gÃ³p má»™t loáº¡i thÃ´ng tin khÃ¡c nhau, giÃºp cho biá»ƒu diá»…n tá»•ng há»£p trá»Ÿ nÃªn phong phÃº vÃ  máº¡nh máº½ hÆ¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 50: Theo káº¿t quáº£ á»Ÿ slide 17, mÃ´ hÃ¬nh Wide&Deep cÃ³ hiá»‡u suáº¥t nhÆ° tháº¿ nÃ o so vá»›i LSTUR-con?</p><ul class="options"><li>A. Tá»‘t hÆ¡n</li><li class="correct-answer">B. KÃ©m hÆ¡n Ä‘Ã¡ng ká»ƒ</li><li>C. TÆ°Æ¡ng Ä‘Æ°Æ¡ng</li><li>D. KhÃ´ng thá»ƒ so sÃ¡nh</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> TrÃªn táº¥t cáº£ cÃ¡c Ä‘á»™ Ä‘o (AUC, MRR, nDCG), cÃ¡c giÃ¡ trá»‹ cá»§a LSTUR-con (vÃ­ dá»¥ AUC 63.47) Ä‘á»u cao hÆ¡n Ä‘Ã¡ng ká»ƒ so vá»›i Wide&Deep (vÃ­ dá»¥ AUC 58.07).</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 51: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 22, má»™t nhÆ°á»£c Ä‘iá»ƒm cá»§a mÃ´ hÃ¬nh lÃ  viá»‡c há»c biá»ƒu diá»…n riÃªng ráº½ cho "cat to" vÃ  "cat nhá»" cÃ³ thá»ƒ lÃ m "sai lá»‡ch" Ä‘iá»u gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">ThÃ´ng tin</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 22 ghi: "...há»c biá»ƒu diá»…n riÃªng ráº½ cat to vÃ  cat nhá» cÃ³ thá»ƒ lÃ m â€œsai lá»‡châ€ thÃ´ng tin".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 52: Trong Title encoder (slide 8), hÃ m `ReLU` Ä‘Æ°á»£c Ã¡p dá»¥ng sau phÃ©p toÃ¡n nÃ o?</p><ul class="options"><li class="correct-answer">A. TÃ­ch cháº­p (Convolution)</li><li>B. Embedding tá»«</li><li>C. Attention</li><li>D. Láº¥y tá»•ng cÃ³ trá»ng sá»‘</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> CÃ´ng thá»©c `c_i = ReLU(C Ã— W[...] + b)` cho tháº¥y hÃ m ReLU Ä‘Æ°á»£c Ã¡p dá»¥ng ngay sau phÃ©p tÃ­ch cháº­p Ä‘á»ƒ táº¡o ra cÃ¡c feature map.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 53: Táº¡i sao bÃ i bÃ¡o láº¡i chá»n GRU thay vÃ¬ LSTM cho viá»‡c há»c sá»Ÿ thÃ­ch ngáº¯n háº¡n (theo Figure 5)?</p><ul class="options"><li>A. VÃ¬ GRU phá»©c táº¡p hÆ¡n LSTM.</li><li class="correct-answer">B. VÃ¬ trong thá»±c nghiá»‡m cá»§a há», GRU cho hiá»‡u suáº¥t cao hÆ¡n LSTM.</li><li>C. VÃ¬ GRU khÃ´ng bá»‹ vanishing gradient.</li><li>D. VÃ¬ LSTM khÃ´ng thá»ƒ xá»­ lÃ½ dá»¯ liá»‡u tin tá»©c.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Biá»ƒu Ä‘á»“ á»Ÿ Figure 5 (slide 18) so sÃ¡nh trá»±c tiáº¿p cÃ¡c phÆ°Æ¡ng phÃ¡p há»c biá»ƒu diá»…n ngáº¯n háº¡n. Káº¿t quáº£ cho tháº¥y cá»™t GRU cao hÆ¡n cá»™t LSTM, chá»©ng tá» trong bá»‘i cáº£nh nÃ y, GRU hoáº¡t Ä‘á»™ng hiá»‡u quáº£ hÆ¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 54: Náº¿u má»™t bÃ i bÃ¡o khÃ´ng cÃ³ subtopic, Topic encoder sáº½ xá»­ lÃ½ nhÆ° tháº¿ nÃ o?</p><ul class="options"><li>A. BÃ¡o lá»—i.</li><li class="correct-answer">B. Vector embedding cá»§a subtopic cÃ³ thá»ƒ lÃ  má»™t vector khÃ´ng hoáº·c má»™t vector "unknown" Ä‘áº·c biá»‡t.</li><li>C. Láº¥y subtopic cá»§a bÃ i bÃ¡o trÆ°á»›c Ä‘Ã³.</li><li>D. Sá»­ dá»¥ng má»™t subtopic ngáº«u nhiÃªn.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong cÃ¡c há»‡ thá»‘ng thá»±c táº¿, cÃ¡c giÃ¡ trá»‹ bá»‹ thiáº¿u (missing categorical features) thÆ°á»ng Ä‘Æ°á»£c xá»­ lÃ½ báº±ng cÃ¡ch gÃ¡n cho chÃºng má»™t embedding riÃªng (vÃ­ dá»¥: má»™t vector há»c Ä‘Æ°á»£c cho giÃ¡ trá»‹ "NULL" hoáº·c "UNKNOWN").</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 55: (Äiá»n Ä‘Ã¡p Ã¡n) Trong hÃ m má»¥c tiÃªu á»Ÿ slide 15, `cx` lÃ  gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Item á»©ng viÃªn (candidate item)</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> `s(u, cx)` lÃ  Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng giá»¯a user `u` vÃ  item á»©ng viÃªn `cx`. Trong cÃ´ng thá»©c log-likelihood, `c_i^p` lÃ  item dÆ°Æ¡ng (positive) vÃ  `c_{i,k}^n` lÃ  cÃ¡c item Ã¢m (negative).</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 56: Ká»¹ thuáº­t Word2vec Ä‘Æ°á»£c Ä‘á» cáº­p trong slide 8 Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬?</p><ul class="options"><li>A. Äá»ƒ huáº¥n luyá»‡n toÃ n bá»™ mÃ´ hÃ¬nh.</li><li class="correct-answer">B. Äá»ƒ khá»Ÿi táº¡o cÃ¡c vector embedding cho cÃ¡c tá»« trong title (pre-train).</li><li>C. Äá»ƒ phÃ¢n loáº¡i topic.</li><li>D. Äá»ƒ tÃ­nh toÃ¡n hÃ m loss.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 8 ghi: "Title Ä‘Æ°á»£c embedding (dÃ¹ng pre-train word2vec,...)". Sá»­ dá»¥ng cÃ¡c embedding Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c (pre-trained) lÃ  má»™t ká»¹ thuáº­t phá»• biáº¿n Ä‘á»ƒ khá»Ÿi táº¡o táº§ng embedding, giÃºp mÃ´ hÃ¬nh há»™i tá»¥ nhanh hÆ¡n vÃ  tá»‘t hÆ¡n, Ä‘áº·c biá»‡t khi dá»¯ liá»‡u huáº¥n luyá»‡n khÃ´ng quÃ¡ lá»›n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 57: Theo slide 17, AUC lÃ  viáº¿t táº¯t cá»§a thuáº­t ngá»¯ gÃ¬?</p><ul class="options"><li>A. Average User Clicks</li><li class="correct-answer">B. Area Under the (ROC) Curve</li><li>C. Attention-based User-CNN</li><li>D. A-User-Centric model</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> AUC lÃ  má»™t Ä‘á»™ Ä‘o tiÃªu chuáº©n trong há»c mÃ¡y, viáº¿t táº¯t cá»§a Area Under the Receiver Operating Characteristic Curve. NÃ³ Ä‘o kháº£ nÄƒng cá»§a má»™t mÃ´ hÃ¬nh phÃ¢n loáº¡i trong viá»‡c xáº¿p háº¡ng cÃ¡c máº«u dÆ°Æ¡ng cao hÆ¡n cÃ¡c máº«u Ã¢m.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 58: Theo slide 17, MRR lÃ  viáº¿t táº¯t cá»§a thuáº­t ngá»¯ gÃ¬?</p><ul class="options"><li>A. Mean Rating Rank</li><li class="correct-answer">B. Mean Reciprocal Rank</li><li>C. Most Relevant Rank</li><li>D. Model Recommendation Rate</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> MRR lÃ  viáº¿t táº¯t cá»§a Mean Reciprocal Rank. NÃ³ Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t xáº¿p háº¡ng báº±ng cÃ¡ch láº¥y giÃ¡ trá»‹ trung bÃ¬nh cá»§a nghá»‹ch Ä‘áº£o thá»© háº¡ng cá»§a cÃ¢u tráº£ lá»i Ä‘Ãºng Ä‘áº§u tiÃªn.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 59: Theo slide 17, nDCG lÃ  viáº¿t táº¯t cá»§a thuáº­t ngá»¯ gÃ¬?</p><ul class="options"><li>A. New Document-Centric Gain</li><li class="correct-answer">B. Normalized Discounted Cumulative Gain</li><li>C. Neural Dynamic Contextual Graph</li><li>D. Negative Document Correlation Grade</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> nDCG lÃ  viáº¿t táº¯t cá»§a Normalized Discounted Cumulative Gain, má»™t Ä‘á»™ Ä‘o xáº¿p háº¡ng phá»• biáº¿n cÃ³ tÃ­nh Ä‘áº¿n cáº£ thá»© háº¡ng vÃ  má»©c Ä‘á»™ liÃªn quan cá»§a cÃ¡c item Ä‘Æ°á»£c gá»£i Ã½.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 60: Káº¿t quáº£ thá»±c nghiá»‡m á»Ÿ slide 17 cho tháº¥y mÃ´ hÃ¬nh LSTUR-ini vÃ  LSTUR-con vÆ°á»£t trá»™i hÆ¡n cÃ¡c baseline trÃªn nhá»¯ng Ä‘á»™ Ä‘o nÃ o?</p><ul class="options"><li>A. Chá»‰ AUC</li><li>B. Chá»‰ MRR</li><li>C. Chá»‰ nDCG</li><li class="correct-answer">D. Táº¥t cáº£ cÃ¡c Ä‘á»™ Ä‘o (AUC, MRR, nDCG@5, nDCG@10)</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> NhÃ¬n vÃ o hai hÃ ng cuá»‘i cá»§a báº£ng, cÃ¡c giÃ¡ trá»‹ cá»§a LSTUR-con vÃ  LSTUR-ini Ä‘á»u cao hÆ¡n táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ tÆ°Æ¡ng á»©ng cá»§a cÃ¡c mÃ´ hÃ¬nh baseline khÃ¡c trÃªn táº¥t cáº£ cÃ¡c cá»™t Ä‘á»™ Ä‘o.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 61: Trong User Encoder (slide 12), phÃ©p toÃ¡n `rt âŠ™ ht-1` trong cÃ´ng thá»©c tÃ­nh `h_tilde_t` cÃ³ Ã½ nghÄ©a gÃ¬?</p><ul class="options"><li>A. Káº¿t há»£p tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³ vá»›i cá»•ng reset.</li><li class="correct-answer">B. Ãp dá»¥ng cá»•ng reset `rt` lÃªn tráº¡ng thÃ¡i áº©n trÆ°á»›c Ä‘Ã³ `ht-1` Ä‘á»ƒ quyáº¿t Ä‘á»‹nh xem bao nhiÃªu thÃ´ng tin tá»« quÃ¡ khá»© cáº§n Ä‘Æ°á»£c "quÃªn" Ä‘i.</li><li>C. Cáº­p nháº­t tráº¡ng thÃ¡i áº©n.</li><li>D. TÃ­nh toÃ¡n cá»•ng cáº­p nháº­t.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> `rt` lÃ  cá»•ng reset (reset gate) trong GRU. PhÃ©p nhÃ¢n element-wise nÃ y cho phÃ©p mÃ´ hÃ¬nh quyáº¿t Ä‘á»‹nh má»™t cÃ¡ch linh hoáº¡t pháº§n nÃ o cá»§a tráº¡ng thÃ¡i quÃ¡ khá»© `ht-1` lÃ  khÃ´ng cÃ²n liÃªn quan vÃ  cáº§n Ä‘Æ°á»£c bá» qua khi tÃ­nh toÃ¡n tráº¡ng thÃ¡i á»©ng viÃªn má»›i `h_tilde_t`.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 62: Trong User Encoder (slide 12), phÃ©p toÃ¡n `zt âŠ™ ht + (1-zt) âŠ™ h_tilde_t` cÃ³ Ã½ nghÄ©a gÃ¬?</p><ul class="options"><li>A. TÃ­nh toÃ¡n cá»•ng reset.</li><li class="correct-answer">B. Cáº­p nháº­t tráº¡ng thÃ¡i áº©n `ht` báº±ng cÃ¡ch káº¿t há»£p cÃ³ trá»ng sá»‘ giá»¯a tráº¡ng thÃ¡i áº©n cÅ© `ht` vÃ  tráº¡ng thÃ¡i á»©ng viÃªn má»›i `h_tilde_t`, Ä‘Æ°á»£c Ä‘iá»u khiá»ƒn bá»Ÿi cá»•ng cáº­p nháº­t `zt`.</li><li>C. TÃ­nh toÃ¡n Ä‘iá»ƒm attention.</li><li>D. Khá»Ÿi táº¡o tráº¡ng thÃ¡i áº©n.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> `zt` lÃ  cá»•ng cáº­p nháº­t (update gate). CÃ´ng thá»©c nÃ y mÃ´ táº£ cÃ¡ch GRU quyáº¿t Ä‘á»‹nh xem nÃªn giá»¯ láº¡i bao nhiÃªu pháº§n trÄƒm thÃ´ng tin tá»« tráº¡ng thÃ¡i cÅ© `ht` (vá»›i trá»ng sá»‘ `zt`) vÃ  cáº­p nháº­t bao nhiÃªu pháº§n trÄƒm thÃ´ng tin tá»« tráº¡ng thÃ¡i á»©ng viÃªn má»›i `h_tilde_t` (vá»›i trá»ng sá»‘ `1-zt`).</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 63: Táº¡i sao láº¡i cáº§n cÃ³ "Padding" trong sÆ¡ Ä‘á»“ News Encoder (slide 10)?</p><ul class="options"><li>A. Äá»ƒ lÃ m cho title dÃ i hÆ¡n.</li><li class="correct-answer">B. Äá»ƒ Ä‘áº£m báº£o cÃ¡c phÃ©p toÃ¡n tÃ­ch cháº­p cá»§a CNN cÃ³ thá»ƒ Ä‘Æ°á»£c Ã¡p dá»¥ng á»Ÿ cÃ¡c vá»‹ trÃ­ Ä‘áº§u vÃ  cuá»‘i cá»§a chuá»—i tá»«.</li><li>C. Äá»ƒ tÄƒng sá»‘ lÆ°á»£ng tá»«.</li><li>D. Äá»ƒ cáº£i thiá»‡n embedding.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Khi Ã¡p dá»¥ng má»™t bá»™ lá»c (filter) cÃ³ kÃ­ch thÆ°á»›c lá»›n hÆ¡n 1, vÃ­ dá»¥ kÃ­ch thÆ°á»›c 3, lÃªn má»™t chuá»—i, cÃ¡c tá»« á»Ÿ biÃªn (tá»« Ä‘áº§u tiÃªn vÃ  cuá»‘i cÃ¹ng) sáº½ khÃ´ng thá»ƒ náº±m á»Ÿ trung tÃ¢m cá»§a cá»­a sá»• filter. Padding thÃªm cÃ¡c vector khÃ´ng (hoáº·c vector Ä‘áº·c biá»‡t) vÃ o hai Ä‘áº§u chuá»—i Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y, Ä‘áº£m báº£o Ä‘áº§u ra cá»§a CNN cÃ³ cÃ¹ng Ä‘á»™ dÃ i vá»›i Ä‘áº§u vÃ o.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 64: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 16, bá»™ dá»¯ liá»‡u MSN cÃ³ bao nhiÃªu ngÆ°á»i dÃ¹ng (# of users)?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">25,000</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Báº£ng á»Ÿ slide 16, hÃ ng "# of users" cÃ³ giÃ¡ trá»‹ lÃ  25,000.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 65: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 16, sá»‘ lÆ°á»£ng máº«u dÆ°Æ¡ng (# of positive samples) trong bá»™ dá»¯ liá»‡u MSN lÃ  bao nhiÃªu?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">492,185</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Báº£ng á»Ÿ slide 16, hÃ ng "# of positive samples" cÃ³ giÃ¡ trá»‹ lÃ  492,185.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 66: DKN trong báº£ng so sÃ¡nh (slide 17) cÃ³ thá»ƒ lÃ  viáº¿t táº¯t cá»§a thuáº­t ngá»¯ gÃ¬?</p><ul class="options"><li class="correct-answer">A. Deep Knowledge-Aware Network</li><li>B. Dynamic K-Nearest Neighbors</li><li>C. Deep Kernel Network</li><li>D. Double-task Knowledge Network</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> DKN lÃ  má»™t mÃ´ hÃ¬nh ná»•i tiáº¿ng khÃ¡c cho gá»£i Ã½ tin tá»©c, viáº¿t táº¯t cá»§a Deep Knowledge-Aware Network. NÃ³ káº¿t há»£p thÃ´ng tin tá»« Ä‘á»“ thá»‹ tri thá»©c Ä‘á»ƒ lÃ m giÃ u cho biá»ƒu diá»…n cá»§a tin tá»©c.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 67: Viá»‡c mÃ´ hÃ¬nh LSTUR-ini hoáº¡t Ä‘á»™ng tá»‘t hÆ¡n LSTUR-con (slide 17, 18) cho tháº¥y Ä‘iá»u gÃ¬?</p><ul class="options"><li>A. PhÃ©p ghÃ©p ná»‘i luÃ´n tá»‡ hÆ¡n.</li><li class="correct-answer">B. Viá»‡c sá»­ dá»¥ng sá»Ÿ thÃ­ch dÃ i háº¡n Ä‘á»ƒ Ä‘á»‹nh hÆ°á»›ng (khá»Ÿi táº¡o) cho quÃ¡ trÃ¬nh xá»­ lÃ½ sá»Ÿ thÃ­ch ngáº¯n háº¡n cÃ³ thá»ƒ hiá»‡u quáº£ hÆ¡n lÃ  chá»‰ káº¿t há»£p chÃºng má»™t cÃ¡ch Ä‘Æ¡n giáº£n á»Ÿ cuá»‘i.</li><li>C. Sá»Ÿ thÃ­ch dÃ i háº¡n khÃ´ng quan trá»ng.</li><li>D. Sá»Ÿ thÃ­ch ngáº¯n háº¡n khÃ´ng quan trá»ng.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Káº¿t quáº£ nÃ y gá»£i Ã½ ráº±ng sá»Ÿ thÃ­ch dÃ i háº¡n cÃ³ thá»ƒ cung cáº¥p má»™t "ngá»¯ cáº£nh" hoáº·c "prior" há»¯u Ã­ch cho viá»‡c diá»…n giáº£i cÃ¡c hÃ nh vi ngáº¯n háº¡n. Viá»‡c khá»Ÿi táº¡o tráº¡ng thÃ¡i cá»§a GRU báº±ng vector sá»Ÿ thÃ­ch dÃ i háº¡n lÃ  má»™t cÃ¡ch Ä‘á»ƒ truyá»n táº£i ngá»¯ cáº£nh nÃ y vÃ o mÃ´ hÃ¬nh tuáº§n tá»±.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 68: Náº¿u xÃ¡c suáº¥t mask `p=1`, ká»¹ thuáº­t mask (slide 15) sáº½ cÃ³ tÃ¡c dá»¥ng gÃ¬?</p><ul class="options"><li>A. Giá»¯ nguyÃªn biá»ƒu diá»…n dÃ i háº¡n.</li><li class="correct-answer">B. LuÃ´n luÃ´n vÃ´ hiá»‡u hÃ³a (mask) biá»ƒu diá»…n dÃ i háº¡n.</li><li>C. VÃ´ hiá»‡u hÃ³a biá»ƒu diá»…n ngáº¯n háº¡n.</li><li>D. KhÃ´ng cÃ³ tÃ¡c dá»¥ng gÃ¬.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Biáº¿n Bernoulli `M` Ä‘Æ°á»£c láº¥y máº«u tá»« `B(1, 1-p)`. Náº¿u `p=1`, thÃ¬ `1-p=0`. XÃ¡c suáº¥t Ä‘á»ƒ `M=1` lÃ  `1-p=0`. Do Ä‘Ã³, `M` sáº½ luÃ´n báº±ng 0, vÃ  `u_l = M * W_u[u]` sáº½ luÃ´n báº±ng 0, tá»©c lÃ  biá»ƒu diá»…n dÃ i háº¡n luÃ´n bá»‹ vÃ´ hiá»‡u hÃ³a.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 69: Náº¿u xÃ¡c suáº¥t mask `p=0`, ká»¹ thuáº­t mask (slide 15) sáº½ cÃ³ tÃ¡c dá»¥ng gÃ¬?</p><ul class="options"><li class="correct-answer">A. LuÃ´n luÃ´n giá»¯ nguyÃªn biá»ƒu diá»…n dÃ i háº¡n.</li><li>B. LuÃ´n luÃ´n vÃ´ hiá»‡u hÃ³a biá»ƒu diá»…n dÃ i háº¡n.</li><li>C. VÃ´ hiá»‡u hÃ³a biá»ƒu diá»…n ngáº¯n háº¡n.</li><li>D. KhÃ´ng cÃ³ tÃ¡c dá»¥ng gÃ¬.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Náº¿u `p=0`, thÃ¬ `1-p=1`. XÃ¡c suáº¥t Ä‘á»ƒ `M=1` lÃ  `1-p=1`. Do Ä‘Ã³, `M` sáº½ luÃ´n báº±ng 1, vÃ  `u_l` sáº½ luÃ´n báº±ng `W_u[u]`, tá»©c lÃ  biá»ƒu diá»…n dÃ i háº¡n khÃ´ng bao giá» bá»‹ mask.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 70: Táº¡i sao biá»ƒu diá»…n user láº¡i cáº§n Ä‘Æ°á»£c cáº­p nháº­t theo thá»i gian?</p><ul class="options"><li>A. VÃ¬ vector sáº½ bá»‹ cÅ©.</li><li class="correct-answer">B. VÃ¬ sá»Ÿ thÃ­ch cá»§a ngÆ°á»i dÃ¹ng, Ä‘áº·c biá»‡t lÃ  sá»Ÿ thÃ­ch ngáº¯n háº¡n, thay Ä‘á»•i liÃªn tá»¥c khi há» tÆ°Æ¡ng tÃ¡c vá»›i cÃ¡c tin tá»©c má»›i.</li><li>C. VÃ¬ News Encoder thay Ä‘á»•i.</li><li>D. VÃ¬ há»‡ thá»‘ng yÃªu cáº§u váº­y.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> MÃ´ hÃ¬nh sá»­ dá»¥ng GRU chÃ­nh lÃ  Ä‘á»ƒ náº¯m báº¯t sá»± thay Ä‘á»•i nÃ y. Khi cÃ³ má»™t lÆ°á»£t click má»›i (`e_t`), GRU sáº½ cáº­p nháº­t tráº¡ng thÃ¡i áº©n (`h_t`) Ä‘á»ƒ pháº£n Ã¡nh sá»± thay Ä‘á»•i trong sá»Ÿ thÃ­ch ngáº¯n háº¡n cá»§a ngÆ°á»i dÃ¹ng.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 71: ToÃ n bá»™ mÃ´ hÃ¬nh LSTUR Ä‘Æ°á»£c huáº¥n luyá»‡n theo phÆ°Æ¡ng phÃ¡p nÃ o?</p><ul class="options"><li>A. Tá»«ng pháº§n riÃªng biá»‡t.</li><li class="correct-answer">B. End-to-end (tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i).</li><li>C. Chá»‰ huáº¥n luyá»‡n User Encoder.</li><li>D. Chá»‰ huáº¥n luyá»‡n News Encoder.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> CÃ¡c mÃ´ hÃ¬nh deep learning hiá»‡n Ä‘áº¡i nhÆ° tháº¿ nÃ y thÆ°á»ng Ä‘Æ°á»£c huáº¥n luyá»‡n end-to-end. Äiá»u nÃ y cÃ³ nghÄ©a lÃ  gradient tá»« hÃ m loss cuá»‘i cÃ¹ng sáº½ Ä‘Æ°á»£c lan truyá»n ngÆ°á»£c (backpropagation) qua toÃ n bá»™ kiáº¿n trÃºc Ä‘á»ƒ cáº­p nháº­t táº¥t cáº£ cÃ¡c tham sá»‘, tá»« cÃ¡c táº§ng embedding cho Ä‘áº¿n cÃ¡c táº§ng GRU, CNN, Attention.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 72: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 17, mÃ´ hÃ¬nh CNN trong báº£ng so sÃ¡nh cÃ³ AUC lÃ  bao nhiÃªu?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">61.13 Â± 0.77</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong báº£ng, táº¡i hÃ ng "CNN" vÃ  cá»™t "AUC", giÃ¡ trá»‹ lÃ  61.13 Â± 0.77.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 73: Dá»±a trÃªn sÆ¡ Ä‘á»“ News Encoder (slide 10), vector `e_v` Ä‘Æ°á»£c táº¡o ra tá»« Ä‘Ã¢u?</p><ul class="options"><li>A. Tá»« Title.</li><li class="correct-answer">B. Tá»« Topic vÃ  Subtopic.</li><li>C. Tá»« Content.</li><li>D. Tá»« Attention.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> SÆ¡ Ä‘á»“ cho tháº¥y `e_v` lÃ  káº¿t quáº£ cá»§a viá»‡c káº¿t há»£p `e_sv` (Subtopic Embedding) vÃ  `e_v` (Topic Embedding), do Ä‘Ã³ nÃ³ Ä‘áº¡i diá»‡n cho thÃ´ng tin vá» topic.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 74: Trong LSTUR-con, táº¡i sao láº¡i cáº§n ghÃ©p ná»‘i (`Concatenation`) biá»ƒu diá»…n dÃ i háº¡n vÃ  ngáº¯n háº¡n thay vÃ¬ chá»‰ cá»™ng chÃºng?</p><ul class="options"><li>A. VÃ¬ cá»™ng vector ráº¥t cháº­m.</li><li class="correct-answer">B. VÃ¬ ghÃ©p ná»‘i cho phÃ©p giá»¯ láº¡i thÃ´ng tin tá»« cáº£ hai nguá»“n má»™t cÃ¡ch riÃªng biá»‡t, Ä‘á»ƒ cÃ¡c táº§ng tiáº¿p theo cÃ³ thá»ƒ há»c cÃ¡ch káº¿t há»£p chÃºng má»™t cÃ¡ch linh hoáº¡t. PhÃ©p cá»™ng sáº½ trá»™n láº«n thÃ´ng tin vÃ  cÃ³ thá»ƒ lÃ m máº¥t mÃ¡t.</li><li>C. VÃ¬ hai vector cÃ³ sá»‘ chiá»u khÃ¡c nhau.</li><li>D. VÃ¬ cá»™ng vector sáº½ cho káº¿t quáº£ sai.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> GhÃ©p ná»‘i lÃ  má»™t cÃ¡ch phá»• biáº¿n vÃ  hiá»‡u quáº£ Ä‘á»ƒ káº¿t há»£p cÃ¡c biá»ƒu diá»…n khÃ¡c nhau trong deep learning. NÃ³ báº£o toÃ n toÃ n bá»™ thÃ´ng tin tá»« má»—i nguá»“n vÃ  Ä‘á»ƒ cho cÃ¡c táº§ng FNN tiáº¿p theo tá»± há»c cÃ¡ch tÆ°Æ¡ng tÃ¡c vÃ  káº¿t há»£p chÃºng.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 75: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 22, má»™t Æ°u Ä‘iá»ƒm cá»§a mÃ´ hÃ¬nh lÃ  nÃ³ biá»ƒu diá»…n Ä‘Æ°á»£c Ä‘á»§ thÃ´ng tin cá»§a cÃ¡i gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">news</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 22, má»¥c "Æ¯u Ä‘iá»ƒm", dÃ²ng Ä‘áº§u tiÃªn ghi: "Biá»ƒu diá»…n Ä‘Æ°á»£c Ä‘á»§ thÃ´ng tin cá»§a news".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 76: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 18, trong Figure 4, STUR lÃ  viáº¿t táº¯t cá»§a gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Short-term user representations</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> ChÃº thÃ­ch cá»§a Figure 4 ghi: "...and short-term user representations (STUR)".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 77: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 18, trong Figure 4, LTUR lÃ  viáº¿t táº¯t cá»§a gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Long-term user representations</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> ChÃº thÃ­ch cá»§a Figure 4 ghi: "...incorporating long-term user representations (LTUR)...".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 78: Theo Figure 6 (slide 19), phÆ°Æ¡ng phÃ¡p nÃ o há»c biá»ƒu diá»…n title kÃ©m hiá»‡u quáº£ nháº¥t?</p><ul class="options"><li class="correct-answer">A. LSTM</li><li>B. LSTM+Att</li><li>C. CNN</li><li>D. CNN+Att</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong cáº£ hai biá»ƒu Ä‘á»“, cá»™t mÃ u vÃ ng (LSTM) lÃ  cá»™t tháº¥p nháº¥t, cho tháº¥y trong thá»­ nghiá»‡m nÃ y, LSTM Ä‘Æ¡n giáº£n hoáº¡t Ä‘á»™ng kÃ©m hiá»‡u quáº£ nháº¥t Ä‘á»ƒ mÃ£ hÃ³a title.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 79: Theo Figure 7 (slide 20), viá»‡c chá»‰ thÃªm thÃ´ng tin Subtopic cÃ³ hiá»‡u quáº£ hÆ¡n chá»‰ thÃªm thÃ´ng tin Topic khÃ´ng?</p><ul class="options"><li>A. CÃ³, luÃ´n hiá»‡u quáº£ hÆ¡n.</li><li class="correct-answer">B. KhÃ´ng, cá»™t +Topic (xanh lÃ¡ nháº¡t) cao hÆ¡n cá»™t +Subtopic (xanh dÆ°Æ¡ng).</li><li>C. ChÃºng hiá»‡u quáº£ nhÆ° nhau.</li><li>D. KhÃ´ng thá»ƒ so sÃ¡nh.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong cáº£ hai biá»ƒu Ä‘á»“, cá»™t +Topic (xanh lÃ¡ nháº¡t) Ä‘á»u cao hÆ¡n má»™t chÃºt so vá»›i cá»™t +Subtopic (xanh dÆ°Æ¡ng), cho tháº¥y thÃ´ng tin vá» topic chÃ­nh cÃ³ giÃ¡ trá»‹ hÆ¡n thÃ´ng tin vá» subtopic khi xÃ©t riÃªng láº».</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 80: Má»¥c tiÃªu "minimize -Î£ log(...)" (slide 15) tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c tá»‘i Ä‘a hÃ³a Ä‘áº¡i lÆ°á»£ng nÃ o?</p><ul class="options"><li>A. Sai sá»‘</li><li class="correct-answer">B. Log-likelihood</li><li>C. Entropy</li><li>D. TÆ°Æ¡ng quan</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Tá»‘i thiá»ƒu hÃ³a negative log-likelihood (`-log(P)`) chÃ­nh lÃ  tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c tá»‘i Ä‘a hÃ³a log-likelihood (`log(P)`), vÃ  cÅ©ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c tá»‘i Ä‘a hÃ³a likelihood (`P`). ÄÃ¢y lÃ  nguyÃªn táº¯c Maximum Likelihood Estimation (MLE).</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 81: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 8, `v` vÃ  `v_b` trong cÃ´ng thá»©c tÃ­nh `a_i` lÃ  cÃ¡c tham sá»‘ Ä‘Æ°á»£c há»c trong quÃ¡ trÃ¬nh nÃ o?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Training</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Slide 8 ghi rÃµ: "Vá»›i v,v_b lÃ  cÃ¡c tham sá»‘ Ä‘Æ°á»£c há»c trong quÃ¡ trÃ¬nh training."</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 82: Trong User Encoder, táº¡i sao láº¡i cáº§n attention thay vÃ¬ chá»‰ láº¥y tráº¡ng thÃ¡i áº©n cuá»‘i cÃ¹ng `h_t` cá»§a GRU?</p><ul class="options"><li>A. VÃ¬ `h_t` khÃ´ng chá»©a Ä‘á»§ thÃ´ng tin.</li><li class="correct-answer">B. VÃ¬ tráº¡ng thÃ¡i áº©n cuá»‘i cÃ¹ng cÃ³ thá»ƒ bá»‹ chi phá»‘i bá»Ÿi cÃ¡c item gáº§n nháº¥t, trong khi attention cho phÃ©p mÃ´ hÃ¬nh nhÃ¬n láº¡i vÃ  tá»•ng há»£p thÃ´ng tin tá»« toÃ n bá»™ lá»‹ch sá»­ má»™t cÃ¡ch cÃ³ trá»ng sá»‘.</li><li>C. VÃ¬ attention nhanh hÆ¡n.</li><li>D. VÃ¬ `h_t` khÃ´ng thá»ƒ Ä‘Æ°á»£c sá»­ dá»¥ng.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> ÄÃ¢y lÃ  má»™t háº¡n cháº¿ cá»§a cÃ¡c mÃ´ hÃ¬nh RNN/GRU/LSTM tiÃªu chuáº©n. Tráº¡ng thÃ¡i áº©n cuá»‘i cÃ¹ng cÃ³ thá»ƒ trá»Ÿ thÃ nh má»™t "nÃºt cá»• chai" thÃ´ng tin. Attention cho phÃ©p mÃ´ hÃ¬nh táº¡o ra má»™t vector ngá»¯ cáº£nh báº±ng cÃ¡ch xem xÃ©t táº¥t cáº£ cÃ¡c tráº¡ng thÃ¡i áº©n trong chuá»—i, giÃºp náº¯m báº¯t thÃ´ng tin tá»‘t hÆ¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 83: MÃ´ hÃ¬nh nÃ y cÃ³ pháº£i lÃ  má»™t há»‡ thá»‘ng hybrid khÃ´ng?</p><ul class="options"><li>A. KhÃ´ng, nÃ³ chá»‰ lÃ  deep learning.</li><li class="correct-answer">B. CÃ³, cÃ³ thá»ƒ xem nÃ³ lÃ  má»™t dáº¡ng lai ghÃ©p giá»¯a cÃ¡c thÃ nh pháº§n khÃ¡c nhau (CNN, GRU, Attention) vÃ  giá»¯a cÃ¡c biá»ƒu diá»…n khÃ¡c nhau (long-term, short-term).</li><li>C. Chá»‰ lÃ  lá»c cá»™ng tÃ¡c.</li><li>D. Chá»‰ lÃ  gá»£i Ã½ dá»±a trÃªn ná»™i dung.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Máº·c dÃ¹ khÃ´ng Ä‘Æ°á»£c gá»i lÃ  "hybrid" theo cÃ¡c Ä‘á»‹nh nghÄ©a á»Ÿ slide trÆ°á»›c, kiáº¿n trÃºc cá»§a LSTUR lÃ  má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh cá»§a viá»‡c káº¿t há»£p nhiá»u ká»¹ thuáº­t vÃ  nguá»“n thÃ´ng tin khÃ¡c nhau Ä‘á»ƒ táº¡o ra má»™t mÃ´ hÃ¬nh máº¡nh máº½ hÆ¡n, hoÃ n toÃ n phÃ¹ há»£p vá»›i tinh tháº§n cá»§a gá»£i Ã½ lai ghÃ©p.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 84: MÃ´ hÃ¬nh sá»­ dá»¥ng User ID Ä‘á»ƒ lÃ m gÃ¬?</p><ul class="options"><li>A. Äá»ƒ hiá»ƒn thá»‹ tÃªn ngÆ°á»i dÃ¹ng.</li><li class="correct-answer">B. Äá»ƒ tra cá»©u (lookup) vector embedding dÃ i háº¡n (`u` hoáº·c `u_s`) riÃªng biá»‡t cho má»—i ngÆ°á»i dÃ¹ng.</li><li>C. Äá»ƒ Ä‘áº¿m sá»‘ lÆ°á»£ng ngÆ°á»i dÃ¹ng.</li><li>D. Äá»ƒ tÃ­nh toÃ¡n hÃ m loss.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong cÃ¡c mÃ´ hÃ¬nh deep learning cho gá»£i Ã½, ID cá»§a user (hoáº·c item) thÆ°á»ng Ä‘Æ°á»£c dÃ¹ng lÃ m "chÃ¬a khÃ³a" Ä‘á»ƒ láº¥y ra vector embedding tÆ°Æ¡ng á»©ng tá»« má»™t ma tráº­n embedding lá»›n. ÄÃ¢y lÃ  cÃ¡ch mÃ´ hÃ¬nh lÆ°u trá»¯ vÃ  há»c cÃ¡c Ä‘áº·c trÆ°ng riÃªng cho tá»«ng thá»±c thá»ƒ.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 85: Theo slide 16, tá»•ng sá»‘ lÆ°á»£t hiá»ƒn thá»‹ tin tá»©c (# of imprs) trong bá»™ dá»¯ liá»‡u MSN lÃ  bao nhiÃªu?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">393,191</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Báº£ng á»Ÿ slide 16, hÃ ng "# of imprs" cÃ³ giÃ¡ trá»‹ lÃ  393,191. "imprs" lÃ  viáº¿t táº¯t cá»§a "impressions".</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 86: Theo Figure 5 (slide 18), "Average" lÃ  phÆ°Æ¡ng phÃ¡p baseline nÃ o?</p><ul class="options"><li>A. Trung bÃ¬nh cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh.</li><li class="correct-answer">B. Má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ¡n giáº£n há»c biá»ƒu diá»…n ngáº¯n háº¡n báº±ng cÃ¡ch láº¥y trung bÃ¬nh cÃ¡c vector embedding cá»§a cÃ¡c tin tá»©c Ä‘Ã£ xem.</li><li>C. Má»™t phÆ°Æ¡ng phÃ¡p khÃ´ng cÃ¡ nhÃ¢n hÃ³a.</li><li>D. Má»™t phÆ°Æ¡ng phÃ¡p dá»±a trÃªn Attention.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> "Average" lÃ  má»™t baseline phá»• biáº¿n Ä‘á»ƒ so sÃ¡nh vá»›i cÃ¡c mÃ´ hÃ¬nh tuáº§n tá»± phá»©c táº¡p hÆ¡n nhÆ° GRU/LSTM/Attention. NÃ³ thá»ƒ hiá»‡n hiá»‡u suáº¥t khi ta chá»‰ tá»•ng há»£p lá»‹ch sá»­ má»™t cÃ¡ch Ä‘Æ¡n giáº£n nháº¥t.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 87: Náº¿u má»™t user chÆ°a Ä‘á»c bÃ i bÃ¡o nÃ o, biá»ƒu diá»…n ngáº¯n háº¡n cá»§a há» sáº½ nhÆ° tháº¿ nÃ o?</p><ul class="options"><li>A. Sáº½ Ä‘Æ°á»£c tÃ­nh tá»« biá»ƒu diá»…n dÃ i háº¡n.</li><li class="correct-answer">B. Sáº½ lÃ  má»™t vector khÃ´ng hoáº·c má»™t vector khá»Ÿi táº¡o ban Ä‘áº§u, vÃ¬ GRU chÆ°a cÃ³ Ä‘áº§u vÃ o nÃ o Ä‘á»ƒ xá»­ lÃ½.</li><li>C. Sáº½ báº±ng biá»ƒu diá»…n cá»§a bÃ i bÃ¡o phá»• biáº¿n nháº¥t.</li><li>D. Há»‡ thá»‘ng sáº½ bÃ¡o lá»—i.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> ÄÃ¢y lÃ  má»™t trÆ°á»ng há»£p "cold-start" cho sá»Ÿ thÃ­ch ngáº¯n háº¡n. Náº¿u khÃ´ng cÃ³ lá»‹ch sá»­ click, chuá»—i GRU sáº½ rá»—ng, vÃ  khÃ´ng thá»ƒ táº¡o ra Ä‘Æ°á»£c biá»ƒu diá»…n `u_t`. Há»‡ thá»‘ng cáº§n má»™t chiáº¿n lÆ°á»£c dá»± phÃ²ng, vÃ­ dá»¥ nhÆ° chá»‰ dá»±a vÃ o biá»ƒu diá»…n dÃ i háº¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 88: So sÃ¡nh News Encoder cá»§a HyperNews (slide 8-11) vÃ  LSTUR (slide 7-10), chÃºng cÃ³ Ä‘iá»ƒm gÃ¬ chung?</p><ul class="options"><li>A. KhÃ´ng cÃ³ Ä‘iá»ƒm chung.</li><li class="correct-answer">B. Cáº£ hai Ä‘á»u sá»­ dá»¥ng cÃ¡c kiáº¿n trÃºc deep learning (CNN, Attention) Ä‘á»ƒ mÃ£ hÃ³a cÃ¡c thuá»™c tÃ­nh cá»§a tin tá»©c (title, category).</li><li>C. Cáº£ hai Ä‘á»u chá»‰ sá»­ dá»¥ng LDA.</li><li>D. Cáº£ hai Ä‘á»u chá»‰ sá»­ dá»¥ng GRU.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Cáº£ hai bÃ i bÃ¡o Ä‘á»u giáº£i quyáº¿t bÃ i toÃ¡n gá»£i Ã½ tin tá»©c vÃ  sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t SOTA (state-of-the-art) trong NLP vÃ  deep learning Ä‘á»ƒ táº¡o ra cÃ¡c biá»ƒu diá»…n tin tá»©c, bao gá»“m viá»‡c sá»­ dá»¥ng CNN cho title vÃ  attention Ä‘á»ƒ tá»•ng há»£p thÃ´ng tin.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 89: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 21, cÃ¡c Ä‘Æ°á»ng cong trÃªn biá»ƒu Ä‘á»“ cÃ³ cÃ¡c "váº¡ch" nhá» thá»ƒ hiá»‡n Ä‘iá»u gÃ¬?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">Äá»™ lá»‡ch chuáº©n / Khoáº£ng tin cáº­y (Standard deviation / Confidence interval)</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> CÃ¡c váº¡ch nÃ y, thÆ°á»ng Ä‘Æ°á»£c gá»i lÃ  error bars, thá»ƒ hiá»‡n sá»± biáº¿n Ä‘á»™ng hoáº·c Ä‘á»™ khÃ´ng cháº¯c cháº¯n cá»§a phÃ©p Ä‘o sau khi cháº¡y thá»±c nghiá»‡m nhiá»u láº§n. ChÃºng cho biáº¿t káº¿t quáº£ cÃ³ á»•n Ä‘á»‹nh hay khÃ´ng.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 90: Táº¡i sao trong hÃ m má»¥c tiÃªu (slide 15), láº¡i cáº§n cÃ³ cáº£ `táº­p positive` vÃ  `táº­p negative`?</p><ul class="options"><li>A. Äá»ƒ lÃ m cho viá»‡c huáº¥n luyá»‡n cháº­m hÆ¡n.</li><li class="correct-answer">B. Äá»ƒ mÃ´ hÃ¬nh há»c cÃ¡ch phÃ¢n biá»‡t, tá»©c lÃ  gÃ¡n Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng cao cho cÃ¡c cáº·p (user, item) dÆ°Æ¡ng vÃ  Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng tháº¥p cho cÃ¡c cáº·p (user, item) Ã¢m.</li><li>C. VÃ¬ dá»¯ liá»‡u luÃ´n Ä‘Æ°á»£c chia lÃ m hai pháº§n.</li><li>D. Äá»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p positive.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Náº¿u chá»‰ há»c tá»« cÃ¡c máº«u dÆ°Æ¡ng, mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c má»™t cÃ¡ch táº§m thÆ°á»ng lÃ  gÃ¡n Ä‘iá»ƒm cao cho táº¥t cáº£ má»i thá»©. Viá»‡c Ä‘Æ°a vÃ o cÃ¡c máº«u Ã¢m buá»™c mÃ´ hÃ¬nh pháº£i há»c cÃ¡ch phÃ¢n biá»‡t vÃ  Ä‘Æ°a ra cÃ¡c dá»± Ä‘oÃ¡n cÃ³ Ã½ nghÄ©a.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 91: Theo Figure 7 (slide 20), Ä‘Ã¢u lÃ  káº¿t há»£p thÃ´ng tin hiá»‡u quáº£ nháº¥t cho News Encoder?</p><ul class="options"><li>A. KhÃ´ng dÃ¹ng topic hay subtopic (None)</li><li>B. Chá»‰ dÃ¹ng Topic (+Topic)</li><li>C. Chá»‰ dÃ¹ng Subtopic (+Subtopic)</li><li class="correct-answer">D. DÃ¹ng cáº£ hai (+Both)</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Trong cáº£ hai biá»ƒu Ä‘á»“ AUC vÃ  nDCG@10, cá»™t "+Both" lÃ  cá»™t cao nháº¥t, cho tháº¥y viá»‡c káº¿t há»£p cáº£ thÃ´ng tin topic vÃ  subtopic mang láº¡i hiá»‡u quáº£ tá»‘t nháº¥t.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 92: Viá»‡c sá»­ dá»¥ng pre-trained word2vec cÃ³ lá»£i Ã­ch gÃ¬?</p><ul class="options"><li>A. LÃ m cho mÃ´ hÃ¬nh phá»©c táº¡p hÆ¡n.</li><li class="correct-answer">B. Táº­n dá»¥ng kiáº¿n thá»©c ngá»¯ nghÄ©a Ä‘Ã£ Ä‘Æ°á»£c há»c tá»« má»™t kho vÄƒn báº£n lá»›n, giÃºp mÃ´ hÃ¬nh cÃ³ Ä‘iá»ƒm khá»Ÿi Ä‘áº§u tá»‘t hÆ¡n vÃ  hoáº¡t Ä‘á»™ng hiá»‡u quáº£ hÆ¡n, Ä‘áº·c biá»‡t khi dá»¯ liá»‡u training cho nhiá»‡m vá»¥ cá»¥ thá»ƒ khÃ´ng lá»›n.</li><li>C. LuÃ´n Ä‘áº£m báº£o káº¿t quáº£ tá»‘t nháº¥t.</li><li>D. Giáº£m sá»‘ lÆ°á»£ng tá»«.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> ÄÃ¢y lÃ  má»™t dáº¡ng cá»§a há»c chuyá»ƒn giao (transfer learning). Thay vÃ¬ há»c embedding tá»« Ä‘áº§u, ta sá»­ dá»¥ng cÃ¡c embedding Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn hÃ ng tá»· tá»«, chá»©a Ä‘á»±ng nhiá»u thÃ´ng tin ngá»¯ nghÄ©a phong phÃº.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 93: MÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ xá»­ lÃ½ cÃ¡c tá»« má»›i (out-of-vocabulary) trong title khÃ´ng?</p><ul class="options"><li>A. CÃ³, má»™t cÃ¡ch hoÃ n háº£o.</li><li class="correct-answer">B. Gáº·p khÃ³ khÄƒn. CÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn word2vec truyá»n thá»‘ng thÆ°á»ng gÃ¡n má»™t vector "unknown" chung cho táº¥t cáº£ cÃ¡c tá»« má»›i, lÃ m máº¥t thÃ´ng tin.</li><li>C. KhÃ´ng thá»ƒ xá»­ lÃ½.</li><li>D. CÃ³, báº±ng cÃ¡ch bá» qua chÃºng.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> ÄÃ¢y lÃ  má»™t háº¡n cháº¿ cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p embedding dá»±a trÃªn tá»« Ä‘iá»ƒn cá»‘ Ä‘á»‹nh nhÆ° word2vec. CÃ¡c phÆ°Æ¡ng phÃ¡p hiá»‡n Ä‘áº¡i hÆ¡n nhÆ° FastText (há»c embedding cho cÃ¡c n-gram kÃ½ tá»±) hoáº·c cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn Transformer (nhÆ° BERT) cÃ³ kháº£ nÄƒng xá»­ lÃ½ tá»« má»›i tá»‘t hÆ¡n.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 94: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 20, mÃ´ hÃ¬nh LSTUR-ini cÃ³ AUC lÃ  bao nhiÃªu?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">0.634 (khoáº£ng)</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> NhÃ¬n vÃ o biá»ƒu Ä‘á»“ (a) AUC cá»§a Figure 7, cá»™t LSTUR-ini, trÆ°á»ng há»£p "+Both", cÃ³ chiá»u cao tÆ°Æ¡ng á»©ng vá»›i khoáº£ng 0.634 trÃªn trá»¥c tung.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 95: (Äiá»n Ä‘Ã¡p Ã¡n) Theo slide 19, mÃ´ hÃ¬nh LSTUR-con cÃ³ AUC lÃ  bao nhiÃªu?</p><div class="explanation"><p><b>âœ… ÄÃ¡p Ã¡n:</b> <span class="fill-in-answer">0.6347 (khoáº£ng)</span></p><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> NhÃ¬n vÃ o biá»ƒu Ä‘á»“ Figure 6(a), cá»™t LSTUR-con, trÆ°á»ng há»£p CNN+Att, cÃ³ chiá»u cao tÆ°Æ¡ng á»©ng vá»›i khoáº£ng 0.634-0.635 trÃªn trá»¥c tung.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 96: Náº¿u bá» Ä‘i GRU vÃ  chá»‰ dÃ¹ng biá»ƒu diá»…n dÃ i háº¡n, mÃ´ hÃ¬nh sáº½ tÆ°Æ¡ng tá»± vá»›i phÆ°Æ¡ng phÃ¡p nÃ o?</p><ul class="options"><li class="correct-answer">A. Matrix Factorization (MF) hoáº·c cÃ¡c biáº¿n thá»ƒ deep learning cá»§a nÃ³.</li><li>B. Gá»£i Ã½ dá»±a trÃªn ná»™i dung.</li><li>C. Gá»£i Ã½ dá»±a trÃªn lÃ¡ng giá»ng.</li><li>D. Gá»£i Ã½ khÃ´ng cÃ¡ nhÃ¢n hÃ³a.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Náº¿u chá»‰ cÃ²n biá»ƒu diá»…n dÃ i háº¡n cá»§a user (`u`) vÃ  biá»ƒu diá»…n cá»§a news (`e`), vÃ  Ä‘iá»ƒm sá»‘ Ä‘Æ°á»£c tÃ­nh báº±ng tÃ­ch vÃ´ hÆ°á»›ng, mÃ´ hÃ¬nh nÃ y sáº½ quay vá» dáº¡ng cÆ¡ báº£n cá»§a Matrix Factorization, nÆ¡i sá»Ÿ thÃ­ch khÃ´ng thay Ä‘á»•i theo thá»i gian.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 97: Sá»± khÃ¡c biá»‡t chÃ­nh giá»¯a GRU vÃ  má»™t máº¡ng FNN thÃ´ng thÆ°á»ng lÃ  gÃ¬?</p><ul class="options"><li>A. GRU cÃ³ nhiá»u táº§ng hÆ¡n.</li><li class="correct-answer">B. GRU cÃ³ "bá»™ nhá»›" thÃ´ng qua cÃ¡c tráº¡ng thÃ¡i áº©n Ä‘Æ°á»£c truyá»n tá»« bÆ°á»›c thá»i gian nÃ y sang bÆ°á»›c thá»i gian khÃ¡c, trong khi FNN xá»­ lÃ½ má»—i Ä‘áº§u vÃ o má»™t cÃ¡ch Ä‘á»™c láº­p.</li><li>C. GRU sá»­ dá»¥ng CNN.</li><li>D. FNN khÃ´ng cÃ³ hÃ m kÃ­ch hoáº¡t.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> Äáº·c Ä‘iá»ƒm cá»‘t lÃµi cá»§a cÃ¡c máº¡ng nÆ¡-ron há»“i quy (RNN) vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³ nhÆ° GRU/LSTM lÃ  kháº£ nÄƒng xá»­ lÃ½ dá»¯ liá»‡u tuáº§n tá»± báº±ng cÃ¡ch duy trÃ¬ má»™t tráº¡ng thÃ¡i ná»™i táº¡i (bá»™ nhá»›) Ä‘á»ƒ náº¯m báº¯t cÃ¡c phá»¥ thuá»™c theo thá»i gian.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 98: MÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ giáº£i quyáº¿t Ä‘Æ°á»£c váº¥n Ä‘á» item cold-start khÃ´ng?</p><ul class="options"><li>A. CÃ³, má»™t cÃ¡ch hoÃ n háº£o.</li><li class="correct-answer">B. CÃ³, vÃ¬ nÃ³ cÃ³ má»™t News Encoder cÃ³ thá»ƒ táº¡o ra biá»ƒu diá»…n cho má»™t bÃ i bÃ¡o má»›i ngay khi cÃ³ title vÃ  topic cá»§a nÃ³, khÃ´ng cáº§n lá»‹ch sá»­ tÆ°Æ¡ng tÃ¡c.</li><li>C. KhÃ´ng, nÃ³ yÃªu cáº§u má»i bÃ i bÃ¡o pháº£i cÃ³ ngÆ°á»i Ä‘á»c trÆ°á»›c.</li><li>D. Chá»‰ giáº£i quyáº¿t Ä‘Æ°á»£c user cold-start.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> VÃ¬ mÃ´ hÃ¬nh há»c cÃ¡ch biá»ƒu diá»…n tin tá»©c tá»« cÃ¡c thuá»™c tÃ­nh ná»™i táº¡i cá»§a nÃ³ (title, topic), nÃ³ cÃ³ thá»ƒ táº¡o ra má»™t vector cho má»™t bÃ i bÃ¡o hoÃ n toÃ n má»›i. Vector nÃ y sau Ä‘Ã³ cÃ³ thá»ƒ Ä‘Æ°á»£c so sÃ¡nh vá»›i cÃ¡c vector cá»§a ngÆ°á»i dÃ¹ng Ä‘á»ƒ Ä‘Æ°a ra gá»£i Ã½. ÄÃ¢y lÃ  má»™t lá»£i tháº¿ cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn ná»™i dung/mÃ´ hÃ¬nh.</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 99: Trong thá»±c táº¿, viá»‡c huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh phá»©c táº¡p nhÆ° LSTUR Ä‘Ã²i há»i tÃ i nguyÃªn gÃ¬?</p><ul class="options"><li>A. Chá»‰ cáº§n má»™t CPU thÃ´ng thÆ°á»ng.</li><li class="correct-answer">B. Má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  tÃ i nguyÃªn tÃ­nh toÃ¡n máº¡nh máº½ (thÆ°á»ng lÃ  GPU).</li><li>C. KhÃ´ng cáº§n dá»¯ liá»‡u.</li><li>D. Chá»‰ cáº§n má»™t Ã­t bá»™ nhá»› RAM.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> CÃ¡c mÃ´ hÃ¬nh deep learning vá»›i hÃ ng triá»‡u tham sá»‘ nhÆ° tháº¿ nÃ y Ä‘Ã²i há»i pháº£i Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn cÃ¡c bá»™ dá»¯ liá»‡u ráº¥t lá»›n Ä‘á»ƒ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c cÃ¡c máº«u cÃ³ Ã½ nghÄ©a vÃ  trÃ¡nh overfitting. QuÃ¡ trÃ¬nh huáº¥n luyá»‡n nÃ y ráº¥t tá»‘n kÃ©m vá» máº·t tÃ­nh toÃ¡n vÃ  thÆ°á»ng Ä‘Æ°á»£c tÄƒng tá»‘c báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c Ä‘Æ¡n vá»‹ xá»­ lÃ½ Ä‘á»“ há»a (GPU).</p></div></div>
        <div class="question-block"><p class="question-text">CÃ¢u 100: BÃ i há»c quan trá»ng nháº¥t tá»« case study nÃ y lÃ  gÃ¬?</p><ul class="options"><li>A. Gá»£i Ã½ tin tá»©c lÃ  má»™t bÃ i toÃ¡n Ä‘Ã£ Ä‘Æ°á»£c giáº£i quyáº¿t.</li><li class="correct-answer">B. Viá»‡c mÃ´ hÃ¬nh hÃ³a cÃ¡c khÃ­a cáº¡nh khÃ¡c nhau cá»§a sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng (dÃ i háº¡n, ngáº¯n háº¡n) vÃ  káº¿t há»£p nhiá»u nguá»“n thÃ´ng tin (title, topic, lá»‹ch sá»­,...) thÃ´ng qua cÃ¡c kiáº¿n trÃºc deep learning phÃ¹ há»£p cÃ³ thá»ƒ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t cá»§a há»‡ gá»£i Ã½.</li><li>C. CÃ¡c mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n luÃ´n tá»‘t hÆ¡n.</li><li>D. Dá»¯ liá»‡u khÃ´ng quan trá»ng báº±ng thuáº­t toÃ¡n.</li></ul><div class="explanation"><p><b>ğŸ’¡ Giáº£i thÃ­ch:</b> BÃ i bÃ¡o nÃ y lÃ  má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh cho xu hÆ°á»›ng hiá»‡n táº¡i trong nghiÃªn cá»©u há»‡ gá»£i Ã½: Ä‘i sÃ¢u vÃ o viá»‡c hiá»ƒu vÃ  mÃ´ hÃ¬nh hÃ³a cÃ¡c hÃ nh vi phá»©c táº¡p cá»§a ngÆ°á»i dÃ¹ng, vÃ  xÃ¢y dá»±ng cÃ¡c kiáº¿n trÃºc máº¡ng nÆ¡-ron chuyÃªn biá»‡t Ä‘á»ƒ náº¯m báº¯t cÃ¡c hÃ nh vi Ä‘Ã³, thay vÃ¬ chá»‰ Ã¡p dá»¥ng má»™t thuáº­t toÃ¡n chung chung.</p></div></div>
    </div>
</body>
</html>