<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>100 Câu Hỏi Trắc Nghiệm - Case Study: Long- and Short-term News Recommendation</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; background-color: #f4f4f4; color: #333; }
        .container { max-width: 900px; margin: auto; background: #fff; padding: 30px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); }
        header { text-align: center; border-bottom: 2px solid #d9534f; margin-bottom: 30px; padding-bottom: 20px; }
        header h1 { color: #d9534f; margin: 0; }
        header p { margin: 5px 0 0; font-style: italic; color: #555; }
        .question-block { margin-bottom: 25px; padding: 20px; border: 1px solid #ddd; border-left: 5px solid #d9534f; border-radius: 5px; background-color: #fdfdfd; }
        .question-text { font-weight: bold; font-size: 1.1em; margin-bottom: 15px; }
        .options { list-style-type: none; padding-left: 0; }
        .options li { margin-bottom: 10px; padding: 8px; border-radius: 4px; }
        .explanation { margin-top: 15px; padding: 15px; background-color: #e9f7ef; border: 1px solid #a3d9b8; border-radius: 5px; }
        .explanation b { color: #1d7b46; }
        .correct-answer { background-color: #dff0d8; border-left: 3px solid #3c763d; }
        .fill-in-answer { font-weight: bold; color: #3c763d; font-size: 1.2em; }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>100 Câu Hỏi Trắc Nghiệm - Case Study: Long- and Short-term News Recommendation</h1>
            <p>Dựa trên nội dung slide "Phân tích một số hệ gợi ý cụ thể" - Viện CNTT&TT - ĐHBK Hà Nội</p>
        </header>

        <!-- CATEGORY: INTRODUCTION & MOTIVATION -->
        <div class="question-block"><p class="question-text">Câu 1: Vấn đề chính mà bài báo "Neural News Recommendation with Long- and Short-term User Representations" giải quyết là gì?</p><ul class="options"><li class="correct-answer">A. Người dùng có cả sở thích dài hạn (ổn định) và ngắn hạn (thay đổi theo thời gian), và cần khai thác cả hai.</li><li>B. Gợi ý tin tức là một bài toán khó vì có quá nhiều tin mới mỗi ngày.</li><li>C. Các mô hình hiện tại không thể xử lý được văn bản.</li><li>D. Cần dự đoán thời gian người dùng đọc tin tức.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 5 nêu rõ: "User có sở thích dài hạn và ngắn hạn. => Cần phải khai thác cả 2." và "paper tiếp cận theo hướng biểu diễn user theo long-term và short-term".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 2: Sở thích ngắn hạn (short-term) của người dùng thể hiện điều gì?</p><ul class="options"><li>A. Các chủ đề người dùng luôn luôn quan tâm.</li><li class="correct-answer">B. Sự quan tâm tạm thời, thường liên quan đến các tin tức vừa đọc gần đây.</li><li>C. Thông tin nhân khẩu học của người dùng.</li><li>D. Sở thích của bạn bè người dùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sở thích ngắn hạn phản ánh sự quan tâm tức thời. Ví dụ, sau khi đọc một tin về "Rami Malek Wins the 2019 Oscar" (slide 5), người dùng có thể quan tâm đến các tin tức liên quan đến giải Oscar ngay sau đó.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 3: Sở thích dài hạn (long-term) của người dùng thể hiện điều gì?</p><ul class="options"><li class="correct-answer">A. Các chủ đề cốt lõi, ổn định mà người dùng quan tâm, ví dụ như một người luôn thích đọc tin về đội bóng rổ "Golden State Warriors".</li><li>B. Sự quan tâm đến một tin tức vừa mới xảy ra.</li><li>C. Thời gian trong ngày mà người dùng hay đọc tin.</li><li>D. Loại thiết bị người dùng sử dụng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sở thích dài hạn là những mối quan tâm tương đối không thay đổi của người dùng qua thời gian, ví dụ như hâm mộ một đội thể thao, quan tâm đến một lĩnh vực chính trị, hoặc theo dõi một mảng công nghệ cụ thể.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 4: Mô hình được đề xuất trong bài báo bao gồm hai thành phần chính nào?</p><ul class="options"><li>A. Title encoder và Topic encoder.</li><li class="correct-answer">B. News encoder và User encoder.</li><li>C. Long-term encoder và Short-term encoder.</li><li>D. CNN và GRU.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6 liệt kê 2 thành phần chính của Model là "1. News encoder" và "2. User encoder".</p></div></div>
        
        <!-- CATEGORY: NEWS ENCODER -->
        <div class="question-block"><p class="question-text">Câu 5: News encoder được dùng để học các thông tin gì từ một bài báo?</p><ul class="options"><li>A. Chỉ Title.</li><li>B. Chỉ Topic.</li><li class="correct-answer">C. Title, topic và subtopic categories.</li><li>D. Toàn bộ nội dung văn bản.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 nêu rõ News encoder "Được dùng để học title, topic, subtopic categories."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 6: News encoder được chia làm 2 phần nào?</p><ul class="options"><li>A. CNN và Attention.</li><li>B. Long-term và Short-term.</li><li class="correct-answer">C. Title encoder và Topic encoder.</li><li>D. Word embedding và Context embedding.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 ghi rõ News encoder "Chia làm 2 phần: ○ Title encoder ○ Topic encoder".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 7: Trong Title encoder, sau khi embedding các từ, mô hình sử dụng mạng nào để trích xuất context?</p><ul class="options"><li>A. GRU</li><li class="correct-answer">B. CNN</li><li>C. FNN (Feed-forward Neural Network)</li><li>D. Autoencoder</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 mô tả: "Sau khi embedding, đưa qua mạng CNN để trích xuất context".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 8: Trong Title encoder, sau khi có các vector context `c_i` từ CNN, kỹ thuật nào được sử dụng để lấy tổng có trọng số và tạo ra vector đại diện cho title `e_t`?</p><ul class="options"><li>A. Max-pooling</li><li>B. Average-pooling</li><li class="correct-answer">C. Attention</li><li>D. GRU</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 trình bày các công thức tính trọng số attention `α_i` và sau đó tính tổng có trọng số `e_t = Σ α_i * c_i`. Điều này cho thấy cơ chế attention được sử dụng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 9: Topic encoder xử lý những thông tin gì?</p><ul class="options"><li>A. Chỉ title.</li><li class="correct-answer">B. Topic và subtopic categories.</li><li>C. Chỉ nội dung.</li><li>D. Chỉ các từ khóa.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 nêu rõ Topic encoder xử lý "topic và subtopic categories".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 10: Vector biểu diễn cuối cùng của một bài báo (`e` trong sơ đồ slide 10) được tạo ra như thế nào?</p><ul class="options"><li>A. Chỉ bằng vector của title (`e_t`).</li><li>B. Chỉ bằng vector của topic (`e_v`).</li><li class="correct-answer">C. Bằng cách ghép nối (concatenate) vector của title (`e_t`) và vector của topic (`e_v`).</li><li>D. Bằng cách lấy trung bình của `e_t` và `e_v`.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ trên slide 10 cho thấy `e_t` và `e_v` được đưa vào một khối có ký hiệu `⊕` (thường có nghĩa là concat hoặc cộng) để tạo ra vector cuối cùng `e`.</p></div></div>
        
        <!-- CATEGORY: USER ENCODER -->
        <div class="question-block"><p class="question-text">Câu 11: User encoder được thiết kế để nắm bắt 2 loại sở thích nào?</p><ul class="options"><li>A. Tích cực và tiêu cực.</li><li>B. Xã hội và cá nhân.</li><li class="correct-answer">C. Ngắn hạn (Short-term) và Dài hạn (Long-term).</li><li>D. Rõ ràng và tiềm ẩn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 11 liệt kê 2 thành phần của User encoder là "• Short-term" và "• Long-term".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 12: Mô hình nào được sử dụng để biểu diễn sở thích ngắn hạn (short-term) của người dùng?</p><ul class="options"><li>A. CNN</li><li class="correct-answer">B. GRU (Gated Recurrent Unit)</li><li>C. FNN</li><li>D. Attention</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 12 ghi rõ: "Sử dụng GRU để biểu diễn short term". GRU là một loại mạng nơ-ron hồi quy, rất phù hợp để xử lý dữ liệu tuần tự như lịch sử click của người dùng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 13: Đầu vào của GRU ở mỗi bước thời gian `t` là gì?</p><ul class="options"><li>A. ID của bài báo thứ `t`.</li><li class="correct-answer">B. Vector biểu diễn `e_t` của bài báo thứ `t` (từ News Encoder) và trạng thái ẩn trước đó `h_{t-1}`.</li><li>C. Chỉ trạng thái ẩn `h_{t-1}`.</li><li>D. Chỉ vector `e_t`.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các công thức trên slide 12 (`r_t = σ(W_r[h_{t-1}, e_t])`,...) cho thấy GRU nhận cả trạng thái ẩn trước đó và đầu vào hiện tại (`e_t`) để tính toán trạng thái ẩn mới.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 14: Biểu diễn sở thích dài hạn (long-term) được học như thế nào?</p><ul class="options"><li>A. Được tính toán từ các thuộc tính nhân khẩu học.</li><li class="correct-answer">B. Được biểu diễn bằng một vector embedding riêng cho mỗi user, được khởi tạo ngẫu nhiên và học trong quá trình train.</li><li>C. Bằng cách lấy trung bình tất cả các bài báo người dùng đã đọc.</li><li>D. Nó là một giá trị cố định.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 13 mô tả biểu diễn Long-term: "Được khởi tạo ngẫu nhiên" và "Được học trong quá trình train." Điều này có nghĩa là mỗi user có một vector `u` riêng, và vector này được cập nhật cùng với các tham số khác của mô hình.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 15: Mô hình "LSTUR-ini" (slide 14a) kết hợp sở thích dài hạn và ngắn hạn như thế nào?</p><ul class="options"><li>A. Bằng cách cộng chúng lại với nhau.</li><li class="correct-answer">B. Biểu diễn dài hạn `u` được dùng để khởi tạo trạng thái ẩn ban đầu của GRU.</li><li>C. Biểu diễn dài hạn được ghép nối với đầu ra của GRU.</li><li>D. Chúng không được kết hợp.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ (a) trên slide 14 cho thấy vector `u_i` từ "User Embedding" (biểu diễn dài hạn) được đưa vào vị trí đầu tiên của chuỗi GRU, đóng vai trò là trạng thái ẩn ban đầu. Tên gọi "-ini" (initialization) cũng gợi ý điều này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 16: Mô hình "LSTUR-con" (slide 14b) kết hợp sở thích dài hạn và ngắn hạn như thế nào?</p><ul class="options"><li>A. Bằng cách cộng chúng lại với nhau.</li><li>B. Biểu diễn dài hạn được dùng để khởi tạo trạng thái ẩn ban đầu của GRU.</li><li class="correct-answer">C. Biểu diễn dài hạn `u_s` được ghép nối (Concatenation) với biểu diễn ngắn hạn `u_t` (đầu ra cuối cùng của GRU).</li><li>D. Chúng không được kết hợp.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ (b) trên slide 14 cho thấy `u_s` (long-term) và `u_t` (short-term, từ GRU) được đưa vào một khối "Concatenation". Tên gọi "-con" (concatenation) cũng gợi ý điều này.</p></div></div>

        <!-- CATEGORY: TRAINING & EXPERIMENTS -->
        <div class="question-block"><p class="question-text">Câu 17: Độ tương đồng giữa user `u` và item ứng viên `c_x` được tính bằng phép toán gì?</p><ul class="options"><li>A. Cosine similarity</li><li class="correct-answer">B. Tích vô hướng (dot-product)</li><li>C. Khoảng cách Euclidean</li><li>D. Tích Hadamard</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 15 đưa ra công thức `s(u, c_x) = u^T * e_x`. Đây là phép toán tích vô hướng. Lý do được nêu là "nhằm giảm thời gian tính toán".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 18: Hàm mục tiêu của mô hình (slide 15) dựa trên kỹ thuật nào?</p><ul class="options"><li>A. Tối đa hóa sai số bình phương.</li><li class="correct-answer">B. Tối thiểu hóa negative log-likelihood (sử dụng negative sampling).</li><li>C. Tối đa hóa độ tương đồng.</li><li>D. Tối thiểu hóa số lượng tham số.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Hàm mục tiêu có dạng `-Σ log(...)`. Đây là dạng chuẩn của việc tối thiểu hóa negative log-likelihood. Mẫu số có dạng `exp(...) + Σ exp(...)` là đặc trưng của hàm softmax, thường được dùng trong các mô hình có negative sampling.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 19: "Negative sampling" được sử dụng trong quá trình huấn luyện có nghĩa là gì?</p><ul class="options"><li>A. Chỉ học từ các ví dụ người dùng không thích.</li><li class="correct-answer">B. Với mỗi ví dụ dương (positive, người dùng đã click), hệ thống lấy mẫu một số K ví dụ âm (negative, người dùng chưa click) để giúp mô hình học cách phân biệt.</li><li>C. Lấy mẫu ngẫu nhiên từ toàn bộ dữ liệu.</li><li>D. Loại bỏ các mẫu âm.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức trên slide 15 có tập `P` (positive) và `K` (negative). Kỹ thuật này giúp việc huấn luyện hiệu quả hơn nhiều so với việc tính toán hàm softmax trên toàn bộ các item.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 20: Kỹ thuật "mask" `u_l = M * W_u[u]` được thêm vào để giải quyết vấn đề gì?</p><ul class="options"><li>A. Để giảm thời gian tính toán.</li><li class="correct-answer">B. Để giải quyết vấn đề không phải user nào cũng có sở thích dài hạn rõ ràng (hoặc là người dùng mới).</li><li>C. Để tăng số lượng tham số.</li><li>D. Để xử lý dữ liệu thiếu.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 15 ghi: "Không phải user nào cũng có long-term => thêm mask". `M` là một biến Bernoulli, có thể bằng 0 với một xác suất `p`. Nếu `M=0`, biểu diễn dài hạn của người dùng đó sẽ bị vô hiệu hóa, và mô hình chỉ dựa vào sở thích ngắn hạn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 21: (Điền đáp án) Bộ dữ liệu được sử dụng trong thực nghiệm của bài báo là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">MSN</span></p><p><b>💡 Giải thích:</b> Slide 16 có tiêu đề "Bộ dữ liệu MSN".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 22: (Điền đáp án) Theo bảng thống kê dữ liệu (slide 16), số lượng mẫu âm (negative samples) nhiều hơn số lượng mẫu dương (positive samples) khoảng bao nhiêu lần?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Khoảng 18-19 lần</span></p><p><b>💡 Giải thích:</b> Số mẫu âm là 9,224,537 và số mẫu dương là 492,185. Tỷ lệ `9,224,537 / 492,185 ≈ 18.74`. Giá trị `NP ratio` trong bảng cũng là 18.74.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 23: Theo bảng kết quả ở slide 17, mô hình nào cho kết quả AUC cao nhất?</p><ul class="options"><li>A. LSTUR-con</li><li class="correct-answer">B. LSTUR-ini</li><li>C. GRU</li><li>D. CNN</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong bảng ở slide 17, hàng "LSTUR-ini" có giá trị AUC cao nhất là 63.56 ± 0.42.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 24: Theo bảng kết quả ở slide 17, mô hình nào là baseline yếu nhất?</p><ul class="options"><li class="correct-answer">A. LibFM</li><li>B. DeepFM</li><li>C. GRU</li><li>D. LSTUR-ini</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong bảng, hàng "LibFM" có các giá trị AUC, MRR, nDCG@5, nDCG@10 đều thấp nhất so với các phương pháp khác.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 25: Theo Figure 4 (slide 18), việc kết hợp cả sở thích dài hạn (LTUR) và ngắn hạn (STUR) mang lại hiệu quả như thế nào so với chỉ dùng một trong hai?</p><ul class="options"><li>A. Không có sự khác biệt.</li><li class="correct-answer">B. Các mô hình kết hợp (LSTUR-con, LSTUR-ini) cho kết quả tốt hơn đáng kể so với chỉ dùng LTUR hoặc STUR.</li><li>C. Chỉ dùng LTUR là tốt nhất.</li><li>D. Chỉ dùng STUR là tốt nhất.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Biểu đồ trong Figure 4 cho thấy các cột màu xanh lá (LSTUR-con, LSTUR-ini) cao hơn hẳn các cột màu vàng (LTUR) và xanh dương (STUR) ở cả hai độ đo AUC và nDCG@10, chứng minh hiệu quả của việc kết hợp.</p></div></div>
        
        <!-- ... More questions from 26 to 100 ... -->
        <div class="question-block"><p class="question-text">Câu 26: Theo Figure 5 (slide 18), phương pháp nào học biểu diễn ngắn hạn hiệu quả nhất?</p><ul class="options"><li>A. Average</li><li>B. Attention</li><li>C. LSTM</li><li class="correct-answer">D. GRU</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong Figure 5, cột màu xanh lá đậm (GRU) là cột cao nhất ở cả AUC và nDCG@10, cho thấy GRU là lựa chọn tốt nhất để mô hình hóa sở thích ngắn hạn trong các phương pháp được so sánh.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 27: Theo Figure 6 (slide 19), việc thêm cơ chế Attention vào các mô hình học biểu diễn title (LSTM+Att, CNN+Att) có tác dụng gì?</p><ul class="options"><li>A. Làm giảm hiệu suất.</li><li class="correct-answer">B. Cải thiện hiệu suất (tăng AUC và nDCG).</li><li>C. Không thay đổi hiệu suất.</li><li>D. Làm cho mô hình chạy chậm hơn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ (a) và (b), cột LSTM+Att cao hơn LSTM, và cột CNN+Att cao hơn CNN. Điều này cho thấy việc thêm Attention để chọn lọc các từ quan trọng trong title là một cải tiến hiệu quả.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 28: Theo Figure 7 (slide 20), việc thêm thông tin Topic và Subtopic vào News Encoder có hiệu quả không?</p><ul class="options"><li class="correct-answer">A. Có, việc thêm cả hai (+Both) cho kết quả tốt nhất.</li><li>B. Không, việc không dùng thông tin này (None) lại tốt hơn.</li><li>C. Chỉ thêm Topic là tốt.</li><li>D. Chỉ thêm Subtopic là tốt.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ, cột màu xanh lá đậm (+Both) là cột cao nhất, cho thấy việc kết hợp thông tin từ cả Topic và Subtopic giúp cải thiện hiệu suất gợi ý.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 29: Biểu đồ ở slide 21 phân tích ảnh hưởng của tham số nào?</p><ul class="options"><li>A. Kích thước embedding.</li><li>B. Số lượng tầng GRU.</li><li class="correct-answer">C. Xác suất mask (Mask probability p).</li><li>D. Tốc độ học.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trục hoành của cả hai biểu đồ trên slide 21 đều được gán nhãn là "Mask probability p".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 30: (Điền đáp án) Theo slide 22, một trong những ưu điểm của mô hình là "Thêm attention weight để...". Hoàn thành câu này.</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">đánh trọng số cho context quan trọng.</span></p><p><b>💡 Giải thích:</b> Slide 22, mục "Ưu điểm", có ghi: "Thêm attention weight để đánh trọng số cho context quan trọng."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 31: (Điền đáp án) Theo slide 22, một trong những nhược điểm của mô hình là chưa khai thác được loại thông tin nào?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Entity (thực thể)</span></p><p><b>💡 Giải thích:</b> Slide 22, mục "Nhược điểm", ghi: "Chưa khai thác được entity".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 32: GRU là một biến thể của mạng nơ-ron nào?</p><ul class="options"><li class="correct-answer">A. RNN (Recurrent Neural Network)</li><li>B. CNN (Convolutional Neural Network)</li><li>C. FNN (Feed-forward Neural Network)</li><li>D. Transformer</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> GRU và LSTM là hai biến thể phổ biến nhất của RNN, được thiết kế để giải quyết vấn đề vanishing/exploding gradient và nắm bắt các phụ thuộc dài hạn tốt hơn RNN truyền thống.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 33: Trong công thức của Title Encoder (slide 8), `C × W[i-M:i+M]` là một phép toán đặc trưng của mạng nào?</p><ul class="options"><li>A. RNN</li><li class="correct-answer">B. CNN</li><li>C. Attention</li><li>D. FNN</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là phép toán tích chập (convolution) một chiều. `W[i-M:i+M]` là một cửa sổ (window) các vector embedding của từ, và `C` là ma trận bộ lọc (filter). Phép toán này là cốt lõi của mạng CNN khi áp dụng cho dữ liệu tuần tự.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 34: Hàm kích hoạt `tanh` được sử dụng ở đâu trong mô hình?</p><ul class="options"><li>A. Ở tầng cuối cùng của mô hình.</li><li class="correct-answer">B. Trong cơ chế attention để tính điểm attention thô (`a_i`).</li><li>C. Để khởi tạo embedding.</li><li>D. Trong hàm loss.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 trình bày công thức `a_i = tanh(v × c_i + v_b)`. Hàm `tanh` được dùng để đưa ra một điểm số trước khi qua hàm softmax để tính trọng số attention.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 35: Việc concat các vector topic và subtopic (slide 9) có ý nghĩa gì?</p><ul class="options"><li>A. Để giảm số chiều.</li><li class="correct-answer">B. Để kết hợp thông tin từ cả hai cấp độ category (chủ đề chính và chủ đề phụ) thành một biểu diễn duy nhất.</li><li>C. Để tính toán attention.</li><li>D. Để so sánh chúng với nhau.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Một bài báo có thể thuộc một topic lớn (ví dụ: Thể thao) và một subtopic nhỏ hơn (ví dụ: Bóng rổ). Việc kết hợp cả hai embedding này giúp mô hình có được một biểu diễn danh mục chi tiết và đầy đủ hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 36: Dựa trên sơ đồ ở slide 14, mô hình LSTUR-ini và LSTUR-con khác nhau ở đâu?</p><ul class="options"><li>A. Ở News Encoder.</li><li class="correct-answer">B. Ở cách biểu diễn dài hạn (user embedding) được tích hợp vào mô hình.</li><li>C. Ở loại RNN được sử dụng.</li><li>D. Ở hàm loss.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Cả hai mô hình đều có cùng cấu trúc News Encoder và GRU. Điểm khác biệt duy nhất được minh họa là LSTUR-ini sử dụng user embedding để khởi tạo GRU, trong khi LSTUR-con ghép nối user embedding với đầu ra của GRU.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 37: (Điền đáp án) Theo slide 15, `P` và `K` trong hàm mục tiêu lần lượt là tập positive và tập gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Negative</span></p><p><b>💡 Giải thích:</b> Slide 15 định nghĩa "○ P : tập positive" và "○ K : tập negative".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 38: Theo bảng kết quả ở slide 17, mô hình LSTUR-ini và LSTUR-con, mô hình nào cho kết quả tốt hơn một cách nhất quán?</p><ul class="options"><li>A. LSTUR-con</li><li class="correct-answer">B. LSTUR-ini</li><li>C. Chúng tương đương nhau.</li><li>D. Tùy thuộc vào độ đo.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> So sánh hai hàng cuối cùng trong bảng, LSTUR-ini (63.56, 30.98, 33.45, 41.37) có tất cả các chỉ số đều cao hơn một chút so với LSTUR-con (63.47, 30.94, 33.43, 41.34). Điều này cho thấy chiến lược khởi tạo GRU bằng biểu diễn dài hạn hiệu quả hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 39: Theo Figure 6 (slide 19), mô hình nào học biểu diễn title tốt nhất?</p><ul class="options"><li>A. LSTM</li><li>B. LSTM+Att</li><li>C. CNN</li><li class="correct-answer">D. CNN+Att</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ (a) và (b), cột màu xanh lá đậm (CNN+Att) là cột cao nhất, cho thấy CNN kết hợp với Attention là phương pháp hiệu quả nhất để mã hóa tiêu đề trong các lựa chọn được so sánh.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 40: (Điền đáp án) Theo slide 22, một nhược điểm của mô hình là việc học cùng lúc cho item và user có thể dẫn đến điều gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Train lâu</span></p><p><b>💡 Giải thích:</b> Slide 22, mục "Nhược điểm", ghi: "Việc học cùng 1 lúc cho việc biểu diễn item và user có thể dẫn đến train lâu."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 41: "Cat lớn và Cat nhỏ sẽ có phần nào đó liên quan đến nhau" (slide 22) là một nhận xét về mối quan hệ giữa cái gì?</p><ul class="options"><li>A. Title và Content.</li><li class="correct-answer">B. Topic và Subtopic.</li><li>C. User và Item.</li><li>D. Long-term và Short-term.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> "Cat" là viết tắt của Category. "Cat lớn" (ví dụ: Thể thao) và "Cat nhỏ" (ví dụ: Bóng đá) rõ ràng có liên quan. Nhược điểm được nêu là việc học biểu diễn riêng rẽ cho chúng có thể làm mất đi thông tin về mối quan hệ phân cấp này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 42: Theo slide 21, khi xác suất mask `p` tăng lên, hiệu suất của mô hình có xu hướng thay đổi như thế nào?</p><ul class="options"><li>A. Luôn tăng.</li><li>B. Luôn giảm.</li><li class="correct-answer">C. Tăng lên đến một điểm tối ưu rồi sau đó giảm xuống.</li><li>D. Không thay đổi.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các đường cong trong cả hai biểu đồ trên slide 21 đều cho thấy hiệu suất (AUC, MRR, nDCG) tăng khi `p` đi từ 0.0 đến khoảng 0.5-0.6, sau đó bắt đầu giảm dần khi `p` tiến đến 0.9. Điều này cho thấy có một mức độ "che" biểu diễn dài hạn tối ưu.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 43: (Điền đáp án) Theo slide 16, NP ratio là viết tắt của cái gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Tỷ lệ mẫu Âm/Dương (Negative/Positive ratio)</span></p><p><b>💡 Giải thích:</b> NP ratio là tỷ lệ giữa số lượng mẫu âm (# of negative samples) và số lượng mẫu dương (# of positive samples).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 44: Mô hình LSTUR-ini và LSTUR-con là các biến thể của mô hình nào?</p><ul class="options"><li>A. HyperNews</li><li class="correct-answer">B. LSTUR (Long- and Short-term User Representation)</li><li>C. LibFM</li><li>D. DKN</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tên gọi LSTUR-ini và LSTUR-con cho thấy chúng là hai phiên bản/chiến lược cụ thể của một mô hình chung gọi là LSTUR, được đề xuất trong bài báo này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 45: Nếu một user là người dùng mới, biểu diễn dài hạn `u` của họ sẽ như thế nào?</p><ul class="options"><li>A. Sẽ được tính từ người dùng tương tự.</li><li class="correct-answer">B. Sẽ chỉ là một vector được khởi tạo ngẫu nhiên và chưa được huấn luyện.</li><li>C. Sẽ bằng vector không.</li><li>D. Hệ thống không thể xử lý.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Vì biểu diễn dài hạn được học từ lịch sử, người dùng mới sẽ chưa có dữ liệu để học. Họ sẽ bắt đầu với một vector embedding ngẫu nhiên. Kỹ thuật mask (slide 15) cũng có thể giúp xử lý trường hợp này bằng cách vô hiệu hóa ảnh hưởng của biểu diễn dài hạn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 46: (Điền đáp án) Theo slide 16, NP ratio là viết tắt của cái gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Tỷ lệ mẫu Âm/Dương (Negative/Positive ratio)</span></p><p><b>💡 Giải thích:</b> NP ratio là tỷ lệ giữa số lượng mẫu âm (# of negative samples) và số lượng mẫu dương (# of positive samples).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 47: Mô hình LSTUR-ini và LSTUR-con là các biến thể của mô hình nào?</p><ul class="options"><li>A. HyperNews</li><li class="correct-answer">B. LSTUR (Long- and Short-term User Representation)</li><li>C. LibFM</li><li>D. DKN</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tên gọi LSTUR-ini và LSTUR-con cho thấy chúng là hai phiên bản/chiến lược cụ thể của một mô hình chung gọi là LSTUR, được đề xuất trong bài báo này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 48: Nếu một user là người dùng mới, biểu diễn dài hạn `u` của họ sẽ như thế nào?</p><ul class="options"><li>A. Sẽ được tính từ người dùng tương tự.</li><li class="correct-answer">B. Sẽ chỉ là một vector được khởi tạo ngẫu nhiên và chưa được huấn luyện.</li><li>C. Sẽ bằng vector không.</li><li>D. Hệ thống không thể xử lý.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Vì biểu diễn dài hạn được học từ lịch sử, người dùng mới sẽ chưa có dữ liệu để học. Họ sẽ bắt đầu với một vector embedding ngẫu nhiên. Kỹ thuật mask (slide 15) cũng có thể giúp xử lý trường hợp này bằng cách vô hiệu hóa ảnh hưởng của biểu diễn dài hạn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 49: Tại sao mô hình lại sử dụng cả LDA và Doc2vec để biểu diễn nội dung?</p><ul class="options"><li>A. Vì một trong hai có thể bị lỗi.</li><li class="correct-answer">B. Để tận dụng điểm mạnh của cả hai phương pháp; LDA nắm bắt tốt các chủ đề (topic), trong khi Doc2vec nắm bắt tốt ngữ nghĩa ở cấp độ câu/đoạn văn.</li><li>C. Vì tác giả không quyết định được nên dùng cái nào.</li><li>D. Để tăng số chiều của vector.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là một kỹ thuật ensemble ở mức đặc trưng. Bằng cách kết hợp các biểu diễn từ các mô hình khác nhau, mỗi mô hình có thể đóng góp một loại thông tin khác nhau, giúp cho biểu diễn tổng hợp trở nên phong phú và mạnh mẽ hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 50: Theo kết quả ở slide 17, mô hình Wide&Deep có hiệu suất như thế nào so với LSTUR-con?</p><ul class="options"><li>A. Tốt hơn</li><li class="correct-answer">B. Kém hơn đáng kể</li><li>C. Tương đương</li><li>D. Không thể so sánh</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trên tất cả các độ đo (AUC, MRR, nDCG), các giá trị của LSTUR-con (ví dụ AUC 63.47) đều cao hơn đáng kể so với Wide&Deep (ví dụ AUC 58.07).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 51: (Điền đáp án) Theo slide 22, một nhược điểm của mô hình là việc học biểu diễn riêng rẽ cho "cat to" và "cat nhỏ" có thể làm "sai lệch" điều gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Thông tin</span></p><p><b>💡 Giải thích:</b> Slide 22 ghi: "...học biểu diễn riêng rẽ cat to và cat nhỏ có thể làm “sai lệch” thông tin".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 52: Trong Title encoder (slide 8), hàm `ReLU` được áp dụng sau phép toán nào?</p><ul class="options"><li class="correct-answer">A. Tích chập (Convolution)</li><li>B. Embedding từ</li><li>C. Attention</li><li>D. Lấy tổng có trọng số</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức `c_i = ReLU(C × W[...] + b)` cho thấy hàm ReLU được áp dụng ngay sau phép tích chập để tạo ra các feature map.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 53: Tại sao bài báo lại chọn GRU thay vì LSTM cho việc học sở thích ngắn hạn (theo Figure 5)?</p><ul class="options"><li>A. Vì GRU phức tạp hơn LSTM.</li><li class="correct-answer">B. Vì trong thực nghiệm của họ, GRU cho hiệu suất cao hơn LSTM.</li><li>C. Vì GRU không bị vanishing gradient.</li><li>D. Vì LSTM không thể xử lý dữ liệu tin tức.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Biểu đồ ở Figure 5 (slide 18) so sánh trực tiếp các phương pháp học biểu diễn ngắn hạn. Kết quả cho thấy cột GRU cao hơn cột LSTM, chứng tỏ trong bối cảnh này, GRU hoạt động hiệu quả hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 54: Nếu một bài báo không có subtopic, Topic encoder sẽ xử lý như thế nào?</p><ul class="options"><li>A. Báo lỗi.</li><li class="correct-answer">B. Vector embedding của subtopic có thể là một vector không hoặc một vector "unknown" đặc biệt.</li><li>C. Lấy subtopic của bài báo trước đó.</li><li>D. Sử dụng một subtopic ngẫu nhiên.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong các hệ thống thực tế, các giá trị bị thiếu (missing categorical features) thường được xử lý bằng cách gán cho chúng một embedding riêng (ví dụ: một vector học được cho giá trị "NULL" hoặc "UNKNOWN").</p></div></div>
        <div class="question-block"><p class="question-text">Câu 55: (Điền đáp án) Trong hàm mục tiêu ở slide 15, `cx` là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Item ứng viên (candidate item)</span></p><p><b>💡 Giải thích:</b> `s(u, cx)` là điểm tương đồng giữa user `u` và item ứng viên `cx`. Trong công thức log-likelihood, `c_i^p` là item dương (positive) và `c_{i,k}^n` là các item âm (negative).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 56: Kỹ thuật Word2vec được đề cập trong slide 8 được dùng để làm gì?</p><ul class="options"><li>A. Để huấn luyện toàn bộ mô hình.</li><li class="correct-answer">B. Để khởi tạo các vector embedding cho các từ trong title (pre-train).</li><li>C. Để phân loại topic.</li><li>D. Để tính toán hàm loss.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 ghi: "Title được embedding (dùng pre-train word2vec,...)". Sử dụng các embedding đã được huấn luyện trước (pre-trained) là một kỹ thuật phổ biến để khởi tạo tầng embedding, giúp mô hình hội tụ nhanh hơn và tốt hơn, đặc biệt khi dữ liệu huấn luyện không quá lớn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 57: Theo slide 17, AUC là viết tắt của thuật ngữ gì?</p><ul class="options"><li>A. Average User Clicks</li><li class="correct-answer">B. Area Under the (ROC) Curve</li><li>C. Attention-based User-CNN</li><li>D. A-User-Centric model</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> AUC là một độ đo tiêu chuẩn trong học máy, viết tắt của Area Under the Receiver Operating Characteristic Curve. Nó đo khả năng của một mô hình phân loại trong việc xếp hạng các mẫu dương cao hơn các mẫu âm.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 58: Theo slide 17, MRR là viết tắt của thuật ngữ gì?</p><ul class="options"><li>A. Mean Rating Rank</li><li class="correct-answer">B. Mean Reciprocal Rank</li><li>C. Most Relevant Rank</li><li>D. Model Recommendation Rate</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> MRR là viết tắt của Mean Reciprocal Rank. Nó đánh giá hiệu suất xếp hạng bằng cách lấy giá trị trung bình của nghịch đảo thứ hạng của câu trả lời đúng đầu tiên.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 59: Theo slide 17, nDCG là viết tắt của thuật ngữ gì?</p><ul class="options"><li>A. New Document-Centric Gain</li><li class="correct-answer">B. Normalized Discounted Cumulative Gain</li><li>C. Neural Dynamic Contextual Graph</li><li>D. Negative Document Correlation Grade</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> nDCG là viết tắt của Normalized Discounted Cumulative Gain, một độ đo xếp hạng phổ biến có tính đến cả thứ hạng và mức độ liên quan của các item được gợi ý.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 60: Kết quả thực nghiệm ở slide 17 cho thấy mô hình LSTUR-ini và LSTUR-con vượt trội hơn các baseline trên những độ đo nào?</p><ul class="options"><li>A. Chỉ AUC</li><li>B. Chỉ MRR</li><li>C. Chỉ nDCG</li><li class="correct-answer">D. Tất cả các độ đo (AUC, MRR, nDCG@5, nDCG@10)</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Nhìn vào hai hàng cuối của bảng, các giá trị của LSTUR-con và LSTUR-ini đều cao hơn tất cả các giá trị tương ứng của các mô hình baseline khác trên tất cả các cột độ đo.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 61: Trong User Encoder (slide 12), phép toán `rt ⊙ ht-1` trong công thức tính `h_tilde_t` có ý nghĩa gì?</p><ul class="options"><li>A. Kết hợp trạng thái ẩn trước đó với cổng reset.</li><li class="correct-answer">B. Áp dụng cổng reset `rt` lên trạng thái ẩn trước đó `ht-1` để quyết định xem bao nhiêu thông tin từ quá khứ cần được "quên" đi.</li><li>C. Cập nhật trạng thái ẩn.</li><li>D. Tính toán cổng cập nhật.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> `rt` là cổng reset (reset gate) trong GRU. Phép nhân element-wise này cho phép mô hình quyết định một cách linh hoạt phần nào của trạng thái quá khứ `ht-1` là không còn liên quan và cần được bỏ qua khi tính toán trạng thái ứng viên mới `h_tilde_t`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 62: Trong User Encoder (slide 12), phép toán `zt ⊙ ht + (1-zt) ⊙ h_tilde_t` có ý nghĩa gì?</p><ul class="options"><li>A. Tính toán cổng reset.</li><li class="correct-answer">B. Cập nhật trạng thái ẩn `ht` bằng cách kết hợp có trọng số giữa trạng thái ẩn cũ `ht` và trạng thái ứng viên mới `h_tilde_t`, được điều khiển bởi cổng cập nhật `zt`.</li><li>C. Tính toán điểm attention.</li><li>D. Khởi tạo trạng thái ẩn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> `zt` là cổng cập nhật (update gate). Công thức này mô tả cách GRU quyết định xem nên giữ lại bao nhiêu phần trăm thông tin từ trạng thái cũ `ht` (với trọng số `zt`) và cập nhật bao nhiêu phần trăm thông tin từ trạng thái ứng viên mới `h_tilde_t` (với trọng số `1-zt`).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 63: Tại sao lại cần có "Padding" trong sơ đồ News Encoder (slide 10)?</p><ul class="options"><li>A. Để làm cho title dài hơn.</li><li class="correct-answer">B. Để đảm bảo các phép toán tích chập của CNN có thể được áp dụng ở các vị trí đầu và cuối của chuỗi từ.</li><li>C. Để tăng số lượng từ.</li><li>D. Để cải thiện embedding.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Khi áp dụng một bộ lọc (filter) có kích thước lớn hơn 1, ví dụ kích thước 3, lên một chuỗi, các từ ở biên (từ đầu tiên và cuối cùng) sẽ không thể nằm ở trung tâm của cửa sổ filter. Padding thêm các vector không (hoặc vector đặc biệt) vào hai đầu chuỗi để giải quyết vấn đề này, đảm bảo đầu ra của CNN có cùng độ dài với đầu vào.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 64: (Điền đáp án) Theo slide 16, bộ dữ liệu MSN có bao nhiêu người dùng (# of users)?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">25,000</span></p><p><b>💡 Giải thích:</b> Bảng ở slide 16, hàng "# of users" có giá trị là 25,000.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 65: (Điền đáp án) Theo slide 16, số lượng mẫu dương (# of positive samples) trong bộ dữ liệu MSN là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">492,185</span></p><p><b>💡 Giải thích:</b> Bảng ở slide 16, hàng "# of positive samples" có giá trị là 492,185.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 66: DKN trong bảng so sánh (slide 17) có thể là viết tắt của thuật ngữ gì?</p><ul class="options"><li class="correct-answer">A. Deep Knowledge-Aware Network</li><li>B. Dynamic K-Nearest Neighbors</li><li>C. Deep Kernel Network</li><li>D. Double-task Knowledge Network</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> DKN là một mô hình nổi tiếng khác cho gợi ý tin tức, viết tắt của Deep Knowledge-Aware Network. Nó kết hợp thông tin từ đồ thị tri thức để làm giàu cho biểu diễn của tin tức.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 67: Việc mô hình LSTUR-ini hoạt động tốt hơn LSTUR-con (slide 17, 18) cho thấy điều gì?</p><ul class="options"><li>A. Phép ghép nối luôn tệ hơn.</li><li class="correct-answer">B. Việc sử dụng sở thích dài hạn để định hướng (khởi tạo) cho quá trình xử lý sở thích ngắn hạn có thể hiệu quả hơn là chỉ kết hợp chúng một cách đơn giản ở cuối.</li><li>C. Sở thích dài hạn không quan trọng.</li><li>D. Sở thích ngắn hạn không quan trọng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Kết quả này gợi ý rằng sở thích dài hạn có thể cung cấp một "ngữ cảnh" hoặc "prior" hữu ích cho việc diễn giải các hành vi ngắn hạn. Việc khởi tạo trạng thái của GRU bằng vector sở thích dài hạn là một cách để truyền tải ngữ cảnh này vào mô hình tuần tự.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 68: Nếu xác suất mask `p=1`, kỹ thuật mask (slide 15) sẽ có tác dụng gì?</p><ul class="options"><li>A. Giữ nguyên biểu diễn dài hạn.</li><li class="correct-answer">B. Luôn luôn vô hiệu hóa (mask) biểu diễn dài hạn.</li><li>C. Vô hiệu hóa biểu diễn ngắn hạn.</li><li>D. Không có tác dụng gì.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Biến Bernoulli `M` được lấy mẫu từ `B(1, 1-p)`. Nếu `p=1`, thì `1-p=0`. Xác suất để `M=1` là `1-p=0`. Do đó, `M` sẽ luôn bằng 0, và `u_l = M * W_u[u]` sẽ luôn bằng 0, tức là biểu diễn dài hạn luôn bị vô hiệu hóa.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 69: Nếu xác suất mask `p=0`, kỹ thuật mask (slide 15) sẽ có tác dụng gì?</p><ul class="options"><li class="correct-answer">A. Luôn luôn giữ nguyên biểu diễn dài hạn.</li><li>B. Luôn luôn vô hiệu hóa biểu diễn dài hạn.</li><li>C. Vô hiệu hóa biểu diễn ngắn hạn.</li><li>D. Không có tác dụng gì.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Nếu `p=0`, thì `1-p=1`. Xác suất để `M=1` là `1-p=1`. Do đó, `M` sẽ luôn bằng 1, và `u_l` sẽ luôn bằng `W_u[u]`, tức là biểu diễn dài hạn không bao giờ bị mask.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 70: Tại sao biểu diễn user lại cần được cập nhật theo thời gian?</p><ul class="options"><li>A. Vì vector sẽ bị cũ.</li><li class="correct-answer">B. Vì sở thích của người dùng, đặc biệt là sở thích ngắn hạn, thay đổi liên tục khi họ tương tác với các tin tức mới.</li><li>C. Vì News Encoder thay đổi.</li><li>D. Vì hệ thống yêu cầu vậy.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Mô hình sử dụng GRU chính là để nắm bắt sự thay đổi này. Khi có một lượt click mới (`e_t`), GRU sẽ cập nhật trạng thái ẩn (`h_t`) để phản ánh sự thay đổi trong sở thích ngắn hạn của người dùng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 71: Toàn bộ mô hình LSTUR được huấn luyện theo phương pháp nào?</p><ul class="options"><li>A. Từng phần riêng biệt.</li><li class="correct-answer">B. End-to-end (từ đầu đến cuối).</li><li>C. Chỉ huấn luyện User Encoder.</li><li>D. Chỉ huấn luyện News Encoder.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các mô hình deep learning hiện đại như thế này thường được huấn luyện end-to-end. Điều này có nghĩa là gradient từ hàm loss cuối cùng sẽ được lan truyền ngược (backpropagation) qua toàn bộ kiến trúc để cập nhật tất cả các tham số, từ các tầng embedding cho đến các tầng GRU, CNN, Attention.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 72: (Điền đáp án) Theo slide 17, mô hình CNN trong bảng so sánh có AUC là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">61.13 ± 0.77</span></p><p><b>💡 Giải thích:</b> Trong bảng, tại hàng "CNN" và cột "AUC", giá trị là 61.13 ± 0.77.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 73: Dựa trên sơ đồ News Encoder (slide 10), vector `e_v` được tạo ra từ đâu?</p><ul class="options"><li>A. Từ Title.</li><li class="correct-answer">B. Từ Topic và Subtopic.</li><li>C. Từ Content.</li><li>D. Từ Attention.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ cho thấy `e_v` là kết quả của việc kết hợp `e_sv` (Subtopic Embedding) và `e_v` (Topic Embedding), do đó nó đại diện cho thông tin về topic.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 74: Trong LSTUR-con, tại sao lại cần ghép nối (`Concatenation`) biểu diễn dài hạn và ngắn hạn thay vì chỉ cộng chúng?</p><ul class="options"><li>A. Vì cộng vector rất chậm.</li><li class="correct-answer">B. Vì ghép nối cho phép giữ lại thông tin từ cả hai nguồn một cách riêng biệt, để các tầng tiếp theo có thể học cách kết hợp chúng một cách linh hoạt. Phép cộng sẽ trộn lẫn thông tin và có thể làm mất mát.</li><li>C. Vì hai vector có số chiều khác nhau.</li><li>D. Vì cộng vector sẽ cho kết quả sai.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Ghép nối là một cách phổ biến và hiệu quả để kết hợp các biểu diễn khác nhau trong deep learning. Nó bảo toàn toàn bộ thông tin từ mỗi nguồn và để cho các tầng FNN tiếp theo tự học cách tương tác và kết hợp chúng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 75: (Điền đáp án) Theo slide 22, một ưu điểm của mô hình là nó biểu diễn được đủ thông tin của cái gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">news</span></p><p><b>💡 Giải thích:</b> Slide 22, mục "Ưu điểm", dòng đầu tiên ghi: "Biểu diễn được đủ thông tin của news".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 76: (Điền đáp án) Theo slide 18, trong Figure 4, STUR là viết tắt của gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Short-term user representations</span></p><p><b>💡 Giải thích:</b> Chú thích của Figure 4 ghi: "...and short-term user representations (STUR)".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 77: (Điền đáp án) Theo slide 18, trong Figure 4, LTUR là viết tắt của gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Long-term user representations</span></p><p><b>💡 Giải thích:</b> Chú thích của Figure 4 ghi: "...incorporating long-term user representations (LTUR)...".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 78: Theo Figure 6 (slide 19), phương pháp nào học biểu diễn title kém hiệu quả nhất?</p><ul class="options"><li class="correct-answer">A. LSTM</li><li>B. LSTM+Att</li><li>C. CNN</li><li>D. CNN+Att</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ, cột màu vàng (LSTM) là cột thấp nhất, cho thấy trong thử nghiệm này, LSTM đơn giản hoạt động kém hiệu quả nhất để mã hóa title.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 79: Theo Figure 7 (slide 20), việc chỉ thêm thông tin Subtopic có hiệu quả hơn chỉ thêm thông tin Topic không?</p><ul class="options"><li>A. Có, luôn hiệu quả hơn.</li><li class="correct-answer">B. Không, cột +Topic (xanh lá nhạt) cao hơn cột +Subtopic (xanh dương).</li><li>C. Chúng hiệu quả như nhau.</li><li>D. Không thể so sánh.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ, cột +Topic (xanh lá nhạt) đều cao hơn một chút so với cột +Subtopic (xanh dương), cho thấy thông tin về topic chính có giá trị hơn thông tin về subtopic khi xét riêng lẻ.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 80: Mục tiêu "minimize -Σ log(...)" (slide 15) tương đương với việc tối đa hóa đại lượng nào?</p><ul class="options"><li>A. Sai số</li><li class="correct-answer">B. Log-likelihood</li><li>C. Entropy</li><li>D. Tương quan</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tối thiểu hóa negative log-likelihood (`-log(P)`) chính là tương đương với việc tối đa hóa log-likelihood (`log(P)`), và cũng tương đương với việc tối đa hóa likelihood (`P`). Đây là nguyên tắc Maximum Likelihood Estimation (MLE).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 81: (Điền đáp án) Theo slide 8, `v` và `v_b` trong công thức tính `a_i` là các tham số được học trong quá trình nào?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Training</span></p><p><b>💡 Giải thích:</b> Slide 8 ghi rõ: "Với v,v_b là các tham số được học trong quá trình training."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 82: Trong User Encoder, tại sao lại cần attention thay vì chỉ lấy trạng thái ẩn cuối cùng `h_t` của GRU?</p><ul class="options"><li>A. Vì `h_t` không chứa đủ thông tin.</li><li class="correct-answer">B. Vì trạng thái ẩn cuối cùng có thể bị chi phối bởi các item gần nhất, trong khi attention cho phép mô hình nhìn lại và tổng hợp thông tin từ toàn bộ lịch sử một cách có trọng số.</li><li>C. Vì attention nhanh hơn.</li><li>D. Vì `h_t` không thể được sử dụng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là một hạn chế của các mô hình RNN/GRU/LSTM tiêu chuẩn. Trạng thái ẩn cuối cùng có thể trở thành một "nút cổ chai" thông tin. Attention cho phép mô hình tạo ra một vector ngữ cảnh bằng cách xem xét tất cả các trạng thái ẩn trong chuỗi, giúp nắm bắt thông tin tốt hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 83: Mô hình này có phải là một hệ thống hybrid không?</p><ul class="options"><li>A. Không, nó chỉ là deep learning.</li><li class="correct-answer">B. Có, có thể xem nó là một dạng lai ghép giữa các thành phần khác nhau (CNN, GRU, Attention) và giữa các biểu diễn khác nhau (long-term, short-term).</li><li>C. Chỉ là lọc cộng tác.</li><li>D. Chỉ là gợi ý dựa trên nội dung.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Mặc dù không được gọi là "hybrid" theo các định nghĩa ở slide trước, kiến trúc của LSTUR là một ví dụ điển hình của việc kết hợp nhiều kỹ thuật và nguồn thông tin khác nhau để tạo ra một mô hình mạnh mẽ hơn, hoàn toàn phù hợp với tinh thần của gợi ý lai ghép.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 84: Mô hình sử dụng User ID để làm gì?</p><ul class="options"><li>A. Để hiển thị tên người dùng.</li><li class="correct-answer">B. Để tra cứu (lookup) vector embedding dài hạn (`u` hoặc `u_s`) riêng biệt cho mỗi người dùng.</li><li>C. Để đếm số lượng người dùng.</li><li>D. Để tính toán hàm loss.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong các mô hình deep learning cho gợi ý, ID của user (hoặc item) thường được dùng làm "chìa khóa" để lấy ra vector embedding tương ứng từ một ma trận embedding lớn. Đây là cách mô hình lưu trữ và học các đặc trưng riêng cho từng thực thể.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 85: Theo slide 16, tổng số lượt hiển thị tin tức (# of imprs) trong bộ dữ liệu MSN là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">393,191</span></p><p><b>💡 Giải thích:</b> Bảng ở slide 16, hàng "# of imprs" có giá trị là 393,191. "imprs" là viết tắt của "impressions".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 86: Theo Figure 5 (slide 18), "Average" là phương pháp baseline nào?</p><ul class="options"><li>A. Trung bình của tất cả các mô hình.</li><li class="correct-answer">B. Một phương pháp đơn giản học biểu diễn ngắn hạn bằng cách lấy trung bình các vector embedding của các tin tức đã xem.</li><li>C. Một phương pháp không cá nhân hóa.</li><li>D. Một phương pháp dựa trên Attention.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> "Average" là một baseline phổ biến để so sánh với các mô hình tuần tự phức tạp hơn như GRU/LSTM/Attention. Nó thể hiện hiệu suất khi ta chỉ tổng hợp lịch sử một cách đơn giản nhất.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 87: Nếu một user chưa đọc bài báo nào, biểu diễn ngắn hạn của họ sẽ như thế nào?</p><ul class="options"><li>A. Sẽ được tính từ biểu diễn dài hạn.</li><li class="correct-answer">B. Sẽ là một vector không hoặc một vector khởi tạo ban đầu, vì GRU chưa có đầu vào nào để xử lý.</li><li>C. Sẽ bằng biểu diễn của bài báo phổ biến nhất.</li><li>D. Hệ thống sẽ báo lỗi.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là một trường hợp "cold-start" cho sở thích ngắn hạn. Nếu không có lịch sử click, chuỗi GRU sẽ rỗng, và không thể tạo ra được biểu diễn `u_t`. Hệ thống cần một chiến lược dự phòng, ví dụ như chỉ dựa vào biểu diễn dài hạn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 88: So sánh News Encoder của HyperNews (slide 8-11) và LSTUR (slide 7-10), chúng có điểm gì chung?</p><ul class="options"><li>A. Không có điểm chung.</li><li class="correct-answer">B. Cả hai đều sử dụng các kiến trúc deep learning (CNN, Attention) để mã hóa các thuộc tính của tin tức (title, category).</li><li>C. Cả hai đều chỉ sử dụng LDA.</li><li>D. Cả hai đều chỉ sử dụng GRU.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Cả hai bài báo đều giải quyết bài toán gợi ý tin tức và sử dụng các kỹ thuật SOTA (state-of-the-art) trong NLP và deep learning để tạo ra các biểu diễn tin tức, bao gồm việc sử dụng CNN cho title và attention để tổng hợp thông tin.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 89: (Điền đáp án) Theo slide 21, các đường cong trên biểu đồ có các "vạch" nhỏ thể hiện điều gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Độ lệch chuẩn / Khoảng tin cậy (Standard deviation / Confidence interval)</span></p><p><b>💡 Giải thích:</b> Các vạch này, thường được gọi là error bars, thể hiện sự biến động hoặc độ không chắc chắn của phép đo sau khi chạy thực nghiệm nhiều lần. Chúng cho biết kết quả có ổn định hay không.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 90: Tại sao trong hàm mục tiêu (slide 15), lại cần có cả `tập positive` và `tập negative`?</p><ul class="options"><li>A. Để làm cho việc huấn luyện chậm hơn.</li><li class="correct-answer">B. Để mô hình học cách phân biệt, tức là gán điểm tương đồng cao cho các cặp (user, item) dương và điểm tương đồng thấp cho các cặp (user, item) âm.</li><li>C. Vì dữ liệu luôn được chia làm hai phần.</li><li>D. Để tăng độ chính xác trên tập positive.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Nếu chỉ học từ các mẫu dương, mô hình có thể học một cách tầm thường là gán điểm cao cho tất cả mọi thứ. Việc đưa vào các mẫu âm buộc mô hình phải học cách phân biệt và đưa ra các dự đoán có ý nghĩa.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 91: Theo Figure 7 (slide 20), đâu là kết hợp thông tin hiệu quả nhất cho News Encoder?</p><ul class="options"><li>A. Không dùng topic hay subtopic (None)</li><li>B. Chỉ dùng Topic (+Topic)</li><li>C. Chỉ dùng Subtopic (+Subtopic)</li><li class="correct-answer">D. Dùng cả hai (+Both)</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ AUC và nDCG@10, cột "+Both" là cột cao nhất, cho thấy việc kết hợp cả thông tin topic và subtopic mang lại hiệu quả tốt nhất.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 92: Việc sử dụng pre-trained word2vec có lợi ích gì?</p><ul class="options"><li>A. Làm cho mô hình phức tạp hơn.</li><li class="correct-answer">B. Tận dụng kiến thức ngữ nghĩa đã được học từ một kho văn bản lớn, giúp mô hình có điểm khởi đầu tốt hơn và hoạt động hiệu quả hơn, đặc biệt khi dữ liệu training cho nhiệm vụ cụ thể không lớn.</li><li>C. Luôn đảm bảo kết quả tốt nhất.</li><li>D. Giảm số lượng từ.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là một dạng của học chuyển giao (transfer learning). Thay vì học embedding từ đầu, ta sử dụng các embedding đã được huấn luyện trên hàng tỷ từ, chứa đựng nhiều thông tin ngữ nghĩa phong phú.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 93: Mô hình này có thể xử lý các từ mới (out-of-vocabulary) trong title không?</p><ul class="options"><li>A. Có, một cách hoàn hảo.</li><li class="correct-answer">B. Gặp khó khăn. Các mô hình dựa trên word2vec truyền thống thường gán một vector "unknown" chung cho tất cả các từ mới, làm mất thông tin.</li><li>C. Không thể xử lý.</li><li>D. Có, bằng cách bỏ qua chúng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là một hạn chế của các phương pháp embedding dựa trên từ điển cố định như word2vec. Các phương pháp hiện đại hơn như FastText (học embedding cho các n-gram ký tự) hoặc các mô hình dựa trên Transformer (như BERT) có khả năng xử lý từ mới tốt hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 94: (Điền đáp án) Theo slide 20, mô hình LSTUR-ini có AUC là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.634 (khoảng)</span></p><p><b>💡 Giải thích:</b> Nhìn vào biểu đồ (a) AUC của Figure 7, cột LSTUR-ini, trường hợp "+Both", có chiều cao tương ứng với khoảng 0.634 trên trục tung.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 95: (Điền đáp án) Theo slide 19, mô hình LSTUR-con có AUC là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.6347 (khoảng)</span></p><p><b>💡 Giải thích:</b> Nhìn vào biểu đồ Figure 6(a), cột LSTUR-con, trường hợp CNN+Att, có chiều cao tương ứng với khoảng 0.634-0.635 trên trục tung.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 96: Nếu bỏ đi GRU và chỉ dùng biểu diễn dài hạn, mô hình sẽ tương tự với phương pháp nào?</p><ul class="options"><li class="correct-answer">A. Matrix Factorization (MF) hoặc các biến thể deep learning của nó.</li><li>B. Gợi ý dựa trên nội dung.</li><li>C. Gợi ý dựa trên láng giềng.</li><li>D. Gợi ý không cá nhân hóa.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Nếu chỉ còn biểu diễn dài hạn của user (`u`) và biểu diễn của news (`e`), và điểm số được tính bằng tích vô hướng, mô hình này sẽ quay về dạng cơ bản của Matrix Factorization, nơi sở thích không thay đổi theo thời gian.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 97: Sự khác biệt chính giữa GRU và một mạng FNN thông thường là gì?</p><ul class="options"><li>A. GRU có nhiều tầng hơn.</li><li class="correct-answer">B. GRU có "bộ nhớ" thông qua các trạng thái ẩn được truyền từ bước thời gian này sang bước thời gian khác, trong khi FNN xử lý mỗi đầu vào một cách độc lập.</li><li>C. GRU sử dụng CNN.</li><li>D. FNN không có hàm kích hoạt.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đặc điểm cốt lõi của các mạng nơ-ron hồi quy (RNN) và các biến thể của nó như GRU/LSTM là khả năng xử lý dữ liệu tuần tự bằng cách duy trì một trạng thái nội tại (bộ nhớ) để nắm bắt các phụ thuộc theo thời gian.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 98: Mô hình này có thể giải quyết được vấn đề item cold-start không?</p><ul class="options"><li>A. Có, một cách hoàn hảo.</li><li class="correct-answer">B. Có, vì nó có một News Encoder có thể tạo ra biểu diễn cho một bài báo mới ngay khi có title và topic của nó, không cần lịch sử tương tác.</li><li>C. Không, nó yêu cầu mọi bài báo phải có người đọc trước.</li><li>D. Chỉ giải quyết được user cold-start.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Vì mô hình học cách biểu diễn tin tức từ các thuộc tính nội tại của nó (title, topic), nó có thể tạo ra một vector cho một bài báo hoàn toàn mới. Vector này sau đó có thể được so sánh với các vector của người dùng để đưa ra gợi ý. Đây là một lợi thế của các phương pháp dựa trên nội dung/mô hình.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 99: Trong thực tế, việc huấn luyện một mô hình phức tạp như LSTUR đòi hỏi tài nguyên gì?</p><ul class="options"><li>A. Chỉ cần một CPU thông thường.</li><li class="correct-answer">B. Một lượng lớn dữ liệu huấn luyện và tài nguyên tính toán mạnh mẽ (thường là GPU).</li><li>C. Không cần dữ liệu.</li><li>D. Chỉ cần một ít bộ nhớ RAM.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các mô hình deep learning với hàng triệu tham số như thế này đòi hỏi phải được huấn luyện trên các bộ dữ liệu rất lớn để có thể học được các mẫu có ý nghĩa và tránh overfitting. Quá trình huấn luyện này rất tốn kém về mặt tính toán và thường được tăng tốc bằng cách sử dụng các đơn vị xử lý đồ họa (GPU).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 100: Bài học quan trọng nhất từ case study này là gì?</p><ul class="options"><li>A. Gợi ý tin tức là một bài toán đã được giải quyết.</li><li class="correct-answer">B. Việc mô hình hóa các khía cạnh khác nhau của sở thích người dùng (dài hạn, ngắn hạn) và kết hợp nhiều nguồn thông tin (title, topic, lịch sử,...) thông qua các kiến trúc deep learning phù hợp có thể cải thiện đáng kể hiệu suất của hệ gợi ý.</li><li>C. Các mô hình đơn giản luôn tốt hơn.</li><li>D. Dữ liệu không quan trọng bằng thuật toán.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Bài báo này là một ví dụ điển hình cho xu hướng hiện tại trong nghiên cứu hệ gợi ý: đi sâu vào việc hiểu và mô hình hóa các hành vi phức tạp của người dùng, và xây dựng các kiến trúc mạng nơ-ron chuyên biệt để nắm bắt các hành vi đó, thay vì chỉ áp dụng một thuật toán chung chung.</p></div></div>
    </div>
</body>
</html>