<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>100 Câu Hỏi Trắc Nghiệm - Case Study: HyperNews</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; background-color: #f4f4f4; color: #333; }
        .container { max-width: 900px; margin: auto; background: #fff; padding: 30px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); }
        header { text-align: center; border-bottom: 2px solid #d9534f; margin-bottom: 30px; padding-bottom: 20px; }
        header h1 { color: #d9534f; margin: 0; }
        header p { margin: 5px 0 0; font-style: italic; color: #555; }
        .question-block { margin-bottom: 25px; padding: 20px; border: 1px solid #ddd; border-left: 5px solid #d9534f; border-radius: 5px; background-color: #fdfdfd; }
        .question-text { font-weight: bold; font-size: 1.1em; margin-bottom: 15px; }
        .options { list-style-type: none; padding-left: 0; }
        .options li { margin-bottom: 10px; padding: 8px; border-radius: 4px; }
        .explanation { margin-top: 15px; padding: 15px; background-color: #e9f7ef; border: 1px solid #a3d9b8; border-radius: 5px; }
        .explanation b { color: #1d7b46; }
        .correct-answer { background-color: #dff0d8; border-left: 3px solid #3c763d; }
        .fill-in-answer { font-weight: bold; color: #3c763d; font-size: 1.2em; }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>100 Câu Hỏi Trắc Nghiệm - Case Study: HyperNews</h1>
            <p>Dựa trên nội dung slide "Phân tích một số hệ gợi ý cụ thể" - Viện CNTT&TT - ĐHBK Hà Nội</p>
        </header>

        <!-- CATEGORY: INTRODUCTION & MOTIVATION -->
        <div class="question-block"><p class="question-text">Câu 1: Tên đầy đủ của bài báo được trình bày trong slide là gì?</p><ul class="options"><li class="correct-answer">A. HyperNews: Simultaneous News Recommendation and Active-Time Prediction via a Double-Task Deep Neural Network</li><li>B. News Recommendation using Deep Learning</li><li>C. A Survey of News Recommender Systems</li><li>D. Multi-Task Learning for News Recommendation</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 4 ghi rõ tên đầy đủ của bài báo.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 2: Mô hình HyperNews được coi là một Framework học máy loại gì?</p><ul class="options"><li>A. Single-Task Learning</li><li class="correct-answer">B. Multi-Task Learning</li><li>C. Unsupervised Learning</li><li>D. Reinforcement Learning</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 5 nêu rõ: "Model được coi là Multi-Task Learning Framework với 2 nhiệm vụ".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 3: Hai nhiệm vụ chính mà mô hình HyperNews thực hiện đồng thời là gì?</p><ul class="options"><li>A. Gợi ý bài News và Phân loại Categories.</li><li class="correct-answer">B. Gợi ý bài News và Dự đoán thời gian đọc bài News.</li><li>C. Dự đoán thời gian đọc và Tóm tắt bài News.</li><li>D. Gợi ý bài News và Phát hiện tin giả.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 5 liệt kê 2 nhiệm vụ: "- Gợi ý bài News" và "- Dự đoán thời gian đọc bài News".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 4: Theo slide 6, thuộc tính mới mà HyperNews khai thác mà các cách tiếp cận trước đó chưa làm là gì?</p><ul class="options"><li>A. Lịch sử click của User.</li><li class="correct-answer">B. Thuộc tính thời gian đọc bài của User (Active-Time).</li><li>C. Nội dung của bài báo.</li><li>D. Tiêu đề của bài báo.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6 nêu rõ: "Các cách tiếp cận mới nhất hiện nay chưa tiếp cận thuộc tính thời gian đọc bài của User" và "HyperNews sẽ khai thác thuộc tính thời gian đọc của User".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 5: "Active-Time" được định nghĩa như thế nào?</p><ul class="options"><li>A. Thời gian từ khi bài báo được xuất bản đến khi user đọc.</li><li>B. Tổng thời gian user online.</li><li class="correct-answer">C. Khoảng thời gian từ lúc User click mở bài News cho đến khi đóng bài News.</li><li>D. Thời gian trung bình user đọc một bài báo.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6 định nghĩa: "Active-Time được tính từ thời điểm User click mở bài News cho đến khi đóng bài News".</p></div></div>
        
        <!-- CATEGORY: MODEL ARCHITECTURE & ENCODERS -->
        <div class="question-block"><p class="question-text">Câu 6: Trong kiến trúc tổng quan (slide 7a), khối "Multi-task" thực hiện những nhiệm vụ nào?</p><ul class="options"><li>A. Chỉ dự đoán Click probability.</li><li>B. Chỉ dự đoán Active-time.</li><li class="correct-answer">C. Dự đoán cả Click probability và Active-time.</li><li>D. Chỉ mã hóa User.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ trên slide 7a cho thấy khối "Multi-task" ở trên cùng, với hai nhánh đầu ra là "Click probability" và "Active-time".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 7: News Encoder trong HyperNews (slide 8) được xây dựng dựa trên 2 kiểu thuộc tính nào?</p><ul class="options"><li>A. Title và Content</li><li>B. Categories và Content</li><li class="correct-answer">C. Thuộc tính rõ ràng (Explicit) và Thuộc tính ẩn (Implicit).</li><li>D. Thuộc tính người dùng và Thuộc tính hệ thống.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 chia thuộc tính của bài news thành 2 kiểu: "- Thuộc tính rõ ràng: Title, Categories, ..." và "- Thuộc tính ẩn: Content".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 8: Tại sao "Title" và "Categories" được coi là thuộc tính rõ ràng (explicit)?</p><ul class="options"><li>A. Vì chúng luôn đúng.</li><li class="correct-answer">B. Vì người dùng có thể thấy chúng trước khi quyết định click vào bài báo.</li><li>C. Vì chúng được mã hóa bằng CNN.</li><li>D. Vì chúng ngắn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 giải thích trong ngoặc: "(Các thuộc tính User có thể thấy trước khi quyết định click)". Đây là thông tin bề mặt, hiển thị ngay lập tức.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 9: Tại sao "Content" được coi là thuộc tính ẩn (implicit)?</p><ul class="options"><li>A. Vì nó được mã hóa bằng LDA.</li><li class="correct-answer">B. Vì người dùng chỉ có thể nhìn thấy nó sau khi đã click vào bài báo.</li><li>C. Vì nó dài.</li><li>D. Vì nó khó hiểu.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 giải thích trong ngoặc: "(User nhìn thấy sau khi đã click)". Đây là thông tin chìm, chỉ được tiếp cận sau khi đã có một hành động tương tác ban đầu.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 10: Hai phần chính cấu thành nên News Encoder là gì?</p><ul class="options"><li>A. CNN và FNN.</li><li>B. LDA và Doc2vec.</li><li class="correct-answer">C. Explicit Embedding và Implicit Embedding.</li><li>D. Title Embedding và Content Embedding.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8, mục "Bao gồm 2 phần", liệt kê: "Explicit Embedding và Implicit Embedding".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 11: Trong Explicit Embedding (slide 9), kỹ thuật nào được sử dụng để làm biểu diễn cho Title?</p><ul class="options"><li>A. RNN</li><li class="correct-answer">B. CNN (Tầng CNN)</li><li>C. Attention</li><li>D. Doc2vec</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 có mục "Tầng CNN:" và mô tả cách sử dụng các bộ lọc (filters) và concat để tạo ra biểu diễn `eh` cho title.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 12: Trong Explicit Embedding, cơ chế nào được sử dụng để tổng hợp thông tin từ các Categories?</p><ul class="options"><li>A. Max-pooling</li><li>B. CNN</li><li class="correct-answer">C. Attention</li><li>D. FNN</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 mô tả việc "sử dụng Attention để tính toán ra trọng số giữa các Attention" của các category, tạo ra biểu diễn `ec_bar`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 13: Biểu diễn Explicit Embedding cuối cùng (`e_Ne`) được tạo ra như thế nào?</p><ul class="options"><li>A. Bằng cách cộng `eh` và `ec_bar`.</li><li class="correct-answer">B. Bằng cách đưa `eh` và `ec_bar` đã được ghép nối (concat) qua một mạng FNN.</li><li>C. Chỉ bằng `eh`.</li><li>D. Chỉ bằng `ec_bar`.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 đưa ra công thức cuối cùng: `e_Ne = FNN([eh; ec_bar])`, với `[;]` là ký hiệu của phép ghép nối (concatenation).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 14 (Chọn nhiều đáp án): Trong Implicit Embedding (slide 10), những mô hình nào được sử dụng để tạo biểu diễn từ Content?</p><ul class="options"><li class="correct-answer">A. LDA</li><li class="correct-answer">B. Doc2vec</li><li>C. CNN</li><li>D. Attention</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 10 mô tả việc sử dụng cả "LDA (train với tập Adressa)" để tạo `et` và "Doc2vec (train với tập Adressa)" để tạo `ed`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 15: Biểu diễn Implicit Embedding cuối cùng (`e_Ni`) được tạo ra như thế nào?</p><ul class="options"><li>A. Bằng cách cộng các biểu diễn `et`, `ed`, `el`.</li><li class="correct-answer">B. Bằng cách đưa các biểu diễn `et`, `ed`, `el` đã được ghép nối qua một tầng FNN.</li><li>C. Chỉ bằng LDA.</li><li>D. Chỉ bằng Doc2vec.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 10 đưa ra công thức: "ev_i = FNN([et; ed; el])". Chú ý, slide có lỗi đánh máy `ev_i`, đúng ra phải là `e_Ni` theo sơ đồ.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 16: Cơ chế Attention trong News Encoder (slide 11) được sử dụng để làm gì?</p><ul class="options"><li>A. Để kết hợp Title và Categories.</li><li class="correct-answer">B. Để học ra sự tương quan và kết hợp có trọng số giữa biểu diễn Explicit (`e_Ne`) và Implicit (`e_Ni`).</li><li>C. Để mã hóa người dùng.</li><li>D. Để dự đoán thời gian đọc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 11 có tiêu đề "News Encoder - Attention Unit" và mô tả việc "Sử dụng cơ chế Attention để học ra tương quan giữa 2 loại đặc trưng". Công thức `e = ã_n^1 * e_Ne + ã_n^2 * e_Ni` thể hiện rõ việc kết hợp có trọng số.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 17: User Encoder (slide 12) sử dụng thông tin gì làm đầu vào?</p><ul class="options"><li>A. Toàn bộ lịch sử của người dùng.</li><li class="correct-answer">B. 30 bài báo mà người dùng đã click gần nhất.</li><li>C. Thông tin nhân khẩu học của người dùng.</li><li>D. Các bài báo mà bạn bè của người dùng đã click.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 12 ghi rõ: "Sử dụng 30 bài User Click gần nhất".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 18: Kỹ thuật nào được sử dụng trong User Encoder để tổng hợp thông tin từ lịch sử click và tạo ra biểu diễn `e_U`?</p><ul class="options"><li>A. CNN</li><li>B. RNN</li><li class="correct-answer">C. Attention</li><li>D. Max-pooling</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 12 có tiêu đề "Sử dụng cơ chế Attention để học ra biểu diễn User". Sơ đồ và các công thức trên slide cũng mô tả một cơ chế attention tương tự như trong News Encoder.</p></div></div>

        <!-- CATEGORY: 2-TASK PREDICTION -->
        <div class="question-block"><p class="question-text">Câu 19: "Timeliness" trong mô hình HyperNews đo lường điều gì?</p><ul class="options"><li>A. Thời gian người dùng đọc bài báo.</li><li class="correct-answer">B. Thời gian từ khi bài báo được xuất bản đến khi người dùng click vào nó.</li><li>C. Tốc độ tải của bài báo.</li><li>D. Thời điểm trong ngày mà người dùng click.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 13 định nghĩa: "Timeliness: Thời gian từ khi bài News được publich tới khi User click vào bài news".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 20: Để dự đoán xác suất click (`ŷp`), mô hình kết hợp những biểu diễn nào?</p><ul class="options"><li>A. Chỉ `e_N` và `e_U`.</li><li class="correct-answer">B. `e_f` (biểu diễn Timeliness) và `e_N` (biểu diễn News) được nhân element-wise, sau đó kết hợp với `e_U` (biểu diễn User).</li><li>C. Chỉ `e_f` và `e_U`.</li><li>D. Chỉ `e_N`.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ trên slide 13 cho thấy `e_f` và `e_p` (biểu diễn News cho nhiệm vụ click) được nhân Hadamard product. Kết quả này (`e_T`) sau đó được nhân Dot product với `e_U`. Công thức `ŷp = sigmoid(e_T^T × e_U)` cũng thể hiện điều này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 21: (Điền đáp án) Hàm kích hoạt nào được sử dụng ở tầng cuối cùng để tính xác suất click `ŷp`?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Sigmoid</span></p><p><b>💡 Giải thích:</b> Slide 13 có công thức: `ŷp = sigmoid(e_T^T × e_U)`. Hàm Sigmoid phù hợp vì nó ánh xạ kết quả ra khoảng (0, 1), có thể diễn giải như một xác suất.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 22: Tại sao tác giả lại rời rạc hóa thời gian đọc (Active-Time) thành bài toán phân loại?</p><ul class="options"><li>A. Vì bài toán hồi quy quá dễ.</li><li class="correct-answer">B. Vì ước lượng chính xác thời gian đọc (ví dụ 50s vs 60s) không thực sự quan trọng và khó khăn, trong khi phân loại vào các khoảng (bins) thì khả thi hơn.</li><li>C. Vì không có đủ dữ liệu thời gian liên tục.</li><li>D. Để tăng tốc độ huấn luyện.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 14 nhận xét: "Ước lượng chính xác thời gian đọc bài User không thực sự quá quan trọng: ví dụ đọc 50s không quá khác biệt với 60s", do đó họ "Rời rạc hóa thời gian và chuyển thành bài toán phân loại".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 23: (Điền đáp án) Trong nhiệm vụ dự đoán Active-Time, thời gian từ 5s-205s được chia thành bao nhiêu nhãn (lớp)?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">20</span></p><p><b>💡 Giải thích:</b> Slide 14 ghi: "Thời gian 5s-205s được chia thành 20 nhãn (10s))". Mỗi nhãn tương ứng với một khoảng thời gian 10 giây.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 24: (Điền đáp án) Hàm kích hoạt nào được sử dụng ở tầng cuối cùng để dự đoán lớp thời gian đọc `ŷt`?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Softmax</span></p><p><b>💡 Giải thích:</b> Slide 14 có công thức: `ŷt = softmax(Wt × e_T + bt)`. Hàm Softmax phù hợp cho bài toán phân loại đa lớp, vì nó tạo ra một phân phối xác suất trên tất cả các lớp.</p></div></div>

        <!-- CATEGORY: MODEL TRAINING -->
        <div class="question-block"><p class="question-text">Câu 25: Trong một mẫu training `X` của HyperNews, `yp` đại diện cho điều gì?</p><ul class="options"><li>A. Biểu diễn của bài báo.</li><li class="correct-answer">B. Nhãn xác suất click (1 hoặc 0).</li><li>C. Biểu diễn của người dùng.</li><li>D. Nhãn thời gian đọc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 15 mô tả các thành phần của mẫu training. `yp` là "Xác suất click (1-0)".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 26: Hàm loss `Lp` (slide 15) được sử dụng cho nhiệm vụ nào?</p><ul class="options"><li class="correct-answer">A. Dự đoán xác suất click.</li><li>B. Dự đoán thời gian đọc.</li><li>C. Mã hóa News.</li><li>D. Mã hóa User.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> `Lp` là hàm lỗi cho dự đoán `ŷp`, tức là xác suất click. Đây là hàm cross-entropy cho bài toán phân loại nhị phân.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 27: Hàm loss `Lt` (slide 15) được sử dụng cho nhiệm vụ nào?</p><ul class="options"><li>A. Dự đoán xác suất click.</li><li class="correct-answer">B. Dự đoán thời gian đọc.</li><li>C. Mã hóa News.</li><li>D. Mã hóa User.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> `Lt` là hàm lỗi cho dự đoán `ŷt`, tức là nhãn thời gian đọc. Đây là hàm cross-entropy cho bài toán phân loại đa lớp.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 28: Tại sao cần phải định nghĩa lại kích thước của mỗi Class trong nhiệm vụ dự đoán Active-Time (slide 16)?</p><ul class="options"><li>A. Vì các class có kích thước bằng nhau.</li><li class="correct-answer">B. Vì các nhãn thời gian đọc bị mất cân bằng (imbalanced).</li><li>C. Để tăng số lượng class.</li><li>D. Để giảm số lượng class.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 16 có nhận xét: "Các nhãn thời gian đọc mất cân bằng". Hầu hết người dùng có thể chỉ đọc trong một khoảng thời gian ngắn, làm cho các lớp thời gian đọc dài có rất ít mẫu. Việc định nghĩa lại `E_m` là một kỹ thuật để xử lý sự mất cân bằng này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 29: Hàm loss tổng hợp `L` được tính như thế nào?</p><ul class="options"><li>A. `L = Lp * λLt`</li><li>B. `L = Lp - λLt`</li><li class="correct-answer">C. `L = Lp + λLt`</li><li>D. `L = max(Lp, λLt)`</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 16 đưa ra công thức loss tổng hợp: `L = Lp + λL't`. `λ` là một siêu tham số để cân bằng tầm quan trọng giữa hai nhiệm vụ.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 30: Tham số `β` trong công thức `E_m = (1 - β^m)/(1 - β)` (slide 16) dùng để làm gì?</p><ul class="options"><li>A. Tốc độ học.</li><li class="correct-answer">B. Là một siêu tham số (ví dụ 0.99, 0.999) để điều chỉnh trọng số của các class dựa trên số lượng mẫu `m` của chúng.</li><li>C. Là xác suất click.</li><li>D. Là thời gian đọc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 16 định nghĩa `Beta` là "tham số: 0.99, 0.999". Đây là một kỹ thuật được gọi là Class-balanced loss, nơi các class có ít mẫu hơn sẽ được gán trọng số cao hơn trong hàm loss để mô hình chú ý đến chúng hơn.</p></div></div>
        
        <!-- CATEGORY: EXPERIMENTS & RESULTS -->
        <div class="question-block"><p class="question-text">Câu 31: Bộ dữ liệu được sử dụng trong thực nghiệm có tên là gì?</p><ul class="options"><li>A. MovieLens</li><li>B. Netflix</li><li class="correct-answer">C. Adressa</li><li>D. Amazon</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 17 có tiêu đề "dataset: Adressa".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 32: (Điền đáp án) Trong bộ dữ liệu Adressa-4week, có bao nhiêu bài báo (news-articles)?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">37,067</span></p><p><b>💡 Giải thích:</b> Bảng trên slide 17, hàng "#news-articles" và cột "Adressa-4week" có giá trị là 37,067.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 33: (Điền đáp án) Trong bộ dữ liệu Adressa-1week, có bao nhiêu sự kiện có thông tin 'active-time'?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">1,062,793</span></p><p><b>💡 Giải thích:</b> Bảng trên slide 17, hàng "#events-with-'active-time'" và cột "Adressa-1week" có giá trị là 1,062,793.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 34: Kết quả đánh giá trên slide 18 cho thấy điều gì về vai trò của "Timeliness"?</p><ul class="options"><li>A. "Timeliness" không có ảnh hưởng.</li><li class="correct-answer">B. "HyperNews" (có Timeliness) luôn cho kết quả tốt hơn "HyperNews without Timeliness" ở tất cả các độ đo.</li><li>C. "Timeliness" chỉ cải thiện nhiệm vụ gợi ý.</li><li>D. "Timeliness" chỉ cải thiện nhiệm vụ dự đoán thời gian.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ (a) và (b) trên slide 18, các cột màu xanh đậm (HyperNews) luôn cao hơn các cột màu xanh nhạt (HyperNews without Timeliness), cho thấy việc thêm vào thuộc tính Timeliness đã cải thiện hiệu năng của mô hình.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 35: Theo bảng kết quả ở slide 19, mô hình nào cho kết quả AUC và F1 cao nhất cho nhiệm vụ News Recommendation trên cả hai bộ dữ liệu?</p><ul class="options"><li>A. LibFM</li><li>B. LSTUR</li><li class="correct-answer">C. HyperNews</li><li>D. DAN</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong bảng ở slide 19, hàng cuối cùng "HyperNews" có các giá trị AUC và F1 (được in đậm) cao nhất so với tất cả các phương pháp khác được so sánh trên cả hai bộ dữ liệu.</p></div></div>
        
        ... (Tiếp tục với 65 câu hỏi còn lại theo cấu trúc tương tự)
        <div class="question-block"><p class="question-text">Câu 36: Mô hình `HyperNews(NoPred)` (slide 19) là phiên bản nào của HyperNews?</p><ul class="options"><li>A. Phiên bản không có User Encoder.</li><li>B. Phiên bản không có News Encoder.</li><li class="correct-answer">C. Phiên bản chỉ thực hiện nhiệm vụ gợi ý mà không có nhiệm vụ dự đoán thời gian đọc.</li><li>D. Phiên bản không có yếu tố Timeliness.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> `NoPred` có thể là viết tắt của "No Prediction" (không có nhiệm vụ dự đoán phụ). So sánh kết quả của `HyperNews(NoPred)` với `HyperNews` đầy đủ cho thấy việc thêm nhiệm vụ dự đoán Active-Time (Multi-Task Learning) đã giúp cải thiện hiệu suất của nhiệm vụ chính là gợi ý tin tức.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 37: Trong News Encoder - Explicit Embedding (slide 9), tại sao chỉ chọn 3 category quan trọng nhất?</p><ul class="options"><li>A. Vì người dùng chỉ có thể nhớ 3 category.</li><li class="correct-answer">B. Đây là một lựa chọn thiết kế để giới hạn độ phức tạp tính toán của cơ chế Attention.</li><li>C. Vì một bài báo không thể thuộc về nhiều hơn 3 category.</li><li>D. Vì các category khác không có vector embedding.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 ghi "(chọn 3 cats quan trọng nhất)". Việc giới hạn số lượng phần tử đầu vào cho cơ chế Attention là một kỹ thuật phổ biến để giữ cho việc tính toán hiệu quả, đặc biệt khi một bài báo có thể được gán nhiều category.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 38: Biểu diễn `e_l` trong Implicit Embedding (slide 10) đại diện cho điều gì?</p><ul class="options"><li>A. Vector LDA.</li><li>B. Vector Doc2vec.</li><li class="correct-answer">C. Vector biểu diễn cho chiều dài của nội dung.</li><li>D. Vector Attention.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ trên slide 10 cho thấy đầu vào "Length" được đưa qua một tầng "Embedding" để tạo ra `e_l`. Điều này có nghĩa là mô hình học một biểu diễn cho thuộc tính chiều dài của văn bản.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 39: Trong User Encoder (slide 12), `q_u` là gì?</p><ul class="options"><li>A. Vector biểu diễn của người dùng.</li><li class="correct-answer">B. Một vector truy vấn (query vector) được học, dùng trong cơ chế Attention để xác định tầm quan trọng của các bài báo đã xem.</li><li>C. Lịch sử click của người dùng.</li><li>D. ID của người dùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> `q_u` đóng vai trò là vector "truy vấn" trong cơ chế attention. Nó được nhân với vector biểu diễn của mỗi bài báo đã xem để tính điểm `α_i^u`, từ đó xác định bài báo nào quan trọng hơn trong việc hình thành nên sở thích tổng hợp `e_U` của người dùng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 40: (Điền đáp án) Theo slide 13, biểu diễn `e_f` của Timeliness được tạo ra từ đâu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Từ một tầng Embedding và một tầng Dense</span></p><p><b>💡 Giải thích:</b> Sơ đồ trên slide 13 cho thấy đầu vào "Timeliness" đi qua một tầng "Embedding" (tạo ra `e_f'`) và sau đó là một tầng "Dense" để tạo ra biểu diễn cuối cùng `e_f`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 41: Tại sao mô hình HyperNews lại sử dụng phép nhân Hadamard Product giữa `e_f` và `e_p` (slide 13)?</p><ul class="options"><li>A. Vì đây là phép toán nhanh nhất.</li><li class="correct-answer">B. Để cho phép sự tương tác giữa các đặc trưng của tin tức và yếu tố thời gian, tạo ra một biểu diễn kết hợp `e_T`.</li><li>C. Vì hai vector này có cùng số chiều.</li><li>D. Để giảm số chiều.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Hadamard product (nhân element-wise) là một cách hiệu quả để mô hình hóa sự tương tác giữa hai vector. Ở đây, nó cho phép mô hình học được rằng các đặc trưng khác nhau của tin tức có thể có tầm quan trọng khác nhau tùy thuộc vào độ "mới" của tin.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 42: Trong hàm loss `Lp` (slide 15), tại sao lại có hai tổng sigma, một trên `S+` và một trên `S-`?</p><ul class="options"><li>A. Để tính toán song song.</li><li class="correct-answer">B. Vì đây là hàm loss cho bài toán phân loại nhị phân, `S+` là tập các mẫu dương (click=1) và `S-` là tập các mẫu âm (click=0).</li><li>C. Để xử lý hai nhóm người dùng.</li><li>D. Để tăng độ phức tạp.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Hàm loss cross-entropy cho phân loại nhị phân có hai thành phần: `y*log(ŷ)` cho các mẫu dương (y=1) và `(1-y)*log(1-ŷ)` cho các mẫu âm (y=0). Việc tách ra hai tổng sigma chỉ là một cách viết khác của cùng một công thức.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 43: (Điền đáp án) Theo slide 19, mô hình nào có hiệu suất kém nhất trên bộ dữ liệu Adressa-1week?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">LibFM</span></p><p><b>💡 Giải thích:</b> Trong bảng ở slide 19, hàng "LibFM" có các giá trị AUC (0.7025) và F1 (0.6953) thấp nhất so với các mô hình khác trên tập Adressa-1week.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 44: Việc HyperNews vượt trội hơn HyperNews(NoPred) chứng tỏ điều gì về Multi-Task Learning?</p><ul class="options"><li>A. Multi-Task Learning luôn tốt hơn Single-Task Learning.</li><li class="correct-answer">B. Trong trường hợp này, việc học thêm nhiệm vụ phụ (dự đoán Active-Time) đã giúp mô hình học được các biểu diễn tốt hơn, từ đó cải thiện hiệu suất cho nhiệm vụ chính (gợi ý).</li><li>C. Multi-Task Learning làm cho mô hình chạy nhanh hơn.</li><li>D. Single-Task Learning phức tạp hơn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là một trong những lợi ích chính của Multi-Task Learning. Bằng cách buộc mô hình phải học các biểu diễn có thể sử dụng cho nhiều nhiệm vụ liên quan, nó có thể học được các đặc trưng tổng quát và mạnh mẽ hơn, hoạt động như một dạng hiệu chỉnh (regularization).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 45: Trong News Encoder, `e_c'` (slide 9) là gì?</p><ul class="options"><li>A. Biểu diễn của toàn bộ Categories.</li><li class="correct-answer">B. Biểu diễn được học cho một category cụ thể sau khi qua tầng FNN (`tanh`).</li><li>C. Vector attention.</li><li>D. Biểu diễn của Title.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ và công thức `e_c' = tanh(W_c * e_c + b_c)` cho thấy `e_c'` là kết quả của việc đưa vector embedding ban đầu của một category (`e_c`) qua một tầng biến đổi phi tuyến.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 46: (Điền đáp án) Theo slide 6, việc khai thác Active-Time có thể giúp ích cho các vấn đề nào khác ngoài gợi ý?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Quảng cáo, điều khiển</span></p><p><b>💡 Giải thích:</b> Slide 6 ghi: "...giúp ích cho các vấn đề khác như Quảng cáo, điều khiển, ..".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 47: Trong User Encoder (slide 12), `e_i^u` là gì?</p><ul class="options"><li>A. Vector biểu diễn của user `u`.</li><li class="correct-answer">B. Vector biểu diễn (từ News Encoder) của bài báo thứ `i` trong lịch sử của user `u`.</li><li>C. Một vector ngẫu nhiên.</li><li>D. ID của bài báo thứ `i`.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ trên slide 12 cho thấy `e_1^u`, `e_2^u`, ... là các đầu ra từ các "News Encoder", tương ứng với các bài báo `D_1^u`, `D_2^u`, ... trong lịch sử của người dùng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 48: "FNN" được nhắc đến nhiều lần trong các slide là viết tắt của thuật ngữ gì?</p><ul class="options"><li>A. Fast Neural Network</li><li class="correct-answer">B. Feed-forward Neural Network</li><li>C. Fused Neural Network</li><li>D. Final Neural Network</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> FNN là viết tắt phổ biến của Feed-forward Neural Network (Mạng nơ-ron truyền thẳng), một dạng cơ bản của mạng nơ-ron nơi thông tin chỉ di chuyển theo một hướng, từ đầu vào đến đầu ra.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 49: Tại sao mô hình lại sử dụng cả LDA và Doc2vec để biểu diễn nội dung?</p><ul class="options"><li>A. Vì một trong hai có thể bị lỗi.</li><li class="correct-answer">B. Để tận dụng điểm mạnh của cả hai phương pháp; LDA nắm bắt tốt các chủ đề (topic), trong khi Doc2vec nắm bắt tốt ngữ nghĩa ở cấp độ câu/đoạn văn.</li><li>C. Vì tác giả không quyết định được nên dùng cái nào.</li><li>D. Để tăng số chiều của vector.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là một kỹ thuật ensemble ở mức đặc trưng. Bằng cách kết hợp các biểu diễn từ các mô hình khác nhau, mỗi mô hình có thể đóng góp một loại thông tin khác nhau, giúp cho biểu diễn tổng hợp trở nên phong phú và mạnh mẽ hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 50: Theo kết quả ở slide 19, mô hình Wide&Deep có hiệu suất như thế nào so với HyperNews?</p><ul class="options"><li>A. Tốt hơn</li><li class="correct-answer">B. Kém hơn đáng kể</li><li>C. Tương đương</li><li>D. Không thể so sánh</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trên tập Adressa-1week, AUC của Wide&Deep là 0.7415 so với 0.8661 của HyperNews. F1 cũng tương tự (0.7244 vs 0.8027). Sự chênh lệch này là rất đáng kể, cho thấy HyperNews vượt trội hơn hẳn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 51: (Điền đáp án) Theo slide 11, trong Attention Unit của News Encoder, có bao nhiêu bộ tham số (V, q, b) cần được học?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">3</span></p><p><b>💡 Giải thích:</b> Slide 11 ghi: "Thực tế trong model, cần học ra 3 bộ: (V, q, b) với 3 task".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 52: Việc sử dụng "Freshness" (slide 13) làm một đặc trưng đầu vào có ý nghĩa gì?</p><ul class="options"><li>A. Nó đo độ sạch sẽ của dữ liệu.</li><li class="correct-answer">B. Nó nắm bắt thông tin về độ "mới" của tin tức ứng viên, một yếu tố quan trọng trong gợi ý tin tức.</li><li>C. Nó đo lường sự tươi mới của giao diện.</li><li>D. Nó không có ý nghĩa.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong lĩnh vực tin tức, độ mới (freshness) là một yếu tố cực kỳ quan trọng. Một tin tức từ hôm qua có thể không còn giá trị. Việc đưa "Freshness" vào làm một đặc trưng cho phép mô hình học được tầm quan trọng của yếu tố này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 53: Tại sao lại cần có một "User Encoder"?</p><ul class="options"><li>A. Để đếm số lượng người dùng.</li><li class="correct-answer">B. Để tạo ra một vector biểu diễn cho sở thích và lịch sử của người dùng, làm cơ sở cho việc cá nhân hóa gợi ý.</li><li>C. Để mã hóa tên người dùng.</li><li>D. Để tăng tốc độ hệ thống.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Để gợi ý được cá nhân hóa, hệ thống phải hiểu được "bạn là ai" và "bạn thích gì". User Encoder (slide 12) chính là thành phần thực hiện nhiệm vụ này bằng cách tổng hợp lịch sử duyệt web của người dùng thành một vector sở thích duy nhất (`e_U`).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 54: Kiến trúc HyperNews có thể được áp dụng cho lĩnh vực nào khác ngoài gợi ý tin tức?</p><ul class="options"><li>A. Chỉ có thể áp dụng cho tin tức.</li><li class="correct-answer">B. Có thể áp dụng cho các lĩnh vực khác có yếu tố thời gian và tương tác tuần tự, ví dụ như gợi ý video trên YouTube hoặc gợi ý sản phẩm trên các trang thương mại điện tử.</li><li>C. Chỉ áp dụng cho gợi ý phim.</li><li>D. Chỉ áp dụng cho dữ liệu văn bản.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Mặc dù được thiết kế cho tin tức, các ý tưởng cốt lõi của HyperNews (Multi-Task Learning, mã hóa lịch sử bằng Attention, xem xét Timeliness) là rất tổng quát và có thể được điều chỉnh để áp dụng cho nhiều bài toán gợi ý tuần tự khác.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 55: (Điền đáp án) Theo slide 16, Loss tổng hợp `L` bằng tổng của `Lp` và thành phần nào?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">λL't</span></p><p><b>💡 Giải thích:</b> Slide 16 có công thức: `L = Lp + λL't`. `L't` là hàm loss đã được điều chỉnh cho việc mất cân bằng lớp.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 56: Trong News Encoder, tại sao lại kết hợp cả biểu diễn Explicit và Implicit?</p><ul class="options"><li>A. Vì một trong hai có thể bị thiếu.</li><li class="correct-answer">B. Vì chúng cung cấp hai loại thông tin bổ sung cho nhau: thông tin bề mặt (để thu hút click) và thông tin chiều sâu (liên quan đến sự hài lòng sau khi đọc).</li><li>C. Để làm cho mô hình phức tạp hơn.</li><li>D. Vì quy định yêu cầu vậy.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 11 nhận xét rằng hai loại đặc trưng này có "các tác động khác nhau đến xác suất User đọc bài và thời gian đọc". Việc kết hợp chúng cho phép mô hình có một cái nhìn toàn diện hơn về một bài báo.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 57: Trong tầng CNN của Explicit Embedding (slide 9), `m = 200` có nghĩa là gì?</p><ul class="options"><li>A. Có 200 tầng CNN.</li><li class="correct-answer">B. Tầng CNN sử dụng 200 bộ lọc (filters) khác nhau để trích xuất đặc trưng.</li><li>C. Kích thước của mỗi bộ lọc là 200.</li><li>D. Biểu diễn title có 200 chiều.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 ghi: "Sử dụng 200 filters (m = 200)". Trong CNN cho văn bản, mỗi bộ lọc sẽ học cách nhận diện một loại n-gram (cụm từ) cụ thể.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 58: Tại sao lại cần có tầng "Dense" sau tầng "Embedding" cho "Freshness" và "Timeliness" (slide 13)?</p><ul class="options"><li>A. Để giảm số chiều.</li><li class="correct-answer">B. Để cho phép mô hình học một phép biến đổi phi tuyến phức tạp hơn trên các biểu diễn embedding ban đầu.</li><li>C. Để tăng số chiều.</li><li>D. Để thêm nhiễu.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tầng embedding chỉ là một phép tra cứu (lookup). Việc thêm một hoặc nhiều tầng Dense (FNN) với các hàm kích hoạt phi tuyến cho phép mô hình học được các mối quan hệ phức tạp hơn giữa các khoảng thời gian và tác động của chúng đến kết quả gợi ý.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 59: (Điền đáp án) Theo slide 8, News Encoder bao gồm 2 phần là Explicit Embedding và loại Embedding nào nữa?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Implicit Embedding</span></p><p><b>💡 Giải thích:</b> Slide 8 ghi rõ: "Bao gồm 2 phần: Explicit Embedding và Implicit Embedding".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 60: Trong kết quả ở slide 18, AUC là độ đo cho nhiệm vụ nào?</p><ul class="options"><li class="correct-answer">A. News Recommendation (phân loại nhị phân: click/không click).</li><li>B. Active-time Prediction.</li><li>C. Cả hai.</li><li>D. Không nhiệm vụ nào.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> AUC (Area Under the ROC Curve) là một độ đo phổ biến cho hiệu suất của các mô hình phân loại nhị phân. Trong bối cảnh này, nó được dùng để đánh giá khả năng của mô hình trong việc xếp hạng các bài báo mà người dùng sẽ click cao hơn các bài báo họ không click. Biểu đồ cũng ghi rõ "News Recommendation" dưới cột AUC.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 61: Trong kết quả ở slide 18, F1 là độ đo cho nhiệm vụ nào?</p><ul class="options"><li>A. Chỉ News Recommendation.</li><li>B. Chỉ Active-time Prediction.</li><li class="correct-answer">C. Cả News Recommendation và Active-time Prediction.</li><li>D. Không nhiệm vụ nào.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Biểu đồ trên slide 18 có hai cột F1 riêng biệt: một cho "News Recommendation" và một cho "Active-time Prediction", cho thấy F1-score được dùng để đánh giá cả hai nhiệm vụ phân loại này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 62: (Điền đáp án) Theo bảng ở slide 19, giá trị AUC của mô hình LSTUR trên tập Adressa-4week là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.7725</span></p><p><b>💡 Giải thích:</b> Trong bảng, tại hàng "LSTUR" và cột "AUC" của "Adressa-4week", giá trị là 0.7725.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 63: (Điền đáp án) Theo bảng ở slide 19, giá trị F1 của mô hình DeepFM trên tập Adressa-1week là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.7288</span></p><p><b>💡 Giải thích:</b> Trong bảng, tại hàng "DeepFM" và cột "F1" của "Adressa-1week", giá trị là 0.7288.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 64: Mô hình DSSM trong bảng so sánh (slide 19) có thể là viết tắt của thuật ngữ gì?</p><ul class="options"><li>A. Deep Structured Semantic Model</li><li class="correct-answer">B. Deep Semantic Similarity Model</li><li>C. Dynamic Social Sensing Model</li><li>D. Double-Task Semantic Model</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> DSSM là một kiến trúc deep learning nổi tiếng của Microsoft, viết tắt của Deep Structured Semantic Model hoặc Deep Semantic Similarity Model, thường được dùng để học các biểu diễn ngữ nghĩa cho văn bản.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 65: Tại sao HyperNews lại vượt trội hơn các mô hình như LSTUR hay DAN?</p><ul class="options"><li>A. Vì nó đơn giản hơn.</li><li class="correct-answer">B. Vì nó khai thác hiệu quả hơn các loại thông tin (explicit, implicit, timeliness, user history) và sử dụng kiến trúc Multi-task learning để học các biểu diễn mạnh mẽ hơn.</li><li>C. Vì nó sử dụng bộ dữ liệu lớn hơn.</li><li>D. Vì nó được huấn luyện lâu hơn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sự vượt trội của HyperNews đến từ thiết kế kiến trúc toàn diện của nó. Nó không chỉ xem xét nội dung và lịch sử như các mô hình khác mà còn tích hợp thông tin "Active-Time" và "Timeliness" thông qua một framework đa nhiệm, cho phép các nhiệm vụ hỗ trợ lẫn nhau.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 66: "Candidate News" trong sơ đồ (slide 7a) là gì?</p><ul class="options"><li>A. Các bài báo người dùng đã đọc.</li><li class="correct-answer">B. Các bài báo ứng viên đang được xem xét để gợi ý cho người dùng.</li><li>C. Các bài báo tin giả.</li><li>D. Các bài báo chưa được xuất bản.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> "Candidate News" là các item tiềm năng. Nhiệm vụ của hệ thống là tính toán một điểm số (ví dụ: xác suất click) cho mỗi Candidate News đối với một User cụ thể, sau đó xếp hạng chúng và chọn ra những item tốt nhất để gợi ý.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 67: Trong kiến trúc tổng thể, đầu ra của User Encoder (`e_U`) và News Encoder (`e_N` của Candidate News) được kết hợp ở đâu?</p><ul class="options"><li class="correct-answer">A. Trong khối Multi-task.</li><li>B. Trong khối News Encoder.</li><li>C. Trong khối User Encoder.</li><li>D. Chúng không được kết hợp.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ trên slide 7a và 13 cho thấy `e_U` và `e_N` (đã được kết hợp với Timeliness) được đưa vào khối Multi-task để thực hiện các phép toán cuối cùng (Dot product, Classifier) và đưa ra dự đoán.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 68: Nếu bỏ đi User Encoder, hệ thống sẽ trở thành loại gợi ý nào?</p><ul class="options"><li>A. Gợi ý được cá nhân hóa cao.</li><li class="correct-answer">B. Gợi ý không được cá nhân hóa (non-personalized) hoặc chỉ dựa trên ngữ cảnh.</li><li>C. Gợi ý dựa trên lọc cộng tác.</li><li>D. Gợi ý dựa trên tri thức.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> User Encoder là thành phần tạo ra biểu diễn cho sở thích cá nhân của người dùng. Nếu không có nó, hệ thống sẽ không thể phân biệt giữa các người dùng khác nhau và chỉ có thể đưa ra các gợi ý chung chung, ví dụ như các bài báo phổ biến nhất.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 69: Việc chia thời gian đọc thành các "bins" (khoảng) là một ví dụ của kỹ thuật gì trong xử lý dữ liệu?</p><ul class="options"><li>A. Chuẩn hóa (Normalization)</li><li class="correct-answer">B. Rời rạc hóa (Discretization)</li><li>C. Lấy mẫu (Sampling)</li><li>D. Giảm chiều (Dimensionality Reduction)</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Rời rạc hóa là quá trình chuyển đổi một biến liên tục thành một biến rời rạc bằng cách chia nó thành một số khoảng (bins) hữu hạn. Đây chính xác là những gì được mô tả ở slide 14.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 70: (Điền đáp án) Theo slide 11, thực tế trong mô hình HyperNews, có bao nhiêu nhiệm vụ (task) cần học các bộ tham số Attention?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">3</span></p><p><b>💡 Giải thích:</b> Slide 11 ghi: "...cần học ra 3 bộ: (V, q, b) với 3 task: Dự đoán xác suất Click, Thời gian đọc, Biểu diễn User". Mỗi nhiệm vụ có thể cần một cơ chế attention riêng để tập trung vào các khía cạnh khác nhau của tin tức.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 71: Theo bạn, tại sao User Encoder lại chỉ sử dụng 30 bài click gần nhất mà không phải tất cả?</p><ul class="options"><li>A. Vì người dùng không bao giờ đọc quá 30 bài.</li><li class="correct-answer">B. Để giới hạn độ phức tạp tính toán và tập trung vào sở thích gần đây của người dùng, vốn có thể thay đổi theo thời gian.</li><li>C. Vì chỉ có 30 bài đầu tiên là quan trọng.</li><li>D. Vì bộ nhớ không đủ.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là một lựa chọn thiết kế thực tế. Xử lý toàn bộ lịch sử có thể rất tốn kém và không cần thiết, vì sở thích của người dùng về tin tức thường thay đổi nhanh. Việc tập trung vào các tương tác gần đây giúp mô hình nắm bắt được sở thích hiện tại một cách hiệu quả hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 72: (Điền đáp án) Theo slide 9, biểu diễn cuối cùng của Explicit Embedding được tạo ra bằng cách đưa vector ghép nối qua một mạng nơ-ron loại gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">FNN (Feed-forward Neural Network)</span></p><p><b>💡 Giải thích:</b> Slide 9 có công thức `e_Ne = FNN([eh; ec_bar])`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 73: Giả sử một bài báo có 10 category. Theo thiết kế trên slide 9, cơ chế Attention sẽ hoạt động trên bao nhiêu category?</p><ul class="options"><li>A. 10</li><li class="correct-answer">B. 3</li><li>C. 1</li><li>D. 100</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 ghi rõ "(chọn 3 cats quan trọng nhất)". Điều này ngụ ý rằng mô hình có một cơ chế (có thể là dựa trên tần suất hoặc một tiêu chí khác) để chọn ra 3 category chính trước khi đưa vào Attention.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 74: Nếu một người dùng click vào một bài báo và đóng ngay lập tức (ví dụ: Active-Time < 5s), dữ liệu này sẽ được xử lý như thế nào trong mô hình?</p><ul class="options"><li>A. Được coi là một tương tác rất tích cực.</li><li class="correct-answer">B. Vẫn được ghi nhận là một "click" (mẫu dương cho nhiệm vụ dự đoán click), nhưng có thể không được sử dụng cho nhiệm vụ dự đoán Active-Time (vì nằm ngoài khoảng 5s-205s).</li><li>C. Bị loại bỏ hoàn toàn.</li><li>D. Được coi là một tương tác tiêu cực.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Mô hình có hai nhiệm vụ riêng biệt. Hành động click tạo ra một mẫu dương cho nhiệm vụ dự đoán click. Tuy nhiên, vì thời gian đọc nằm ngoài khoảng được xem xét (5s-205s), nó sẽ không được dùng để huấn luyện cho mô hình phân loại Active-Time.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 75: (Điền đáp án) Theo slide 15, `S+` và `S-` trong hàm loss `Lp` lần lượt là tập các mẫu nào?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Tập các mẫu dương (positive) và âm (negative)</span></p><p><b>💡 Giải thích:</b> `S+` là tập các mẫu có `y_p=1` (click), và `S-` là tập các mẫu có `y_p=0` (không click). Đây là cách ký hiệu chuẩn cho hàm loss cross-entropy.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 76: Tham số `λ` trong hàm loss tổng hợp `L = Lp + λL't` (slide 16) dùng để làm gì?</p><ul class="options"><li>A. Tốc độ học</li><li class="correct-answer">B. Là một siêu tham số để cân bằng tầm quan trọng giữa hai nhiệm vụ (dự đoán click và dự đoán thời gian).</li><li>C. Tham số hiệu chỉnh (regularization).</li><li>D. Số lượng mẫu.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong Multi-Task Learning, việc cân bằng các hàm loss của các nhiệm vụ khác nhau là rất quan trọng. `λ` cho phép nhà nghiên cứu quyết định xem mô hình nên ưu tiên tối ưu hóa cho nhiệm vụ nào hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 77: Tại sao kết quả trên tập Adressa-4week lại thấp hơn Adressa-1week (slide 18, 19)?</p><ul class="options"><li>A. Do lỗi trong quá trình thử nghiệm.</li><li class="correct-answer">B. Có thể vì Adressa-4week là một bộ dữ liệu lớn hơn và đa dạng hơn, làm cho bài toán trở nên khó hơn.</li><li>C. Vì Adressa-1week có nhiều dữ liệu hơn.</li><li>D. Vì các mô hình bị overfitting trên Adressa-4week.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Một bộ dữ liệu lớn hơn (4 tuần so với 1 tuần) thường chứa nhiều người dùng và sản phẩm hơn, với các mẫu hành vi đa dạng và phức tạp hơn. Điều này làm cho việc học và dự đoán trở thành một thách thức lớn hơn, dẫn đến điểm số các độ đo thường thấp hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 78: Sơ đồ khối "Concatenation" trong các kiến trúc trên slide có chức năng gì?</p><ul class="options"><li>A. Tính tổng các vector.</li><li class="correct-answer">B. Ghép nối các vector lại với nhau để tạo thành một vector dài hơn.</li><li>C. Nhân các vector.</li><li>D. Chọn vector lớn nhất.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Concatenation là một phép toán phổ biến trong deep learning, dùng để kết hợp thông tin từ nhiều nguồn bằng cách đặt các vector nối tiếp nhau.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 79: Trong News Encoder, đầu ra `e` của Attention Unit (slide 11) được dùng làm gì?</p><ul class="options"><li>A. Làm đầu ra cuối cùng của mô hình.</li><li class="correct-answer">B. Là biểu diễn cuối cùng, tổng hợp của bài báo, được đưa vào các bước tiếp theo (ví dụ: User Encoder hoặc khối Multi-task).</li><li>C. Dùng để tính toán loss.</li><li>D. Bị loại bỏ.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> `e` là vector biểu diễn cho một bài báo sau khi đã kết hợp cả thông tin explicit và implicit. Vector này (`e_i^u` trong User Encoder, `e_N` trong khối Multi-task) sau đó được sử dụng trong các phần khác của mô hình.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 80: (Điền đáp án) Theo slide 17, số lượng users trong bộ dữ liệu Adressa-1week là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">601,215</span></p><p><b>💡 Giải thích:</b> Bảng trên slide 17, hàng "#users", cột "Adressa-1week" có giá trị 601,215.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 81: Nhìn chung, kiến trúc HyperNews thuộc trường phái gợi ý nào?</p><ul class="options"><li>A. Gợi ý dựa trên láng giềng (Neighborhood-based)</li><li class="correct-answer">B. Gợi ý dựa trên mô hình (Model-based)</li><li>C. Gợi ý dựa trên quy tắc (Rule-based)</li><li>D. Gợi ý không cá nhân hóa</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> HyperNews xây dựng một mô hình deep learning phức tạp để học các biểu diễn ẩn và dự đoán. Đây là đặc trưng điển hình của các phương pháp gợi ý dựa trên mô hình.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 82: Mô hình có sử dụng cơ chế nào để xử lý các câu có độ dài khác nhau trong Title không?</p><ul class="options"><li class="correct-answer">A. Có, tầng CNN với max-pooling.</li><li>B. Không, nó yêu cầu tất cả Title phải có cùng độ dài.</li><li>C. Có, bằng cách đệm (padding) cho các câu ngắn.</li><li>D. Có, bằng cách cắt (truncating) các câu dài.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Một trong những ưu điểm của việc sử dụng CNN cho xử lý văn bản là tầng max-pooling (slide 9). Nó lấy giá trị lớn nhất từ đầu ra của mỗi bộ lọc, tạo ra một vector có kích thước cố định bất kể độ dài của câu đầu vào. Đây là cách phổ biến để xử lý các câu có độ dài thay đổi.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 83: Theo slide 19, mô hình nào là một baseline (mô hình cơ sở) đơn giản?</p><ul class="options"><li class="correct-answer">A. LibFM</li><li>B. Wide&Deep</li><li>C. LSTUR</li><li>D. HyperNews</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> LibFM là một thư viện mã nguồn mở nổi tiếng cho các mô hình Factorization Machines, thường được sử dụng làm một baseline mạnh cho các bài toán gợi ý. Trong bảng so sánh, nó có hiệu suất thấp nhất, cho thấy các mô hình deep learning phức tạp hơn đã mang lại sự cải thiện đáng kể.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 84: Nếu bỏ đi nhiệm vụ dự đoán Active-Time, mô hình HyperNews sẽ gần giống với mô hình nào trong bảng so sánh?</p><ul class="options"><li>A. LibFM</li><li>B. Wide&Deep</li><li class="correct-answer">C. Các mô hình như LSTUR, DAN, vì chúng cũng là các mô hình deep learning cho gợi ý tin tức dựa trên lịch sử người dùng và nội dung.</li><li>D. Không giống mô hình nào.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Phiên bản `HyperNews(NoPred)` về cơ bản là một hệ thống gợi ý tin tức deep learning đơn nhiệm. Các mô hình như LSTUR và DAN cũng được thiết kế cho cùng một bài toán, sử dụng các kiến trúc tương tự (ví dụ: mã hóa người dùng, mã hóa tin tức, cơ chế attention).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 85: (Điền đáp án) Theo slide 17, số lượng từ trung bình trên mỗi tiêu đề (#words-per-title) trong bộ Adressa-4week là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">6.50</span></p><p><b>💡 Giải thích:</b> Bảng trên slide 17, hàng "#words-per-title", cột "Adressa-4week" có giá trị 6.50.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 86: Tại sao cần định nghĩa lại loss cho nhiệm vụ Active-Time (L't ở slide 16)?</p><ul class="options"><li>A. Để làm cho loss lớn hơn.</li><li class="correct-answer">B. Để xử lý vấn đề mất cân bằng lớp, bằng cách tăng trọng số cho các lớp có ít mẫu (thời gian đọc dài).</li><li>C. Để làm cho loss nhỏ hơn.</li><li>D. Để đơn giản hóa việc tính toán.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Khi các lớp bị mất cân bằng, mô hình có xu hướng chỉ dự đoán các lớp đa số. Bằng cách định nghĩa lại loss với trọng số `1/E_m_yt`, các mẫu thuộc lớp hiếm sẽ có đóng góp lớn hơn vào tổng loss, buộc mô hình phải học cách phân loại chúng một cách chính xác.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 87: Sơ đồ trên slide 7b (News Encoder) cho thấy thông tin Content được xử lý bởi những thành phần nào?</p><ul class="options"><li>A. Chỉ LDA.</li><li>B. Chỉ Doc2vec.</li><li class="correct-answer">C. LDA, Doc2vec, và Embedding cho Length.</li><li>D. Chỉ FNN.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ chi tiết của News Encoder cho thấy khối "Texts Content" được đưa vào cả LDA và Doc2vec, và thông tin "Length" cũng được mã hóa. Tất cả sau đó được đưa qua FNN.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 88: Nếu một người dùng chỉ đọc các bài báo về thể thao, vector biểu diễn `e_U` của người dùng đó sẽ có đặc điểm gì?</p><ul class="options"><li>A. Sẽ là một vector không.</li><li class="correct-answer">B. Sẽ có độ tương đồng cao với các vector `e_N` của các bài báo thể thao khác.</li><li>C. Sẽ có độ tương đồng cao với vector của các bài báo chính trị.</li><li>D. Sẽ thay đổi ngẫu nhiên.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> User Encoder được thiết kế để học một biểu diễn cho sở thích của người dùng. Nếu người dùng chỉ đọc về thể thao, vector `e_U` sẽ được hình thành từ các vector `e_N` của các bài báo thể thao, và do đó nó sẽ nằm trong vùng "thể thao" của không gian đặc trưng ẩn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 89: (Điền đáp án) Theo slide 13, xác suất click `ŷp` được tính bằng hàm sigmoid của tích gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Tích vô hướng của e_T và e_U</span></p><p><b>💡 Giải thích:</b> Slide 13 có công thức `ŷp = sigmoid(e_T^T × e_U)`. `e_T^T × e_U` chính là phép toán tích vô hướng (dot product).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 90: Nếu `β = 1` trong công thức `E_m = (1 - β^m)/(1 - β)`, điều gì sẽ xảy ra?</p><ul class="options"><li>A. `E_m = 0`</li><li class="correct-answer">B. Công thức không xác định (chia cho 0).</li><li>C. `E_m = m`</li><li>D. `E_m = 1`</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Nếu `β = 1`, mẫu số `1 - β` sẽ bằng 0, dẫn đến phép chia không xác định. Trong thực tế, các giá trị của `β` được chọn rất gần 1 (như 0.99, 0.999) nhưng không bao giờ bằng 1.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 91: Theo slide 8, News Encoder cuối cùng bao gồm mấy phần?</p><ul class="options"><li>A. 1</li><li class="correct-answer">B. 2</li><li>C. 3</li><li>D. 4</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 ghi rõ: "Bao gồm 2 phần: Explicit Embedding và Implicit Embedding".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 92: Trong thực tế, việc dự đoán Active-Time có thể giúp các nhà quảng cáo làm gì?</p><ul class="options"><li>A. Xác định giá quảng cáo.</li><li class="correct-answer">B. Hiển thị các quảng cáo video dài hơn cho những người dùng có xu hướng đọc/xem lâu hơn.</li><li>C. Chặn quảng cáo.</li><li>D. Tăng số lượng quảng cáo.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6 đề cập đến ứng dụng cho "Quảng cáo". Nếu hệ thống biết một người dùng có khả năng sẽ ở lại trang trong 3 phút, việc hiển thị một quảng cáo video 30 giây sẽ hiệu quả hơn là hiển thị nó cho một người dùng có xu hướng rời đi sau 10 giây.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 93: Tại sao Attention lại hữu ích trong User Encoder?</p><ul class="options"><li>A. Vì nó làm cho mô hình phức tạp hơn.</li><li class="correct-answer">B. Vì nó cho phép mô hình tự động xác định và gán trọng số cao hơn cho các bài báo quan trọng hơn trong lịch sử của người dùng, thay vì coi tất cả đều quan trọng như nhau.</li><li>C. Vì nó nhanh hơn việc lấy trung bình.</li><li>D. Vì nó chỉ xem xét bài báo cuối cùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Không phải tất cả các bài báo đã đọc đều phản ánh sở thích của người dùng một cách như nhau. Một bài báo người dùng đọc kỹ có thể quan trọng hơn một bài họ chỉ click vào xem lướt. Cơ chế Attention (slide 12) cho phép mô hình học được tầm quan trọng tương đối này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 94: (Điền đáp án) Theo slide 14, vector `e_T` được tạo ra bằng cách ghép nối (concat) những vector nào?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">e_t và e_U</span></p><p><b>💡 Giải thích:</b> Sơ đồ trên slide 14 cho thấy `e_t` (biểu diễn của news và timeliness) và `e_U` (biểu diễn của user) được đưa vào một khối "Concatenation" để tạo ra `e_T`, đầu vào cho tầng phân loại Active-Time.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 95: Giả sử mô hình dự đoán xác suất click là 0.8 cho một bài báo. Điều này có nghĩa là gì?</p><ul class="options"><li>A. Người dùng sẽ đọc 80% bài báo.</li><li class="correct-answer">B. Mô hình tin rằng có 80% khả năng người dùng sẽ click vào bài báo đó.</li><li>C. Có 80% người dùng đã click vào bài báo đó.</li><li>D. Bài báo đó được xếp hạng 0.8.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đầu ra của nhánh dự đoán click là `ŷp` (slide 13), là xác suất click. Giá trị 0.8 là một ước lượng xác suất cho một cặp (user, item) cụ thể.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 96: Nếu không có cơ chế Attention, User Encoder có thể được xây dựng bằng cách nào đơn giản hơn?</p><ul class="options"><li>A. Bỏ qua lịch sử người dùng.</li><li class="correct-answer">B. Lấy trung bình cộng (averaging) các vector biểu diễn của các bài báo đã xem.</li><li>C. Chỉ sử dụng bài báo đầu tiên.</li><li>D. Chỉ sử dụng bài báo cuối cùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Một cách tiếp cận baseline phổ biến để tổng hợp một chuỗi các vector là lấy trung bình của chúng. Tuy nhiên, cách này coi tất cả các item trong lịch sử có tầm quan trọng như nhau, điều mà Attention cố gắng cải thiện.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 97: Theo slide 19, HyperNews vượt trội hơn tất cả các baseline trên độ đo nào?</p><ul class="options"><li>A. Chỉ AUC</li><li>B. Chỉ F1</li><li class="correct-answer">C. Cả AUC và F1</li><li>D. Thời gian huấn luyện</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các giá trị được in đậm trong hàng "HyperNews" ở bảng trên slide 19 đều cao hơn tất cả các giá trị tương ứng trong các hàng khác cho cả hai độ đo AUC và F1 trên cả hai bộ dữ liệu.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 98: Mô hình nào trong các baseline (slide 19) cũng là một mô hình deep learning cho gợi ý?</p><ul class="options"><li>A. Chỉ LibFM.</li><li class="correct-answer">B. DeepFM, Wide&Deep, DSSM, LSTUR, DAN.</li><li>C. Chỉ HyperNews(NoPred).</li><li>D. Không có mô hình nào.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Ngoại trừ LibFM (là Factorization Machine), tất cả các mô hình còn lại trong bảng so sánh đều là các kiến trúc deep learning nổi tiếng được đề xuất cho các bài toán gợi ý hoặc các bài toán liên quan.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 99: Tại sao cần phải có các bài báo "Candidate" trong mẫu training (slide 15)?</p><ul class="options"><li>A. Để làm cho mẫu lớn hơn.</li><li class="correct-answer">B. Để tạo ra các mẫu âm (negative samples). Nếu chỉ học từ các bài báo người dùng đã click (mẫu dương), mô hình sẽ không học được cách phân biệt giữa một bài báo tốt và một bài báo không tốt.</li><li>C. Để kiểm tra mô hình.</li><li>D. Để dự đoán thời gian đọc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong implicit feedback, việc tạo ra các mẫu âm là rất quan trọng. "Candidate" (`xr+1`) ở đây thường là một bài báo được lấy mẫu ngẫu nhiên mà người dùng chưa click. Nó được gán nhãn `yp=0` và đóng vai trò là một mẫu âm trong quá trình huấn luyện.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 100: Mục tiêu cuối cùng của mô hình HyperNews là gì?</p><ul class="options"><li>A. Chỉ để dự đoán thời gian đọc chính xác.</li><li class="correct-answer">B. Cung cấp các gợi ý tin tức được cá nhân hóa và có độ chính xác cao bằng cách đồng thời học cách gợi ý và dự đoán mức độ tương tác (thời gian đọc) của người dùng.</li><li>C. Để chứng minh CNN tốt hơn RNN.</li><li>D. Để tạo ra một bộ dữ liệu mới.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Như tên gọi và phần giới thiệu (slide 5) đã nêu, mô hình này nhằm mục đích cải thiện hiệu suất gợi ý tin tức (nhiệm vụ chính) bằng cách sử dụng thông tin từ một nhiệm vụ phụ hữu ích là dự đoán thời gian đọc, tất cả được đóng gói trong một framework deep learning đa nhiệm.</p></div></div>

    </div>
</body>
</html>