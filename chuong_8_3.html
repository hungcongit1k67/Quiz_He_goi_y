<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>100 Câu Hỏi Trắc Nghiệm - Case Study: NAML</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; background-color: #f4f4f4; color: #333; }
        .container { max-width: 900px; margin: auto; background: #fff; padding: 30px; border-radius: 8px; box-shadow: 0 0 15px rgba(0,0,0,0.1); }
        header { text-align: center; border-bottom: 2px solid #d9534f; margin-bottom: 30px; padding-bottom: 20px; }
        header h1 { color: #d9534f; margin: 0; }
        header p { margin: 5px 0 0; font-style: italic; color: #555; }
        .question-block { margin-bottom: 25px; padding: 20px; border: 1px solid #ddd; border-left: 5px solid #d9534f; border-radius: 5px; background-color: #fdfdfd; }
        .question-text { font-weight: bold; font-size: 1.1em; margin-bottom: 15px; }
        .options { list-style-type: none; padding-left: 0; }
        .options li { margin-bottom: 10px; padding: 8px; border-radius: 4px; }
        .explanation { margin-top: 15px; padding: 15px; background-color: #e9f7ef; border: 1px solid #a3d9b8; border-radius: 5px; }
        .explanation b { color: #1d7b46; }
        .correct-answer { background-color: #dff0d8; border-left: 3px solid #3c763d; }
        .fill-in-answer { font-weight: bold; color: #3c763d; font-size: 1.2em; }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>100 Câu Hỏi Trắc Nghiệm - Case Study: NAML</h1>
            <p>Dựa trên nội dung slide "Neural News Recommendation with Attentive Multi-View Learning" - Viện CNTT&TT - ĐHBK Hà Nội</p>
        </header>

        <!-- CATEGORY: INTRODUCTION & MOTIVATION -->
        <div class="question-block"><p class="question-text">Câu 1: Tên viết tắt của mô hình được giới thiệu trong slide 4 là gì?</p><ul class="options"><li>A. LSTUR</li><li>B. HyperNews</li><li class="correct-answer">C. NAML</li><li>D. DKN</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 4 có tiêu đề "Neural News Recommendation with Attentive Multi-View Learning - NAML".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 2: Theo slide 6, một bài báo thường chứa thông tin ở những phần nào?</p><ul class="options"><li>A. Chỉ tiêu đề và nội dung.</li><li class="correct-answer">B. Tiêu đề, nội dung, và thể loại.</li><li>C. Chỉ nội dung và thể loại.</li><li>D. Chỉ tiêu đề.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6 nêu rõ: "Một bài báo thường chứa thông tin ở ở nhiều phần khác nhau: tiêu đề, nội dung, thể loại."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 3: Tại sao các thành phần khác nhau của một bài báo cần được "sử dụng khác nhau" theo slide 6?</p><ul class="options"><li>A. Vì chúng được viết bởi các tác giả khác nhau.</li><li class="correct-answer">B. Vì chúng có các tính chất khác nhau (ví dụ: tiêu đề ngắn và súc tích, nội dung dài và chi tiết).</li><li>C. Vì chúng có độ dài bằng nhau.</li><li>D. Vì hệ thống yêu cầu vậy.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6 giải thích: "Các thành phần này có các tính chất khác nhau: tiêu đề ngắn và súc tích, nội dung thì dài và chi tiết, thể loại thì thường là cụm từ. → sử dụng khác nhau".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 4: Theo slide 7, tại sao tin "thời tiết" lại đóng góp ít trong việc tìm sở thích người dùng?</p><ul class="options"><li>A. Vì tin thời tiết không quan trọng.</li><li class="correct-answer">B. Vì tin thời tiết có nhiều người cùng xem, nó không thể hiện một sở thích đặc biệt hay "ngách" nào của người dùng.</li><li>C. Vì tin thời tiết quá ngắn.</li><li>D. Vì không ai đọc tin thời tiết.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 giải thích: "tin “thời tiết” có nhiều người khác cũng xem, đóng góp ít trong việc tìm sở thích user". Đây là ví dụ về một chủ đề phổ biến, không có tính phân biệt cao.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 5: Khái niệm "attentive multi-view learning" trong mô hình NAML có nghĩa là gì?</p><ul class="options"><li>A. Học từ nhiều người dùng khác nhau.</li><li class="correct-answer">B. Khai thác thông tin từ nhiều thành phần (view) của bài báo như tiêu đề, nội dung, thể loại và sử dụng cơ chế attention.</li><li>C. Học từ nhiều bộ dữ liệu khác nhau.</li><li>D. Chỉ sử dụng cơ chế attention.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 mô tả mô hình NAML sử dụng "attentive multi-view learning", trong đó "biểu diễn item từ tiêu đề, nội dung, thể loại khi xem xét chúng là các điểm nhìn (view) khác nhau về bài báo".</p></div></div>
        
        <!-- CATEGORY: MODEL ARCHITECTURE -->
        <div class="question-block"><p class="question-text">Câu 6: Mô hình NAML được cấu tạo bởi 3 thành phần chính nào?</p><ul class="options"><li>A. News encoder, Topic encoder, Click predictor.</li><li class="correct-answer">B. News encoder, User encoder, Click predictor.</li><li>C. User encoder, Click predictor, Loss function.</li><li>D. News encoder, CNN, GRU.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 liệt kê 3 thành phần cấu tạo của mô hình là "news encoder", "user encoder", và "click predictor".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 7: News Encoder trong NAML (slide 10) gồm bao nhiêu thành phần con?</p><ul class="options"><li>A. 1</li><li>B. 2</li><li class="correct-answer">C. 3</li><li>D. 4</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 10 liệt kê "Gồm 3 thành phần": title encoder, body encoder, và categories encoder.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 8: Trong title encoder và body encoder, những kỹ thuật nào được sử dụng theo thứ tự?</p><ul class="options"><li>A. Attention -> CNN -> Word embedding.</li><li class="correct-answer">B. Word embedding -> CNN -> Attention.</li><li>C. CNN -> Word embedding -> Attention.</li><li>D. Word embedding -> Attention -> CNN.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 10 liệt kê các bước cho cả title và body encoder là: "Word embedding", sau đó là "CNN", và cuối cùng là "attention".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 9: Categories encoder chỉ bao gồm thành phần nào?</p><ul class="options"><li>A. CNN</li><li>B. Attention</li><li class="correct-answer">C. Categories embedding</li><li>D. Word embedding</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 10, trong mục "categories encoder", chỉ liệt kê "categories embedding".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 10: User Encoder trong NAML được dùng để làm gì?</p><ul class="options"><li>A. Để mã hóa thông tin cá nhân của người dùng.</li><li class="correct-answer">B. Để tạo ra một vector biểu diễn cho người dùng dựa trên lịch sử các bài báo họ đã xem.</li><li>C. Để đếm số lượng người dùng.</li><li>D. Để dự đoán lượt click.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ ở slide 9 cho thấy User Encoder nhận đầu vào là "Browsed News" (các tin đã duyệt) và tạo ra vector biểu diễn người dùng `u`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 11: Click Predictor thực hiện nhiệm vụ gì?</p><ul class="options"><li>A. Đếm số lần click.</li><li class="correct-answer">B. Dự đoán khả năng người dùng sẽ click vào một bài báo ứng viên.</li><li>C. Chọn ra bài báo có nhiều click nhất.</li><li>D. Mã hóa người dùng và bài báo.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tên gọi "Click Predictor" và sơ đồ ở slide 9 cho thấy nó nhận đầu vào là vector người dùng `u` và vector bài báo ứng viên `r_c` để tính ra xác suất click `ŷ`.</p></div></div>
        
        <!-- CATEGORY: ENCODERS IN DETAIL -->
        <div class="question-block"><p class="question-text">Câu 12: Trong Title encoder (slide 8), `c_i` là gì?</p><ul class="options"><li>A. Vector embedding của từ thứ `i`.</li><li class="correct-answer">B. Vector context được trích xuất bởi một bộ lọc CNN tại vị trí `i`.</li><li>C. Trọng số attention của từ thứ `i`.</li><li>D. Một category.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức `c_i = ReLU(C × W[...] + b)` trên slide 8 là công thức của một phép tích chập, do đó `c_i` là đầu ra của một bộ lọc CNN, đại diện cho một đặc trưng ngữ cảnh.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 13: Trong Title encoder (slide 8), `e_t` được tính như thế nào?</p><ul class="options"><li>A. Bằng cách lấy trung bình các vector `c_i`.</li><li class="correct-answer">B. Bằng tổng có trọng số của các vector `c_i`, với trọng số `α_i` được tính bằng cơ chế attention.</li><li>C. Bằng cách ghép nối các vector `c_i`.</li><li>D. Bằng cách lấy vector `c_i` cuối cùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 trình bày công thức `e_t = Σ α_i * c_i`, đây là một phép tổng có trọng số (weighted sum) dựa trên trọng số attention `α_i`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 14: Theo sơ đồ News Encoder (slide 10), vector biểu diễn cuối cùng của bài báo (`r`) được tạo ra như thế nào?</p><ul class="options"><li>A. Chỉ bằng vector của title.</li><li class="correct-answer">B. Bằng một phép tổng hợp có trọng số (attention) của các vector đại diện cho title, body và categories.</li><li>C. Bằng cách ghép nối ba vector của title, body và categories.</li><li>D. Chỉ bằng vector của body.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ cho thấy các biểu diễn từ Title, Body, Categories (là `r^t`, `r^b`, `r^c`) được đưa vào một cơ chế attention (với các trọng số `α_t`, `α_b`, `α_c`) để tạo ra vector cuối cùng `r`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 15: User Encoder (slide 11) áp dụng attention lên đối tượng nào?</p><ul class="options"><li>A. Các từ trong lịch sử duyệt web.</li><li class="correct-answer">B. Các vector biểu diễn của các item (bài báo) mà người dùng đã xem.</li><li>C. Các category mà người dùng đã xem.</li><li>D. Các user khác.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 11 có tiêu đề "Áp dụng attention cho biểu diễn item user đã xem". Công thức `u = Σ α_i^n * r_i` cho thấy nó tính tổng có trọng số của các vector bài báo `r_i` trong lịch sử của người dùng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 16: Click Predictor (slide 12) tính điểm dự đoán `ŷ` bằng cách nào?</p><ul class="options"><li>A. Bằng phép cộng `u + r_c`.</li><li class="correct-answer">B. Bằng tích vô hướng (dot product) `u^T * r_c`.</li><li>C. Bằng cách đưa `u` và `r_c` qua một mạng FNN.</li><li>D. Bằng độ tương đồng Cosine.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 12 đưa ra công thức tính điểm dự đoán là `ŷ = u^T * r_c`, đây chính là phép toán tích vô hướng.</p></div></div>
        
        <!-- CATEGORY: TRAINING & EVALUATION -->
        <div class="question-block"><p class="question-text">Câu 17: Theo slide 15, độ tương đồng giữa user và item được tính bằng dot-product nhằm mục đích gì?</p><ul class="options"><li>A. Tăng độ chính xác.</li><li class="correct-answer">B. Giảm thời gian tính toán.</li><li>C. Dễ triển khai.</li><li>D. Tăng khả năng giải thích.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 15 ghi rõ: "...tính bằng dot-product nhằm giảm thời gian tính toán."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 18: Hàm mục tiêu trong slide 15 được xây dựng dựa trên nguyên tắc gì?</p><ul class="options"><li>A. Tối thiểu hóa sai số bình phương trung bình (MSE).</li><li class="correct-answer">B. Tối đa hóa xác suất của các bài báo người dùng đã click (positive) so với các bài báo họ chưa click (negative).</li><li>C. Tối đa hóa MRR.</li><li>D. Tối thiểu hóa AUC.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức `-Σ log( exp(...) / (exp(...) + Σ exp(...)) )` là dạng negative log-likelihood của hàm softmax. Nó huấn luyện mô hình để gán xác suất cao hơn cho item dương `c_i^p` so với các item âm `c_{i,k}^n`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 19: Kỹ thuật "mask" `u_l = M * W_u[u]` được áp dụng cho thành phần nào của biểu diễn người dùng?</p><ul class="options"><li>A. Sở thích ngắn hạn.</li><li class="correct-answer">B. Sở thích dài hạn.</li><li>C. Cả hai.</li><li>D. Không áp dụng cho thành phần nào.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 15 mô tả kỹ thuật này để giải quyết vấn đề "Không phải user nào cũng có long-term", do đó nó được áp dụng cho biểu diễn dài hạn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 20: (Điền đáp án) Theo bảng dữ liệu MSN News (slide 13), có bao nhiêu mẫu dương (# positive samples)?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">489,644</span></p><p><b>💡 Giải thích:</b> Bảng ở slide 13, hàng "# positive samples" có giá trị là 489,644.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 21: (Điền đáp án) Theo bảng dữ liệu MSN News (slide 13), có bao nhiêu mẫu âm (# negative samples)?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">6,651,940</span></p><p><b>💡 Giải thích:</b> Bảng ở slide 13, hàng "# negative samples" có giá trị là 6,651,940.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 22: Theo bảng kết quả ở slide 13, mô hình NAML* đạt giá trị AUC là bao nhiêu?</p><ul class="options"><li>A. 0.5880</li><li>B. 0.6114</li><li class="correct-answer">C. 0.6434</li><li>D. 0.5966</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong bảng kết quả, hàng "NAML*", cột "AUC" có giá trị là 0.6434.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 23: Dựa vào kết quả ở slide 13, mô hình NAML* có vượt trội hơn tất cả các baseline không?</p><ul class="options"><li class="correct-answer">A. Có, trên tất cả các độ đo.</li><li>B. Không, nó kém hơn CNN.</li><li>C. Không, nó kém hơn DKN.</li><li>D. Có, nhưng chỉ trên AUC.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Hàng "NAML*" có các giá trị AUC, MRR, nDCG@5, và nDCG@10 đều cao nhất so với tất cả các mô hình baseline khác được liệt kê trong bảng.</p></div></div>
        
        <!-- ... More questions from 24 to 100 ... -->
        <div class="question-block"><p class="question-text">Câu 24: So sánh hai mô hình LSTUR-ini và LSTUR-con (slide 14), chúng khác nhau ở chiến lược kết hợp nào?</p><ul class="options"><li>A. Chiến lược mã hóa tin tức.</li><li class="correct-answer">B. Chiến lược kết hợp sở thích dài hạn và ngắn hạn.</li><li>C. Chiến lược huấn luyện.</li><li>D. Chiến lược đánh giá.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 14 giới thiệu hai mô hình này như hai cách khác nhau để kết hợp long-term và short-term user representation: một cách dùng để khởi tạo (initialization), một cách dùng để ghép nối (concatenation).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 25: (Điền đáp án) Theo slide 22, một trong những nhược điểm của mô hình là "Việc học cùng 1 lúc cho việc biểu diễn item và user có thể dẫn đến...". Hoàn thành câu này.</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">train lâu</span></p><p><b>💡 Giải thích:</b> Slide 22, mục "Nhược điểm", ghi: "Việc học cùng 1 lúc cho việc biểu diễn item và user có thể dẫn đến train lâu."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 26: Theo mô hình NAML, thông tin nào được coi là một "view" của một bài báo?</p><ul class="options"><li>A. Chỉ Title</li><li>B. Chỉ Body</li><li>C. Chỉ Categories</li><li class="correct-answer">D. Cả Title, Body, và Categories</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tên gọi "Attentive Multi-View Learning" và cấu trúc của News Encoder (slide 10) cho thấy mô hình xem xét title, body, và categories như những "khía cạnh" hay "view" khác nhau của cùng một bài báo và học cách tổng hợp thông tin từ tất cả các view này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 27: Trong Title encoder (slide 8), `v` và `v_b` trong công thức `a_i = tanh(v × c_i + v_b)` là gì?</p><ul class="options"><li>A. Vector embedding của từ</li><li class="correct-answer">B. Các tham số được học của tầng attention</li><li>C. Hằng số</li><li>D. Đầu ra của CNN</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 ghi rõ "Với v,v_b là các tham số được học trong quá trình training." Chúng là các trọng số và bias của tầng attention, được học để tính điểm quan trọng cho các context vector `c_i`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 28: (Điền đáp án) Theo slide 13, số lượng người dùng trong bộ dữ liệu MSN News là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">10,000</span></p><p><b>💡 Giải thích:</b> Bảng ở slide 13, hàng "# users" có giá trị là 10,000.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 29: (Điền đáp án) Theo slide 13, số lượng bài báo (# news) trong bộ dữ liệu MSN News là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">42,255</span></p><p><b>💡 Giải thích:</b> Bảng ở slide 13, hàng "# news" có giá trị là 42,255.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 30: "Browsed News" trong sơ đồ ở slide 9 đại diện cho điều gì?</p><ul class="options"><li>A. Các bài báo ứng viên.</li><li class="correct-answer">B. Lịch sử các bài báo mà người dùng đã duyệt.</li><li>C. Các bài báo phổ biến nhất.</li><li>D. Tin tức từ một nguồn cụ thể.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> "Browsed News" là đầu vào cho "User Encoder", cho thấy đây là dữ liệu lịch sử dùng để xây dựng mô hình người dùng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 31: Trong mô hình NAML, tại sao lại cần News Encoder?</p><ul class="options"><li>A. Để mã hóa người dùng.</li><li class="correct-answer">B. Để chuyển đổi thông tin thô của một bài báo (title, body, categories) thành một vector số có ý nghĩa (biểu diễn ẩn).</li><li>C. Để dự đoán lượt click.</li><li>D. Để lưu trữ tin tức.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Máy tính không thể làm việc trực tiếp với văn bản. News Encoder (slide 10) là thành phần có nhiệm vụ đọc và "hiểu" một bài báo, sau đó tóm tắt nó thành một vector `r`, để các thành phần khác của mô hình có thể sử dụng cho việc so sánh và dự đoán.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 32: Vector `u` trong mô hình NAML (slide 11) đại diện cho điều gì?</p><ul class="options"><li>A. Biểu diễn của một bài báo.</li><li class="correct-answer">B. Biểu diễn tổng hợp của một người dùng, dựa trên lịch sử duyệt tin.</li><li>C. Một tham số của mô hình.</li><li>D. ID của người dùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức `u = Σ α_i^n * r_i` cho thấy `u` là kết quả của việc tổng hợp có trọng số các biểu diễn của các bài báo đã xem (`r_i`). Đây chính là vector biểu diễn cho người dùng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 33: (Điền đáp án) Theo slide 7, tin "Olympic Toán" đóng góp nhiều hơn vào việc tìm sở thích vì sao?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Vì có ít người khác cùng xem.</span></p><p><b>💡 Giải thích:</b> Slide 7 giải thích: "tin “Olympic Toán” có ít người khác cùng xem, đóng góp nhiều hơn, cho thấy user quan tâm vấn đề này". Đây là một tin tức "ngách", có tính phân biệt cao.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 34: Mô hình LSTUR có thể xử lý "user cold-start" không?</p><ul class="options"><li>A. Có, một cách hoàn hảo.</li><li class="correct-answer">B. Gặp khó khăn, vì cả biểu diễn dài hạn và ngắn hạn đều yêu cầu lịch sử tương tác để học.</li><li>C. Có, bằng cách sử dụng sở thích của người dùng tương tự.</li><li>D. Có, vì nó có News Encoder.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Mô hình này chủ yếu dựa vào lịch sử click. Nếu một người dùng là mới (không có lịch sử), cả GRU (short-term) và User Embedding (long-term) đều không có dữ liệu để học. Mô hình sẽ không thể đưa ra gợi ý cá nhân hóa tốt cho người dùng này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 35: Mô hình NAML có thể xử lý "item cold-start" không?</p><ul class="options"><li>A. Không.</li><li class="correct-answer">B. Có, vì nó có một News Encoder có thể tạo ra biểu diễn cho một bài báo mới dựa trên title, body, và categories của nó.</li><li>C. Chỉ khi bài báo đó có người click.</li><li>D. Chỉ với các bài báo ngắn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Vì NAML có một bộ mã hóa tin tức (News Encoder) hoạt động dựa trên nội dung của tin, nó có thể tạo ra một vector biểu diễn cho một bài báo hoàn toàn mới mà chưa có ai đọc. Vector này sau đó có thể được so sánh với các vector của người dùng để đưa ra gợi ý.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 36: (Điền đáp án) Theo slide 17, mô hình DKN có giá trị nDCG@10 là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">39.59 ± 0.67</span></p><p><b>💡 Giải thích:</b> Trong bảng ở slide 17, hàng "DKN", cột "nDCG@10", giá trị là 39.59 ± 0.67.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 37: (Điền đáp án) Theo slide 17, mô hình DeepFM có giá trị MRR là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">27.01 ± 0.20</span></p><p><b>💡 Giải thích:</b> Trong bảng ở slide 17, hàng "DeepFM", cột "MRR", giá trị là 27.01 ± 0.20.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 38: Theo Figure 5 (slide 18), phương pháp nào học biểu diễn ngắn hạn kém hiệu quả nhất?</p><ul class="options"><li class="correct-answer">A. Average</li><li>B. Attention</li><li>C. LSTM</li><li>D. GRU</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong Figure 5, cột màu vàng (Average) là cột thấp nhất, cho thấy phương pháp đơn giản nhất này cho hiệu suất kém nhất so với các mô hình tuần tự phức tạp hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 39: Theo Figure 6 (slide 19), giữa LSTUR-ini và LSTUR-con, mô hình nào cho kết quả tốt hơn?</p><ul class="options"><li>A. LSTUR-ini</li><li class="correct-answer">B. LSTUR-con</li><li>C. Chúng tương đương.</li><li>D. Không thể kết luận.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ, cụm cột của LSTUR-con đều cao hơn một chút so với cụm cột của LSTUR-ini, cho thấy chiến lược ghép nối (concatenation) hiệu quả hơn trong thực nghiệm này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 40: (Điền đáp án) Theo slide 21, khi xác suất mask p = 0.0, độ đo nDCG@10 của mô hình LSTUR-ini là khoảng bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">32.0</span></p><p><b>💡 Giải thích:</b> Nhìn vào biểu đồ (a) LSTUR-ini, tại điểm `p=0.0` trên trục hoành, đường cong màu xanh dương (nDCG@10) có giá trị là 32.0.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 41: Theo slide 21, giá trị tối ưu của xác suất mask `p` nằm trong khoảng nào?</p><ul class="options"><li>A. 0.0 - 0.2</li><li class="correct-answer">B. 0.3 - 0.6</li><li>C. 0.7 - 0.9</li><li>D. Luôn là 1.0</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các đường cong hiệu suất trên cả hai biểu đồ đều đạt đỉnh khi `p` nằm trong khoảng từ 0.3 đến 0.6, cho thấy việc "che" đi một phần sở thích dài hạn với một xác suất vừa phải là tốt nhất.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 42: (Điền đáp án) Theo slide 22, ưu điểm của mô hình là khai thác được cả long-term và loại sở thích nào nữa?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">short-term</span></p><p><b>💡 Giải thích:</b> Slide 22, mục "Ưu điểm", ghi: "Khai thác được long-term và short-term".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 43: Toàn bộ các mô hình được so sánh trong slide 17 đều thuộc loại gợi ý nào?</p><ul class="options"><li>A. Gợi ý dựa trên tri thức.</li><li class="correct-answer">B. Các phương pháp gợi ý tin tức được cá nhân hóa, chủ yếu dựa trên mô hình.</li><li>C. Lọc cộng tác dựa trên láng giềng.</li><li>D. Gợi ý không cá nhân hóa.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tất cả các mô hình được liệt kê (LibFM, DeepFM, CNN, DKN, GRU, LSTUR) đều là các thuật toán học máy hoặc deep learning được thiết kế để học sở thích từ dữ liệu và đưa ra gợi ý được cá nhân hóa.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 44: Tại sao mô hình NAML lại có lợi thế hơn các mô hình chỉ dùng CNN hoặc chỉ dùng GRU?</p><ul class="options"><li>A. Vì nó đơn giản hơn.</li><li class="correct-answer">B. Vì nó kết hợp có hệ thống nhiều nguồn thông tin và nhiều kỹ thuật (multi-view, attention, v.v.) vào một kiến trúc duy nhất, trong khi các mô hình khác có thể chỉ tập trung vào một khía cạnh.</li><li>C. Vì nó sử dụng nhiều dữ liệu hơn.</li><li>D. Vì nó được huấn luyện trên GPU mạnh hơn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sức mạnh của NAML đến từ kiến trúc tổng hợp của nó. Nó không chỉ sử dụng CNN để mã hóa văn bản mà còn xem xét các "view" khác nhau (body, categories), và quan trọng nhất là nó có User Encoder để nắm bắt sở thích người dùng, điều mà một mô hình CNN đơn thuần không có.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 45: Nếu một người dùng có sở thích dài hạn rất ổn định và ít thay đổi, giá trị mask probability `p` nào có thể là tối ưu cho người dùng đó?</p><ul class="options"><li class="correct-answer">A. Một giá trị `p` thấp (gần 0).</li><li>B. Một giá trị `p` cao (gần 1).</li><li>C. `p = 0.5`.</li><li>D. `p` không ảnh hưởng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Nếu sở thích dài hạn rất đáng tin cậy, ta sẽ không muốn "che" nó đi. Một giá trị `p` thấp có nghĩa là xác suất để `M=1` (giữ lại sở thích dài hạn) sẽ cao, điều này là hợp lý.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 46: Mô hình NAML có xem xét đến thứ tự các bài báo mà người dùng đã đọc không?</p><ul class="options"><li>A. Không, nó chỉ xem chúng như một tập hợp.</li><li class="correct-answer">B. Có, thông qua User Encoder, nhưng nó không sử dụng một mô hình tuần tự như GRU/LSTM. Nó dùng attention trực tiếp trên các item đã duyệt.</li><li>C. Có, nó sử dụng GRU/LSTM.</li><li>D. Chỉ xem xét bài báo cuối cùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Không giống như mô hình LSTUR sử dụng GRU, mô hình NAML trong slide 9-11 sử dụng một cơ chế attention trực tiếp trên tập hợp các bài báo đã duyệt (`D1` đến `DN`) để tạo ra biểu diễn người dùng `u`. Cơ chế này có tính đến sự quan trọng của các bài báo nhưng không tường minh mô hình hóa thứ tự tuần tự như GRU.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 47: (Điền đáp án) Theo slide 8, vector đại diện cho title `e_t` được lấy bằng cách nào?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Tổng có trọng số của các c_i</span></p><p><b>💡 Giải thích:</b> Slide 8 có ghi: "Sau cùng, vector đại diện cho title được lấy tổng có trọng số của các c_i" và đưa ra công thức tương ứng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 48: Trong sơ đồ NAML (slide 10), `q_t`, `q_b`, `q_c` là gì?</p><ul class="options"><li>A. Biểu diễn của title, body, categories.</li><li class="correct-answer">B. Các vector truy vấn (query vectors) của cơ chế attention, được học để xác định tầm quan trọng tương đối của title, body, và categories.</li><li>C. Các hằng số.</li><li>D. Đầu ra của mô hình.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cơ chế attention, `q` thường đại diện cho một vector truy vấn. Ở đây, `q_t`, `q_b`, `q_c` được dùng để tính điểm attention cho từng "view" (title, body, categories), giúp mô hình học được rằng đối với một số người dùng hoặc ngữ cảnh, title có thể quan trọng hơn body và ngược lại.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 49: Nếu một người dùng không có lịch sử duyệt web (user mới), User Encoder của NAML sẽ trả về kết quả gì?</p><ul class="options"><li>A. Một vector lỗi.</li><li class="correct-answer">B. Một vector không hoặc một vector trung bình chung cho tất cả người dùng, vì không có "Browsed News" để tính toán.</li><li>C. Biểu diễn của người dùng tương tự.</li><li>D. Một vector ngẫu nhiên.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Nếu tập "Browsed News" rỗng, cơ chế attention trong User Encoder sẽ không có đầu vào. Hệ thống sẽ phải có một chiến lược dự phòng, ví dụ như sử dụng một vector mặc định, để có thể tiếp tục quá trình dự đoán (mặc dù kết quả sẽ không được cá nhân hóa).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 50: Nhìn chung, case study NAML nhấn mạnh tầm quan trọng của việc gì trong gợi ý tin tức?</p><ul class="options"><li>A. Tốc độ hệ thống.</li><li class="correct-answer">B. Khai thác thông tin từ nhiều khía cạnh (multi-view) của một bài báo và sử dụng attention để tổng hợp chúng một cách thông minh.</li><li>C. Chỉ sử dụng title.</li><li>D. Sử dụng các mô hình đơn giản.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tên gọi "Attentive Multi-View Learning" và toàn bộ cấu trúc của NAML đều xoay quanh ý tưởng cốt lõi này. Nó cho thấy việc chỉ nhìn vào một khía cạnh (ví dụ: chỉ title) là không đủ, và việc kết hợp nhiều view khác nhau bằng một cơ chế thông minh (attention) có thể mang lại hiệu quả vượt trội.</p></div></div>
        
        <div class="question-block"><p class="question-text">Câu 51: (Điền đáp án) Theo slide 22, một ưu điểm của mô hình là "Biểu diễn được đủ thông tin của ...". Điền vào chỗ trống.</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">news</span></p><p><b>💡 Giải thích:</b> Slide 22 liệt kê ưu điểm đầu tiên là "Biểu diễn được đủ thông tin của news", nhờ vào kiến trúc multi-view.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 52: So sánh sơ đồ LSTUR-ini (slide 14a) và NAML (slide 9), điểm khác biệt lớn nhất trong cách chúng mô hình hóa người dùng là gì?</p><ul class="options"><li class="correct-answer">A. LSTUR-ini sử dụng GRU để nắm bắt thứ tự tuần tự, trong khi NAML sử dụng attention trực tiếp trên tập hợp các tin đã duyệt.</li><li>B. NAML sử dụng GRU, LSTUR-ini sử dụng attention.</li><li>C. LSTUR-ini không có biểu diễn dài hạn.</li><li>D. NAML không có News Encoder.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là sự khác biệt kiến trúc cốt lõi. LSTUR-ini dùng GRU để xử lý chuỗi hành vi, nhấn mạnh vào thứ tự. NAML dùng attention để xác định tầm quan trọng của từng tin trong lịch sử, không phụ thuộc chặt chẽ vào thứ tự.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 53: Theo slide 16, bộ dữ liệu MSN có "Avg. # of words per title" là bao nhiêu?</p><ul class="options"><li>A. 25,000</li><li>B. 22,938</li><li class="correct-answer">C. 9.98</li><li>D. 18.74</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Bảng ở slide 16, hàng "Avg. # of words per title", có giá trị là 9.98.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 54: Theo slide 16, bộ dữ liệu MSN có "Avg. # of words per body" là bao nhiêu?</p><ul class="options"><li>A. 9.98</li><li>B. 492,185</li><li class="correct-answer">C. 730.72</li><li>D. 42,255</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Bảng ở slide 13 (đánh số nhầm thành 16) của bài báo NAML cho thấy #news là 42,255 và avg. # words per body là 730.72.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 55: (Điền đáp án) Trong hàm mục tiêu của LSTUR (slide 15), `s(u, c_x)` là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Độ tương đồng giữa user u và item i được tính bằng dot-product.</span></p><p><b>💡 Giải thích:</b> Slide 15 định nghĩa rõ ràng: "Độ tương đồng giữa user u và item i được tính bằng dot-product" và đưa ra công thức `s(u, c_x) = u^T * e_x`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 56: Kỹ thuật negative sampling trong hàm mục tiêu của LSTUR (slide 15) giúp mô hình làm gì?</p><ul class="options"><li>A. Chỉ học từ các tin tức người dùng thích.</li><li class="correct-answer">B. Học cách phân biệt giữa tin tức người dùng đã click (positive) và các tin tức khác mà họ chưa click (negative).</li><li>C. Bỏ qua tất cả các tin tức người dùng chưa click.</li><li>D. Tăng tốc độ dự đoán.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Bằng cách tối ưu hóa để điểm `s(u, c_i^p)` cao hơn các điểm `s(u, c_{i,k}^n)`, mô hình học được ranh giới quyết định giữa các item liên quan và không liên quan cho một người dùng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 57: Trong mô hình NAML, tại sao lại cần có `q_n` (query vector) trong User Encoder?</p><ul class="options"><li>A. Để mã hóa tin tức.</li><li class="correct-answer">B. Để tính điểm attention cho các bài báo trong lịch sử, giúp xác định bài nào quan trọng hơn trong việc hình thành sở thích của người dùng.</li><li>C. Để mã hóa topic.</li><li>D. Để dự đoán click.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tương tự như `q_t` trong News Encoder, `q_n` trong User Encoder (slide 11) là một vector truy vấn được học, dùng để tương tác với các biểu diễn tin tức `r_i` và tính toán trọng số attention `α_i^n`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 58: Mô hình LSTUR có thể được xem là một ví dụ của loại gợi ý nào?</p><ul class="options"><li>A. Gợi ý dựa trên láng giềng.</li><li class="correct-answer">B. Gợi ý tuần tự (Sequential Recommendation).</li><li>C. Gợi ý không cá nhân hóa.</li><li>D. Gợi ý dựa trên ma trận.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Việc sử dụng GRU để mô hình hóa lịch sử click theo thứ tự thời gian cho thấy LSTUR thuộc vào lớp các mô hình gợi ý tuần tự, một lĩnh vực con quan trọng trong hệ gợi ý.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 59: (Điền đáp án) Theo slide 6, các thông tin trong các phần khác nhau của bài báo được dùng để tìm ra cái gì của người dùng?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">sở thích và xu hướng đọc báo</span></p><p><b>💡 Giải thích:</b> Slide 6 ghi: "Các thông tin này được dung để tìm ra sở thích và xu hướng đọc báo của người dùng."</p></div></div>
        <div class="question-block"><p class="question-text">Câu 60: Theo slide 17, mô hình GRU (dùng làm baseline) có giá trị nDCG@5 là bao nhiêu?</p><ul class="options"><li>A. 62.69 ± 0.16</li><li>B. 30.24 ± 0.13</li><li class="correct-answer">C. 32.56 ± 0.17</li><li>D. 40.55 ± 0.13</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong bảng ở slide 17, hàng "GRU", cột "nDCG@5", giá trị là 32.56 ± 0.17.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 61: "Padding" trong sơ đồ News Encoder của NAML (slide 10) có chức năng tương tự như trong mô hình nào?</p><ul class="options"><li>A. User Encoder của NAML</li><li class="correct-answer">B. Title Encoder của LSTUR (slide 8)</li><li>C. Topic Encoder của LSTUR</li><li>D. Click Predictor của NAML</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Cả hai mô hình đều sử dụng CNN để xử lý title. Kỹ thuật padding là một bước chuẩn bị cần thiết cho phép toán tích chập trong CNN trên dữ liệu tuần tự, như đã giải thích ở câu 63.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 62: Nếu một user chỉ đọc 10 bài báo, User Encoder của LSTUR (slide 12) sẽ xử lý chuỗi này như thế nào?</p><ul class="options"><li>A. Sẽ báo lỗi vì cần 30 bài.</li><li class="correct-answer">B. Sẽ xử lý chuỗi 10 bài báo đó bằng GRU.</li><li>C. Sẽ lấy ngẫu nhiên 20 bài báo khác.</li><li>D. Sẽ chỉ dùng biểu diễn dài hạn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> GRU có khả năng xử lý các chuỗi có độ dài thay đổi. Việc nói "sử dụng 30 bài gần nhất" là để giới hạn độ dài tối đa, nhưng nó hoàn toàn có thể hoạt động với các chuỗi ngắn hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 63: Theo slide 17, trong các baseline, mô hình nào là không dựa trên deep learning?</p><ul class="options"><li class="correct-answer">A. LibFM</li><li>B. DeepFM</li><li>C. Wide & Deep</li><li>D. DSSM</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> LibFM là một thư viện cho Factorization Machines, một mô hình hồi quy tuyến tính tổng quát hóa. Các mô hình còn lại đều là các kiến trúc deep learning.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 64: Theo Figure 4 (slide 18), LTUR là viết tắt của "long-term user representations". Mô hình này có thể tương đương với phương pháp nào?</p><ul class="options"><li>A. Gợi ý dựa trên láng giềng</li><li class="correct-answer">B. Matrix Factorization cơ bản</li><li>C. Gợi ý dựa trên nội dung</li><li>D. Gợi ý ngẫu nhiên</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Một mô hình chỉ dựa trên biểu diễn dài hạn (một vector embedding cố định cho mỗi user) sẽ rất giống với Matrix Factorization, nơi điểm số được tính bằng tích vô hướng của vector user và vector item.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 65: Theo Figure 4 (slide 18), STUR là viết tắt của "short-term user representations". Mô hình này có thể tương đương với phương pháp nào?</p><ul class="options"><li>A. Matrix Factorization</li><li class="correct-answer">B. Một mô hình gợi ý tuần tự (ví dụ chỉ dùng GRU)</li><li>C. Gợi ý dựa trên độ phổ biến</li><li>D. Gợi ý dựa trên tri thức</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> STUR chỉ tập trung vào các hành vi gần đây, đây là đặc trưng của các mô hình gợi ý tuần tự. Trong bài báo này, nó được hiện thực hóa bằng GRU.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 66: (Điền đáp án) Theo slide 19, trong biểu đồ (b), độ đo được sử dụng là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">nDCG@10</span></p><p><b>💡 Giải thích:</b> Chú thích dưới biểu đồ (b) trên slide 19 ghi rõ là "(b) nDCG@10".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 67: Kết quả ở Figure 6 (slide 19) cho thấy điều gì về hiệu quả của CNN so với LSTM trong việc mã hóa title?</p><ul class="options"><li>A. LSTM luôn tốt hơn CNN.</li><li class="correct-answer">B. CNN (đặc biệt là CNN+Att) cho hiệu suất tốt hơn LSTM trong thực nghiệm này.</li><li>C. Chúng cho kết quả tương đương.</li><li>D. Không thể so sánh.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ, các cột màu xanh (CNN và CNN+Att) đều cao hơn các cột màu vàng (LSTM và LSTM+Att) tương ứng, cho thấy CNN là lựa chọn tốt hơn để trích xuất đặc trưng từ các chuỗi văn bản ngắn như tiêu đề tin tức.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 68: Theo Figure 7 (slide 20), việc không sử dụng thông tin topic/subtopic (None) cho kết quả như thế nào so với việc sử dụng chúng?</p><ul class="options"><li>A. Tốt hơn.</li><li class="correct-answer">B. Kém hơn đáng kể.</li><li>C. Tương đương.</li><li>D. Không có sự khác biệt.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Cột màu vàng (None) là cột thấp nhất trong tất cả các cụm, cho thấy việc bổ sung thông tin về category (topic/subtopic) là một cải tiến quan trọng, giúp mô hình hiểu rõ hơn về nội dung bài báo.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 69: (Điền đáp án) Theo slide 21, các độ đo trên trục tung của biểu đồ (b) LSTUR-con là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">AUC, MRR, nDCG@5, nDCG@10</span></p><p><b>💡 Giải thích:</b> Chú thích (legend) ở phía trên của biểu đồ (b) liệt kê 4 độ đo này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 70: Tại sao mức độ phổ biến của bài báo lại có thể ảnh hưởng đến việc xác định sở thích (slide 7)?</p><ul class="options"><li>A. Vì các bài báo phổ biến luôn tốt hơn.</li><li class="correct-answer">B. Vì việc một người dùng đọc một bài báo rất phổ biến (mà ai cũng đọc) ít cho thấy sở thích đặc trưng của họ hơn là việc họ đọc một bài báo "ngách" ít người biết đến.</li><li>C. Vì các bài báo phổ biến khó mã hóa hơn.</li><li>D. Vì mức độ phổ biến không ảnh hưởng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là ý tưởng tương tự như IDF trong TF-IDF. Một hành động trên một item phổ biến có giá trị thông tin thấp hơn một hành động trên một item hiếm. Đọc tin "thời tiết" không nói lên nhiều điều, nhưng đọc tin "Olympic Toán" cho thấy một sự quan tâm cụ thể.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 71: Theo slide 8, các tham số `v, v_b, C, b` trong Title encoder được học như thế nào?</p><ul class="options"><li>A. Được gán cố định.</li><li class="correct-answer">B. Được học trong quá trình training.</li><li>C. Được tải từ một mô hình đã huấn luyện trước.</li><li>D. Được tính toán trực tiếp.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 ghi rõ "Với v,v_b là các tham số được học trong quá trình training." Các ma trận bộ lọc `C` và bias `b` của CNN cũng là các tham số được học.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 72: Theo slide 9, các embedding cho topic và subtopic được học như thế nào?</p><ul class="options"><li>A. Sử dụng pre-train.</li><li class="correct-answer">B. Được học trong quá trình training.</li><li>C. Được gán ngẫu nhiên và giữ cố định.</li><li>D. Dùng one-hot encoding.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 ghi: "topic và subtopic categories được embedding ( được học trong quá trình training )".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 73: Trong LSTUR-ini (slide 14a), vector `u_i` từ User Embedding có vai trò gì?</p><ul class="options"><li>A. Là đầu vào ở mỗi bước của GRU.</li><li class="correct-answer">B. Là trạng thái ẩn khởi tạo cho GRU.</li><li>C. Là đầu ra cuối cùng của mô hình.</li><li>D. Được ghép nối với đầu ra của GRU.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ cho thấy `u_i` được đưa vào đầu chuỗi GRU, hoạt động như trạng thái ẩn `h_0`, từ đó định hướng cho quá trình xử lý chuỗi các tin tức đã duyệt.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 74: Trong LSTUR-con (slide 14b), vector `u_s` (long-term) được lấy từ đâu?</p><ul class="options"><li>A. Từ GRU.</li><li class="correct-answer">B. Từ User Embedding.</li><li>C. Từ News Encoder.</li><li>D. Từ Dot Product.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ cho thấy `u_s` được lấy trực tiếp từ "User Embedding", đại diện cho sở thích dài hạn, trong khi `u_t` (không được gán nhãn, nhưng là đầu ra của GRU) đại diện cho sở thích ngắn hạn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 75: (Điền đáp án) Theo slide 15, biến `M` trong công thức `u_l = M * W_u[u]` tuân theo phân phối xác suất nào?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Bernoulli - B(1, 1-p)</span></p><p><b>💡 Giải thích:</b> Slide 15 ghi rõ `M ~ B(1, 1-p)`, tức là M tuân theo phân phối Bernoulli với xác suất thành công (M=1) là `1-p`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 76: (Điền đáp án) Theo slide 17, mô hình LSTUR-ini có giá trị MRR là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">30.98 ± 0.32</span></p><p><b>💡 Giải thích:</b> Trong bảng, hàng "LSTUR-ini", cột "MRR", giá trị là 30.98 ± 0.32.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 77: (Điền đáp án) Theo slide 17, mô hình DSSM có giá trị AUC là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">58.43 ± 0.58</span></p><p><b>💡 Giải thích:</b> Trong bảng, hàng "DSSM", cột "AUC", giá trị là 58.43 ± 0.58.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 78: (Điền đáp án) Theo slide 18, trong Figure 5, "Average" là phương pháp học biểu diễn ngắn hạn có hiệu suất như thế nào so với các phương pháp khác?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Kém nhất</span></p><p><b>💡 Giải thích:</b> Cột màu vàng (Average) là cột thấp nhất trong cả hai biểu đồ, cho thấy nó là baseline yếu nhất.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 79: (Điền đáp án) Theo slide 19, trong Figure 6, việc thêm Attention vào CNN (CNN+Att) có cải thiện hiệu suất so với CNN thường không?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Có</span></p><p><b>💡 Giải thích:</b> Trong cả hai biểu đồ, cột màu xanh lá đậm (CNN+Att) đều cao hơn cột màu xanh lá nhạt (CNN).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 80: (Điền đáp án) Theo slide 20, trong Figure 7, phiên bản nào của News Encoder cho kết quả nDCG@10 cao nhất?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">+Both (sử dụng cả Topic và Subtopic)</span></p><p><b>💡 Giải thích:</b> Cột màu xanh lá đậm (+Both) là cột cao nhất trong biểu đồ (b) nDCG@10.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 81: Nhược điểm "Chưa khai thác được entity" (slide 22) có nghĩa là gì?</p><ul class="options"><li>A. Mô hình không biết người dùng là ai.</li><li class="correct-answer">B. Mô hình chưa sử dụng thông tin về các thực thể được đặt tên (named entities) như tên người, địa điểm, tổ chức trong bài báo để làm giàu biểu diễn.</li><li>C. Mô hình không thể phân biệt các bài báo.</li><li>D. Mô hình không có thực thể.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các thực thể (entities) mang rất nhiều thông tin ngữ nghĩa. Ví dụ, hai bài báo cùng nhắc đến "Donald Trump" và "Nhà Trắng" có khả năng liên quan đến nhau. Việc khai thác các thực thể này có thể giúp mô hình hiểu nội dung tin tức sâu sắc hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 82: Trong sơ đồ NAML (slide 10), vector `r` cuối cùng được tạo ra bằng cách nào?</p><ul class="options"><li>A. Ghép nối `r^t`, `r^b`, `r^c`.</li><li class="correct-answer">B. Tổng có trọng số của `r^t`, `r^b`, `r^c` sử dụng các trọng số attention `α_t`, `α_b`, `α_c`.</li><li>C. Chỉ lấy `r^t`.</li><li>D. Lấy trung bình của 3 vector.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ cho thấy 3 vector biểu diễn cho 3 view được đưa qua một tầng attention để tính toán các trọng số, sau đó được tổng hợp lại để tạo ra vector `r` cuối cùng. Đây là cơ chế "attentive multi-view".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 83: So với HyperNews, mô hình LSTUR tập trung hơn vào khía cạnh nào?</p><ul class="options"><li>A. Multi-task learning.</li><li class="correct-answer">B. Mô hình hóa sự tuần tự và sự kết hợp giữa sở thích dài hạn và ngắn hạn.</li><li>C. Mã hóa nội dung bài báo.</li><li>D. Dự đoán thời gian đọc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Tên gọi "Long- and Short-term User Representations" và việc sử dụng GRU cho thấy trọng tâm chính của LSTUR là mô hình hóa các khía cạnh thời gian trong sở thích của người dùng, trong khi HyperNews tập trung vào việc học đa nhiệm.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 84: Mô hình LSTUR có sử dụng thông tin từ nội dung (body) của bài báo không?</p><ul class="options"><li>A. Có, nó là đầu vào chính.</li><li class="correct-answer">B. Không, theo slide 7, News encoder của nó chỉ học từ title và categories.</li><li>C. Có, nhưng chỉ cho sở thích dài hạn.</li><li>D. Có, nhưng chỉ cho sở thích ngắn hạn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 mô tả News Encoder của mô hình này chỉ gồm "Title encoder" và "Topic encoder", không đề cập đến việc xử lý nội dung chi tiết của bài báo (body).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 85: Theo slide 21, hiệu suất của LSTUR-con trên độ đo AUC thay đổi như thế nào khi `p` tăng từ 0.0 đến 0.9?</p><ul class="options"><li>A. Tăng đều.</li><li>B. Giảm đều.</li><li class="correct-answer">C. Tăng đến khoảng `p=0.4` rồi giảm dần.</li><li>D. Không đổi.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Nhìn vào đường cong màu xanh lá cây trong biểu đồ (b) LSTUR-con, ta thấy nó đạt đỉnh ở khoảng `p` từ 0.3 đến 0.5 và sau đó đi xuống.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 86: Hàm mục tiêu của LSTUR có xem xét đến mức độ tương đồng giữa các item âm không?</p><ul class="options"><li>A. Có, nó cố gắng làm cho chúng khác nhau.</li><li class="correct-answer">B. Không, nó chỉ cố gắng làm cho điểm của item dương cao hơn điểm của các item âm được lấy mẫu.</li><li>C. Có, nó cố gắng làm cho chúng giống nhau.</li><li>D. Chỉ khi `K > 10`.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Hàm loss ở slide 15 chỉ so sánh item dương với từng item âm một cách riêng rẽ trong hàm softmax. Nó không có thành phần nào để mô hình hóa mối quan hệ giữa các item âm với nhau.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 87: (Điền đáp án) Theo slide 6, tiêu đề bài báo về thể thao là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Astros improve outfield, agree to 2-year deal with Brantley</span></p><p><b>💡 Giải thích:</b> Trong bảng ở slide 6, tại hàng "Title" và cột "Sports", nội dung là "Astros improve outfield, agree to 2-year deal with Brantley".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 88: (Điền đáp án) Theo slide 6, tiêu đề bài báo về giải trí là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">The best games of 2018</span></p><p><b>💡 Giải thích:</b> Trong bảng ở slide 6, tại hàng "Title" và cột "Entertainment", nội dung là "The best games of 2018".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 89: Giả định cơ bản đằng sau việc sử dụng attention trong User Encoder là gì?</p><ul class="options"><li>A. Tất cả các bài báo đã xem đều quan trọng như nhau.</li><li class="correct-answer">B. Các bài báo khác nhau trong lịch sử của người dùng có mức độ ảnh hưởng khác nhau đến sở thích hiện tại của họ.</li><li>C. Chỉ bài báo cuối cùng là quan trọng.</li><li>D. Người dùng không nhớ những gì họ đã đọc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Cơ chế attention được sinh ra chính là để mô hình hóa tầm quan trọng có trọng số. Thay vì lấy trung bình (coi mọi thứ như nhau), attention cho phép mô hình tự học để tập trung vào những phần quan trọng nhất của đầu vào.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 90: Mô hình NAML có sử dụng GRU không?</p><ul class="options"><li class="correct-answer">A. Không, theo các slide được cung cấp, nó sử dụng attention trực tiếp trên các tin đã duyệt.</li><li>B. Có, nó sử dụng GRU cho News Encoder.</li><li>C. Có, nó sử dụng GRU cho User Encoder.</li><li>D. Có, nó sử dụng GRU cho cả hai.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Dựa trên các sơ đồ và mô tả của mô hình NAML trong các slide (slide 9, 10, 11), không có thành phần nào sử dụng GRU. Nó sử dụng CNN cho title/body và attention cho việc tổng hợp.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 91: Tại sao việc sử dụng Dot-product để tính điểm tương đồng lại nhanh hơn Cosine Similarity?</p><ul class="options"><li>A. Vì nó là một phép toán phức tạp hơn.</li><li class="correct-answer">B. Vì nó không cần tính toán độ lớn (norm) của các vector, một phép toán tốn kém hơn.</li><li>C. Vì nó luôn cho kết quả tốt hơn.</li><li>D. Không có sự khác biệt về tốc độ.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Cosine Similarity = Dot-product / (Norm_A * Norm_B). Phép tính norm yêu cầu tính tổng bình phương các phần tử rồi lấy căn bậc hai, tốn kém hơn so với chỉ tính tích vô hướng. Trong các hệ thống lớn với hàng triệu lượt dự đoán, sự khác biệt này trở nên đáng kể.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 92: (Điền đáp án) Theo slide 17, mô hình nào có tên là viết tắt của Deep Knowledge-aware Network?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">DKN</span></p><p><b>💡 Giải thích:</b> DKN là một mô hình baseline được liệt kê trong bảng kết quả. Nó là một mô hình nổi tiếng về gợi ý tin tức có sử dụng đồ thị tri thức.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 93: Trong LSTUR, nếu một người dùng đã đọc 5 bài báo, GRU sẽ chạy qua bao nhiêu bước thời gian để tạo ra biểu diễn ngắn hạn?</p><ul class="options"><li>A. 30</li><li class="correct-answer">B. 5</li><li>C. 1</li><li>D. Không chạy bước nào.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> GRU xử lý chuỗi đầu vào một cách tuần tự. Nếu chuỗi lịch sử có 5 bài báo, GRU sẽ thực hiện 5 bước cập nhật trạng thái ẩn, mỗi bước cho một bài báo.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 94: Mô hình này có thể gợi ý các bài báo mà người dùng chưa từng thấy các chủ đề tương tự trước đây không?</p><ul class="options"><li>A. Không, nó bị giới hạn bởi lịch sử người dùng.</li><li class="correct-answer">B. Có, thông qua biểu diễn dài hạn (user embedding), mô hình có thể học được các sở thích ẩn hoặc mối liên hệ với những người dùng khác có sở thích tương tự (một cách ngầm ẩn).</li><li>C. Chỉ khi người dùng yêu cầu.</li><li>D. Chỉ khi bài báo đó rất phổ biến.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Mặc dù mô hình chủ yếu dựa trên nội dung và lịch sử cá nhân, nhưng bản chất của việc học các vector embedding trong một không gian chung (user và item) cho phép nó có một số đặc tính của lọc cộng tác. Nếu vector dài hạn của user A gần với vector của user B, mô hình có thể gợi ý cho A những thứ B thích, ngay cả khi A chưa từng thể hiện sự quan tâm trực tiếp.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 95: (Điền đáp án) Theo slide 21, giá trị nDCG@5 cao nhất mà LSTUR-con đạt được là khoảng bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">33.0</span></p><p><b>💡 Giải thích:</b> Trong biểu đồ (b) LSTUR-con, đường cong màu xanh da trời (nDCG@5) đạt đỉnh ở giá trị khoảng 33.0.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 96: Theo bạn, đâu là hạn chế lớn nhất của mô hình LSTUR?</p><ul class="options"><li>A. Sử dụng deep learning quá phức tạp.</li><li class="correct-answer">B. Việc không mô hình hóa nội dung (body) của bài báo có thể làm mất đi một lượng lớn thông tin ngữ nghĩa quan trọng.</li><li>C. Sử dụng GRU quá chậm.</li><li>D. Không thể xử lý user mới.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> So với NAML, việc LSTUR chỉ dựa vào title và categories (slide 7) là một hạn chế đáng kể, vì nội dung chi tiết của bài báo chứa đựng phần lớn thông tin để xác định sự liên quan thực sự.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 97: Nếu có hai bài báo có cùng title, topic, và subtopic, News Encoder của LSTUR sẽ tạo ra biểu diễn như thế nào cho chúng?</p><ul class="options"><li>A. Khác nhau hoàn toàn.</li><li class="correct-answer">B. Giống hệt nhau.</li><li>C. Gần giống nhau.</li><li>D. Phụ thuộc vào người dùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Vì News Encoder của LSTUR chỉ dựa trên ba thành phần này, nếu chúng giống hệt nhau, đầu ra của bộ mã hóa cũng sẽ giống hệt nhau. Mô hình này không xem xét đến nội dung chi tiết bên trong.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 98: (Điền đáp án) Theo slide 17, mô hình nào có tên là viết tắt của DeepFM?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Deep Factorization Machine</span></p><p><b>💡 Giải thích:</b> DeepFM là một mô hình gợi ý nổi tiếng kết hợp kiến trúc của Factorization Machines và Deep Neural Networks.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 99: Theo Figure 4 (slide 18), LSTUR-ini và LSTUR-con có hiệu suất như thế nào trên độ đo nDCG@10?</p><ul class="options"><li>A. LSTUR-ini tốt hơn.</li><li class="correct-answer">B. Chúng gần như tương đương nhau.</li><li>C. LSTUR-con tốt hơn.</li><li>D. Không thể so sánh.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Nhìn vào các cột bên phải của biểu đồ, chiều cao của hai cột màu xanh lá (LSTUR-ini và LSTUR-con) là rất sát nhau, cho thấy sự khác biệt về hiệu suất trên nDCG@10 là không đáng kể.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 100: Bài học quan trọng nhất từ việc so sánh LSTUR-ini và LSTUR-con là gì?</p><ul class="options"><li>A. Luôn luôn nên dùng phép ghép nối.</li><li class="correct-answer">B. Cách thức kết hợp các nguồn thông tin (trong trường hợp này là sở thích dài hạn và ngắn hạn) là một lựa chọn thiết kế quan trọng và có thể ảnh hưởng đến hiệu suất của mô hình.</li><li>C. Luôn luôn nên dùng để khởi tạo.</li><li>D. Không có bài học nào.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Việc hai chiến lược kết hợp khác nhau cho ra kết quả khác nhau (dù nhỏ) cho thấy tầm quan trọng của việc thiết kế kiến trúc một cách cẩn thận. Không có một cách "đúng" duy nhất để kết hợp thông tin, và lựa chọn tốt nhất thường phụ thuộc vào bản chất của dữ liệu và bài toán, cần được xác định thông qua thực nghiệm.</p></div></div>    
    </div>
</body>
</html>