<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>100 Câu Hỏi Trắc Nghiệm - Chuyên đề Lọc Cộng Tác (CF)</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            max-width: 900px;
            margin: auto;
            background: #fff;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 0 15px rgba(0,0,0,0.1);
        }
        header {
            text-align: center;
            border-bottom: 2px solid #d9534f;
            margin-bottom: 30px;
            padding-bottom: 20px;
        }
        header h1 {
            color: #d9534f;
            margin: 0;
        }
        header p {
            margin: 5px 0 0;
            font-style: italic;
            color: #555;
        }
        .question-block {
            margin-bottom: 25px;
            padding: 20px;
            border: 1px solid #ddd;
            border-left: 5px solid #d9534f;
            border-radius: 5px;
            background-color: #fdfdfd;
        }
        .question-text {
            font-weight: bold;
            font-size: 1.1em;
            margin-bottom: 15px;
        }
        .options {
            list-style-type: none;
            padding-left: 0;
        }
        .options li {
            margin-bottom: 10px;
            padding: 8px;
            border-radius: 4px;
        }
        .explanation {
            margin-top: 15px;
            padding: 15px;
            background-color: #e9f7ef;
            border: 1px solid #a3d9b8;
            border-radius: 5px;
        }
        .explanation b {
            color: #1d7b46;
        }
        .correct-answer {
            background-color: #dff0d8;
            border-left: 3px solid #3c763d;
        }
        .fill-in-answer {
            font-weight: bold;
            color: #3c763d;
            font-size: 1.2em;
        }
    </style>
</head>
<body>

    <div class="container">
        <header>
            <h1>100 Câu Hỏi Trắc Nghiệm - Chuyên đề Lọc Cộng Tác</h1>
            <p>Dựa trên nội dung slide "Lọc cộng tác (Collaborative filtering)" - Viện CNTT&TT - ĐHBK Hà Nội</p>
        </header>

        <!-- CATEGORY: CF FUNDAMENTALS -->
        <div class="question-block"><p class="question-text">Câu 1: Theo slide 6, ý tưởng cốt lõi của Lọc cộng tác (CF) dựa trên nguyên tắc nào? (Câu gốc: "Dựa trên 'wisdom of the crowd' để gợi ý items")</p>
            <ul class="options">
                <li>A. Dựa trên sự thông thái của chuyên gia.</li>
                <li class="correct-answer">B. Dựa trên sự thông thái của đám đông để gợi ý các mục.</li>
                <li>C. Dựa trên nội dung của các mục.</li>
                <li>D. Dựa trên các quy tắc được định nghĩa trước.</li>
            </ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6, mục "Tiếp cận", nêu rõ rằng Lọc cộng tác "Dựa trên 'wisdom of the crowd' để gợi ý items". "Wisdom of the crowd" có nghĩa là "sự thông thái của đám đông".</p></div>
        </div>
        <div class="question-block"><p class="question-text">Câu 2: Giả thiết cơ bản của Lọc cộng tác là gì theo slide 6?</p><ul class="options"><li>A. Người dùng luôn có sở thích ổn định và không bao giờ thay đổi.</li><li>B. Các sản phẩm có nội dung giống nhau thì sẽ được người dùng thích như nhau.</li><li class="correct-answer">C. Những người dùng có sở thích tương tự trong quá khứ cũng sẽ có sở thích tương tự trong tương lai.</li><li>D. Mọi người dùng đều có sở thích giống nhau.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6, trong phần "Giả thiết và ý tưởng", có ghi: "Users có sở thích tương tự nhau trong quá khứ, cũng sẽ có sở thích tương tự trong tương lai".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 3: Dữ liệu đầu vào (Input) chính cho Lọc cộng tác là gì? (Câu gốc: "Only a matrix of given user-item ratings")</p><ul class="options"><li>A. Mô tả chi tiết của sản phẩm.</li><li>B. Thông tin nhân khẩu học của người dùng.</li><li class="correct-answer">C. Chỉ một ma trận đánh giá người dùng-sản phẩm đã cho.</li><li>D. Một đồ thị tri thức về các sản phẩm.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 nêu rất rõ ràng rằng đầu vào là "Only a matrix of given user-item ratings", tức là chỉ cần ma trận đánh giá user-item.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 4 (Chọn nhiều đáp án): Theo slide 7, các loại đầu ra (Output types) của Lọc cộng tác có thể là:</p><ul class="options"><li class="correct-answer">A. Một dự đoán bằng số chỉ ra mức độ người dùng hiện tại sẽ thích hay không thích một mục nào đó.</li><li>B. Một hồ sơ người dùng đã được cập nhật.</li><li class="correct-answer">C. Một danh sách top-N các mục được đề xuất.</li><li>D. Một báo cáo phân tích về thị trường.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 7 liệt kê hai loại đầu ra: "A (numerical) prediction..." và "A top-N list of recommended items". Cả hai đáp án A và C đều dịch chính xác từ nội dung slide.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 5: Đâu là sự khác biệt chính giữa tiếp cận dựa trên memory (memory-based) và dựa trên mô hình (model-based) trong Lọc cộng tác?</p><ul class="options"><li>A. Memory-based nhanh hơn model-based trong quá trình huấn luyện.</li><li class="correct-answer">B. Memory-based sử dụng trực tiếp toàn bộ ma trận ratings khi dự đoán, còn model-based học một mô hình nhỏ gọn từ ma trận trước.</li><li>C. Model-based luôn chính xác hơn memory-based.</li><li>D. Memory-based chỉ áp dụng cho user-based CF, còn model-based chỉ áp dụng cho item-based CF.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 phân biệt rõ: tiếp cận memory-based (dựa trên láng giềng) "sử dụng trực tiếp ma trận ratings để tìm láng giềng". Ngược lại, tiếp cận model-based "phải xây dựng và huấn luyện mô hình trước", sau đó dùng mô hình này để dự đoán.</p></div></div>
        
        <!-- CATEGORY: USER-BASED CF -->
        <div class="question-block"><p class="question-text">Câu 6: Trong CF dựa trên láng giềng gần của user (user-based CF), bước đầu tiên để dự đoán rating cho item `i` của user Alice là gì?</p><ul class="options"><li>A. Sử dụng đánh giá trung bình của Alice.</li><li class="correct-answer">B. Tìm một tập các user đã thích các item giống Alice trong quá khứ và đã đánh giá item `i`.</li><li>C. Tìm các item tương tự với item `i`.</li><li>D. Tính toán rating trung bình của item `i`.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9 mô tả bài toán user-based CF, bước đầu tiên là "Tìm một tập các users mà đã thích các items giống như Alice trong quá khứ, và đã đánh giá sản phẩm item `i`". Đây chính là bước tìm "láng giềng" (neighbors).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 7: Độ đo tương đồng phổ biến nào được giới thiệu cho user-based CF trong slide 12?</p><ul class="options"><li>A. Cosine Similarity</li><li>B. Jaccard Similarity</li><li class="correct-answer">C. Pearson Correlation</li><li>D. Euclidean Distance</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 12 và 13 đều ghi rõ: "Độ đo tương đồng phổ biến cho user-based CF: Pearson correlation".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 8: Giá trị của Pearson correlation nằm trong khoảng nào?</p><ul class="options"><li>A. 0 và 1</li><li class="correct-answer">B. -1 và 1</li><li>C. 0 và vô cùng</li><li>D. - vô cùng và + vô cùng</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 12 và 13 đề cập: "Possible similarity values between -1 and 1".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 9 (Điền đáp án): Dựa vào bảng dữ liệu ở slide 10, rating trung bình của User1 cho các item đã đánh giá (Item1 đến Item5) là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">2.4</span></p><p><b>💡 Giải thích:</b> User1 đã đánh giá các item với điểm số: 3, 1, 2, 3, 3. Trung bình = (3 + 1 + 2 + 3 + 3) / 5 = 12 / 5 = 2.4.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 10 (Điền đáp án): Xét Alice và User2 trong bảng ở slide 10. Tập các item P được đánh giá bởi cả hai là {Item1, Item2, Item3, Item4}. Rating trung bình của Alice trên tập P này là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">4</span></p><p><b>💡 Giải thích:</b> Alice đã đánh giá các item trong tập P với điểm số: 5, 3, 4, 4. Trung bình = (5 + 3 + 4 + 4) / 4 = 16 / 4 = 4. Đây là giá trị `r_a` trong công thức Pearson.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 11 (Điền đáp án): Vẫn với Alice và User2, rating trung bình của User2 trên tập P {Item1, Item2, Item3, Item4} là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">3.5</span></p><p><b>💡 Giải thích:</b> User2 đã đánh giá các item trong tập P với điểm số: 4, 3, 4, 3. Trung bình = (4 + 3 + 4 + 3) / 4 = 14 / 4 = 3.5. Đây là giá trị `r_b` trong công thức Pearson.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 12: Theo slide 13, độ tương đồng (Pearson) giữa Alice và User2 là 0,00. Điều này có ý nghĩa gì?</p><ul class="options"><li>A. Alice và User2 có sở thích hoàn toàn giống nhau.</li><li>B. Alice và User2 có sở thích hoàn toàn trái ngược nhau.</li><li class="correct-answer">C. Không có mối tương quan tuyến tính nào giữa sở thích của Alice và User2.</li><li>D. Alice và User2 chưa bao giờ đánh giá chung một sản phẩm nào.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Giá trị Pearson correlation bằng 0 cho thấy không có mối quan hệ tuyến tính. Điều này không có nghĩa là họ không có điểm chung, mà là xu hướng đánh giá cao/thấp của họ không đi cùng chiều hoặc ngược chiều một cách nhất quán.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 13: Trong công thức dự đoán ở slide 15, `pred(a,p) = r_a + ...`, thành phần `r_a` (rating trung bình của user a) đại diện cho điều gì?</p><ul class="options"><li>A. Sở thích của láng giềng.</li><li class="correct-answer">B. Mức độ "khó tính" hay "dễ tính" chung của người dùng a (mức độ bias).</li><li>C. Độ phổ biến của sản phẩm p.</li><li>D. Độ tương đồng giữa a và láng giềng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 15 giải thích công thức: "Cộng với mức độ bias của user a". `r_a` chính là baseline (đường cơ sở), thể hiện xu hướng đánh giá chung của người dùng a, trước khi được điều chỉnh bởi ý kiến của những người hàng xóm.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 14 (Điền đáp án): Dựa vào giá trị sim đã tính ở slide 13, nếu chọn User1 (sim=0.85) và User3 (sim=0.70) làm láng giềng để dự đoán rating cho Item5 của Alice. Biết r_Alice = 4.0, r_User1 = 2.4, r_User3 = 2.75. Rating của User1 và User3 cho Item5 lần lượt là 3 và 4. Dự đoán rating của Alice là bao nhiêu (làm tròn 2 chữ số)?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">4.89</span></p><p><b>💡 Giải thích:</b><br/>Tử số = 0.85 * (3 - 2.4) + 0.70 * (4 - 2.75) = 0.85 * 0.6 + 0.70 * 1.25 = 0.51 + 0.875 = 1.385<br/>Mẫu số = |0.85| + |0.70| = 1.55<br/>pred(Alice, Item5) = 4.0 + (1.385 / 1.55) = 4.0 + 0.8935... ≈ 4.89</p></div></div>
        <div class="question-block"><p class="question-text">Câu 15: Theo slide 16, tại sao "đồng thuận trên các items được thích phổ biến sẽ không mang lại nhiều thông tin"?</p><ul class="options"><li>A. Vì các item này thường có rating thấp.</li><li class="correct-answer">B. Vì hầu hết mọi người đều thích chúng, việc hai người cùng thích một item phổ biến không nói lên nhiều về sự tương đồng đặc biệt trong sở thích.</li><li>C. Vì các item này khó tìm.</li><li>D. Vì các item này có variance thấp.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Việc hai người cùng thích một bộ phim bom tấn như "Avatar" không có ý nghĩa bằng việc họ cùng thích một bộ phim độc lập ít người biết. Sự đồng thuận trên các item "ngách" (niche) mới thực sự là tín hiệu mạnh cho thấy sự tương đồng trong sở thích.</p></div></div>

        <!-- CATEGORY: ITEM-BASED CF -->
        <div class="question-block"><p class="question-text">Câu 16: Ý tưởng chính của Lọc cộng tác dựa trên item (item-based CF) là gì?</p><ul class="options"><li>A. Tìm những người dùng có sở thích tương tự.</li><li class="correct-answer">B. Sử dụng độ tương đồng giữa các item để đưa ra gợi ý.</li><li>C. Phân tích nội dung của các item.</li><li>D. Xây dựng một mô hình học máy phức tạp.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 17 nêu rõ ý tưởng: "Sử dụng độ tương đồng giữa các items để đưa ra gợi ý". Ví dụ là tìm các item tương tự Item5, rồi dùng rating của Alice trên các item đó để dự đoán rating cho Item5.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 17: Độ đo tương đồng nào thường được sử dụng và cho kết quả tốt trong item-based CF theo slide 18?</p><ul class="options"><li>A. Pearson Correlation</li><li class="correct-answer">B. Adjusted Cosine Similarity</li><li>C. Jaccard Index</li><li>D. Euclidean Distance</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 18 giới thiệu "Độ đo tương đồng cosin" và công thức điều chỉnh (Adjusted Cosine) và nói rằng nó "Đưa ra kết quả tốt trong lọc dựa trên item-to-item".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 18: Tại sao cần phải "điều chỉnh" (adjust) độ đo Cosine cho item-based CF?</p><ul class="options"><li class="correct-answer">B. Để loại bỏ sự khác biệt trong thang đánh giá của người dùng (ví dụ: một người có xu hướng cho điểm cao, người khác cho điểm thấp).</li><li>A. Để làm cho công thức phức tạp hơn.</li><li>C. Để xử lý các giá trị rating âm.</li><li>D. Để tăng tốc độ tính toán.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức Adjusted Cosine trên slide 18 trừ đi rating trung bình của mỗi người dùng (`r_u`). Thao tác này (mean-centering) giúp chuẩn hóa rating, loại bỏ ảnh hưởng của việc một số người dùng vốn "dễ tính" (luôn cho điểm cao) trong khi người khác "khó tính" (luôn cho điểm thấp).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 19: Theo Herlocker et al. (2002), số lượng láng giềng (items) nên đặt trong khoảng nào để có kết quả tốt trên tập MovieLens?</p><ul class="options"><li>A. 5 đến 10</li><li>B. 10 đến 20</li><li class="correct-answer">C. 20 đến 50</li><li>D. 50 đến 100</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 19 trích dẫn phân tích của Herlocker et al. 2002 và kết luận rằng "số lượng láng giềng nên đặt 20 tới 50".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 20: Tại sao lọc dựa trên item (item-based) thường có khả năng mở rộng (scalable) tốt hơn lọc dựa trên user (user-based) trong các hệ thống lớn như Amazon?</p><ul class="options"><li class="correct-answer">B. Vì mối quan hệ tương đồng giữa các item thường ổn định hơn theo thời gian, cho phép tính toán trước và lưu trữ.</li><li>A. Vì số lượng item luôn ít hơn số lượng user.</li><li>C. Vì công thức tính toán đơn giản hơn.</li><li>D. Vì nó không bị vấn đề dữ liệu thưa.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 20 đề cập đến việc "Tiền xử lý trước như hệ thống Amazon.com", tức là "Tính toán tất cả các cặp tương đồng của items trước". Điều này khả thi vì độ tương đồng giữa hai quyển sách không thay đổi nhiều theo thời gian. Ngược lại, sở thích của người dùng thay đổi liên tục, khiến việc tính toán trước độ tương đồng user-user kém hiệu quả hơn.</p></div></div>
        
        <!-- CATEGORY: SPARSITY & COLD START -->
        <div class="question-block"><p class="question-text">Câu 21: Vấn đề "Cold start" trong Lọc cộng tác xảy ra khi nào?</p><ul class="options"><li>A. Khi hệ thống chạy chậm.</li><li>B. Khi có quá nhiều dữ liệu.</li><li class="correct-answer">C. Khi có người dùng mới hoặc sản phẩm mới chưa có dữ liệu tương tác.</li><li>D. Khi các rating không nhất quán.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 21 định nghĩa vấn đề Cold start là "Khi có users hoặc items mới, làm sao thực hiện được quá trình gợi ý?". Đây là vấn đề cốt lõi vì CF dựa vào dữ liệu lịch sử.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 22 (Chọn nhiều đáp án): Theo slide 21, các cách đơn giản để giải quyết vấn đề cold start cho user mới bao gồm:</p><ul class="options"><li class="correct-answer">A. Yêu cầu/Ép buộc user mới đánh giá một tập item.</li><li>B. Bỏ qua user mới cho đến khi họ có đủ rating.</li><li class="correct-answer">C. Sử dụng phương pháp khác như content-based hoặc non-personalized trong giai đoạn đầu.</li><li class="correct-answer">D. Gán các giá trị mặc định cho các item mà chỉ một trong hai user so sánh đã đánh giá.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 21, mục "Một số cách đơn giản", liệt kê cả ba phương án này như những giải pháp ban đầu cho vấn đề cold start.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 23: Ý tưởng của thuật toán Recursive CF (Zhang and Pu 2007) để xử lý dữ liệu thưa là gì?</p><ul class="options"><li class="correct-answer">A. Áp dụng lọc cộng tác lặp đi lặp lại để dự đoán rating còn thiếu của các hàng xóm, sau đó dùng rating dự đoán đó để gợi ý.</li><li>B. Chỉ sử dụng những hàng xóm có độ tương đồng tuyệt đối.</li><li>C. Xóa bỏ những người dùng có ít rating.</li><li>D. Sử dụng một mô hình đồ thị.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 22 mô tả ý tưởng của Recursive CF: "Áp dụng lọc cộng tác lặp đi lặp lại và dự đoán rating cho item i bởi các hàng xóm" và "Sử dụng rating dự đoán thay vì rating trực tiếp".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 24: Phương pháp "Spreading activation" (Huang et al. 2004) khai thác giả thiết nào để bổ sung thông tin?</p><ul class="options"><li>A. Giả thiết về sự ổn định của sở thích.</li><li>B. Giả thiết về phân phối chuẩn của rating.</li><li class="correct-answer">C. Giả thiết về tính bắc cầu của sở thích.</li><li>D. Giả thiết về sự độc lập của các item.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 23 nói rằng phương pháp này "Khai thác giả thiết bắc cầu sở thích của khách hàng". Tức là nếu A giống B và B giống C, thì có một mối liên hệ nào đó giữa A và C.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 25: Trong phương pháp "Spreading activation", việc Item3 được gợi ý cho User1 thông qua User2 (User1-Item2-User2-Item3) là dựa trên một đường đi có độ dài bao nhiêu?</p><ul class="options"><li>A. 2</li><li class="correct-answer">B. 3</li><li>C. 4</li><li>D. 5</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 24 giải thích: "đường đi với độ dài 3 (User1–Item2–User2–Item3) giữa chúng". Một đường đi được tính bằng số cạnh nối các đỉnh.</p></div></div>

        <!-- CATEGORY: MATRIX FACTORIZATION -->
        <div class="question-block"><p class="question-text">Câu 26: Lọc cộng tác dựa trên phân tích ma trận (Matrix Factorization) thuộc loại tiếp cận nào?</p><ul class="options"><li>A. Dựa trên memory</li><li class="correct-answer">B. Dựa trên mô hình</li><li>C. Dựa trên quy tắc</li><li>D. Hybrid</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 26 giới thiệu "Lọc cộng tác dựa trên phân tích ma trận" và ngay dòng đầu tiên khẳng định "Là tiếp cận dựa trên mô hình".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 27: Giả thiết chính của phương pháp Phân tích ma trận là gì?</p><ul class="options"><li>A. Người dùng và sản phẩm có thể được mô tả bằng các thuộc tính tường minh.</li><li class="correct-answer">B. Người dùng và sản phẩm có thể được biểu diễn trên cùng một không gian ẩn (latent space) thể hiện các đặc trưng của chúng.</li><li>C. Mọi người dùng đều có rating trung bình như nhau.</li><li>D. Ma trận rating là một ma trận dày đặc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 26 nêu rõ giả thiết: "users và items được biểu diễn trên cùng không gian ẩn mà thể hiện đặc trưng của chúng".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 28: Trong sơ đồ trên slide 27/28, trục ngang biểu diễn một chiều của không gian ẩn có thể được hiểu là gì?</p><ul class="options"><li>A. Mức độ hài hước</li><li class="correct-answer">B. Mức độ hướng tới khán giả nam so với nữ</li><li>C. Mức độ phổ biến của phim</li><li>D. Năm sản xuất phim</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trục ngang được gán nhãn "Geared toward females" ở bên trái và "Geared toward males" ở bên phải, cho thấy nó biểu diễn một đặc trưng ẩn liên quan đến đối tượng khán giả mục tiêu.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 29: Trong Matrix Factorization, rating `v_ui` được dự đoán bằng cách nào?</p><ul class="options"><li>A. Bằng tổng của vector user và vector item.</li><li>B. Bằng trung bình của vector user và vector item.</li><li class="correct-answer">C. Bằng tích vô hướng (dot product) của vector user và vector item (`w_u^T * h_i`).</li><li>D. Bằng khoảng cách Euclidean giữa vector user và vector item.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 29 và 32 đều trình bày công thức dự đoán rating là `v_ui = w_u^T * h_i`, đây chính là tích vô hướng của hai vector đặc trưng ẩn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 30 (Điền đáp án): Giả sử trong không gian ẩn 2 chiều, vector của user Alice là `w_A = [0.8, -0.2]` và vector của phim "Matrix" là `h_M = [0.9, 0.4]`. Rating dự đoán của Alice cho phim "Matrix" là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.64</span></p><p><b>💡 Giải thích:</b> `v_AM = w_A^T * h_M = (0.8 * 0.9) + (-0.2 * 0.4) = 0.72 - 0.08 = 0.64`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 31: Hàm mục tiêu `argmin Σ (v_ui - w_u^T * h_i)^2` (slide 30) nhằm mục đích gì?</p><ul class="options"><li>A. Tối đa hóa sai số dự đoán.</li><li class="correct-answer">B. Tìm các vector `w` và `h` sao cho tổng bình phương sai số dự đoán trên các rating đã biết là nhỏ nhất.</li><li>C. Tối đa hóa độ tương đồng giữa các user.</li><li>D. Tối thiểu hóa số chiều của không gian ẩn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là hàm mất mát (loss function) dựa trên sai số bình phương trung bình (Mean Squared Error - MSE). Mục tiêu của việc huấn luyện mô hình là tìm các tham số (`w`, `h`) để tối thiểu hóa hàm mất mát này trên tập dữ liệu huấn luyện (tập Z).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 32: Thuật ngữ "đại lượng hiệu chỉnh" (regularization term) `λ(Σ||w_i||^2 + Σ||h_u||^2)` được thêm vào hàm mục tiêu ở slide 31 để làm gì?</p><ul class="options"><li>A. Để tăng tốc độ hội tụ.</li><li>B. Để làm cho mô hình đơn giản hơn.</li><li class="correct-answer">C. Để tránh hiện tượng quá khớp (overfitting) bằng cách phạt các giá trị lớn trong vector đặc trưng.</li><li>D. Để đảm bảo các giá trị rating dự đoán luôn dương.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Overfitting xảy ra khi mô hình học quá tốt trên dữ liệu huấn luyện nhưng lại hoạt động kém trên dữ liệu mới. Đại lượng hiệu chỉnh này sẽ "phạt" các mô hình có các giá trị trong vector `w` và `h` quá lớn, buộc mô hình phải tổng quát hóa tốt hơn thay vì chỉ "học vẹt" dữ liệu huấn luyện.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 33: Thuật toán nào được sử dụng để học (tối ưu hóa hàm mục tiêu) trong Matrix Factorization theo slide 32?</p><ul class="options"><li>A. K-Nearest Neighbors</li><li class="correct-answer">B. Stochastic Gradient Descent (SGD)</li><li>C. Apriori</li><li>D. Decision Tree</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 32 có tiêu đề "Sử dụng SGD để học" và trình bày các bước cập nhật trọng số của thuật toán Stochastic Gradient Descent.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 34: Trong công thức cập nhật SGD `w_u ← w_u + γ(e_ui * h_i - λ * w_u)`, `γ` (gamma) là gì?</p><ul class="options"><li>A. Sai số dự đoán.</li><li>B. Tham số hiệu chỉnh.</li><li class="correct-answer">C. Tốc độ học (learning rate).</li><li>D. Vector đặc trưng của item.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> `γ` là tốc độ học, một siêu tham số (hyperparameter) kiểm soát mức độ cập nhật các vector trong mỗi bước lặp. Một `γ` quá lớn có thể khiến thuật toán không hội tụ, trong khi một `γ` quá nhỏ sẽ khiến việc học diễn ra rất chậm.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 35: Dựa trên biểu đồ ví dụ của Netflix Prize (slide 35), các bộ phim như "The Princess Diaries" và "Sister Act" có xu hướng tập hợp lại với nhau. Điều này cho thấy các vector ẩn đã học được đặc trưng gì?</p><ul class="options"><li>A. Phim hài nhảm (fraternity humor).</li><li class="correct-answer">B. Phim có nhân vật chính là nữ mạnh mẽ (strong female leads).</li><li>C. Phim độc lập, kỳ quặc (quirky independent films).</li><li>D. Phim hành động.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Chú thích dưới biểu đồ (Figure 3) trên slide 35 giải thích rằng "The plot reveals distinct genres, including clusters of movies with strong female leads...".</p></div></div>
        
        <!-- Generate more diverse questions -->
        <div class="question-block"><p class="question-text">Câu 36: CF dựa trên láng giềng gần của user và item thuộc loại tiếp cận nào?</p><ul class="options"><li class="correct-answer">A. Dựa trên memory</li><li>B. Dựa trên mô hình</li><li>C. Dựa trên tri thức</li><li>D. Dựa trên đồ thị</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 định nghĩa rõ: "Lọc cộng tác dựa trên láng giềng là tiếp cận dựa trên memory".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 37: Một nhược điểm của tiếp cận dựa trên memory là gì?</p><ul class="options"><li>A. Cần phải cập nhật mô hình thường xuyên.</li><li class="correct-answer">B. Khó làm việc với dữ liệu lớn và tốn kém tính toán tại thời điểm gợi ý.</li><li>C. Không giải quyết được cold start.</li><li>D. Độ chính xác thấp.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 chỉ ra rằng tiếp cận memory-based "Khó làm việc với dữ liệu lớn" vì nó phải duyệt qua một phần lớn dữ liệu để tìm láng giềng mỗi khi cần đưa ra gợi ý.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 38: Một ưu điểm của tiếp cận dựa trên mô hình là gì?</p><ul class="options"><li>A. Không cần bước training.</li><li class="correct-answer">B. Thời gian đưa ra gợi ý (dự đoán) nhanh.</li><li>C. Luôn dễ hiểu và giải thích được.</li><li>D. Không cần cập nhật.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Theo slide 8, mặc dù việc xây dựng mô hình có thể tốn kém, nhưng "Ở thời điểm gợi ý, mô hình sau khi huấn luyện được sử dụng để đưa ra dự đoán, gợi ý". Vì mô hình thường nhỏ gọn, quá trình này rất nhanh.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 39: Theo slide 14, Pearson correlation được cho là làm việc tốt hơn độ đo nào trong nhiều bài toán?</p><ul class="options"><li>A. Jaccard similarity</li><li>B. Euclidean distance</li><li class="correct-answer">C. Cosine similarity</li><li>D. Manhattan distance</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 14 ghi: "Làm việc tốt trong nhiều bài toán khi so với: Độ đo tương đồng cosine". Điều này là do Pearson có tính đến sự khác biệt trong thang điểm của người dùng (nó là Cosine trên dữ liệu đã được mean-centered).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 40: (Điền đáp án) Theo slide 20, về mặt lý thuyết, số cặp tương đồng item-item cần lưu trữ có thể lên tới N^k, với N là số lượng item. k bằng bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">2</span></p><p><b>💡 Giải thích:</b> Slide 20 nêu: "Về lý thuyết, có thể lên tới N² cặp tương đồng được lưu trữ".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 41: Theo slide 20, một cách để giảm kích thước bộ nhớ khi lưu trữ độ tương đồng item-item là gì?</p><ul class="options"><li>A. Giảm số lượng item.</li><li class="correct-answer">B. Sử dụng ngưỡng tối thiểu co-ratings hoặc giới hạn số lượng láng giềng.</li><li>C. Sử dụng user-based CF thay thế.</li><li>D. Nén dữ liệu.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 20, mục "Có thể giảm kích thước bộ nhớ", liệt kê hai phương pháp: "Sử dụng ngưỡng tối thiểu co-ratings" (chỉ tính độ tương đồng nếu có ít nhất X người dùng cùng đánh giá cả hai item) và "Giới hạn số lượng láng giềng" (chỉ lưu K láng giềng gần nhất cho mỗi item).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 42: Tại sao việc sử dụng "giả thiết tính bắc cầu của hàng xóm" lại hữu ích cho dữ liệu thưa?</p><ul class="options"><li>A. Vì nó làm giảm số lượng tính toán.</li><li class="correct-answer">B. Vì nó giúp tìm thấy các mối liên hệ gián tiếp giữa user và item ngay cả khi không có tương tác trực tiếp.</li><li>C. Vì nó đảm bảo độ chính xác tuyệt đối.</li><li>D. Vì nó đơn giản hóa mô hình.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 21 đề cập đến việc "Sử dụng giả thiết tính bắc cầu của hàng xóm" như một cách giải quyết vấn đề dữ liệu thưa. Slide 23-24 minh họa điều này với "Spreading Activation", cho thấy cách tìm ra gợi ý thông qua các đường đi dài hơn 2, kết nối user và item một cách gián tiếp.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 43: Trong Matrix Factorization, "không gian ẩn" (latent space) có số chiều `r` thường như thế nào so với số lượng user và item?</p><ul class="options"><li>A. Lớn hơn rất nhiều</li><li class="correct-answer">B. Nhỏ hơn rất nhiều</li><li>C. Bằng số lượng user</li><li>D. Bằng số lượng item</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Ý tưởng của MF là giảm chiều dữ liệu. Thay vì biểu diễn user bằng một vector có độ dài bằng số item, ta biểu diễn họ bằng một vector ẩn `w_u` có số chiều `r` nhỏ hơn nhiều (ví dụ r=20, 50, 100). Điều này giúp mô hình nhỏ gọn và có khả năng tổng quát hóa.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 44: Theo slide 28, ma trận gốc `V` có thể được xấp xỉ bằng tích của hai ma trận nào?</p><ul class="options"><li>A. V ≈ W * H^T</li><li>B. V ≈ W^T * H</li><li class="correct-answer">C. V ≈ W * H</li><li>D. V ≈ W + H</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức `v_ui = [WH]_ui` ở slide 28 và 33 cho thấy rằng toàn bộ ma trận V được xấp xỉ bởi tích của ma trận W (user-feature) và ma trận H (feature-item).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 45: (Điền đáp án) Trong SGD cho MF, nếu `v_ui = 4`, và dự đoán `w_u^T * h_i = 3.5`, thì sai số `e_ui` được tính ở bước đầu tiên là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.5</span></p><p><b>💡 Giải thích:</b> Theo slide 32, `e_ui ← v_ui - w_u^T * h_i`. Do đó, `e_ui = 4 - 3.5 = 0.5`.</p></div></div>
        
        <div class="question-block"><p class="question-text">Câu 46: Một giả thiết phụ của user-based CF là gì?</p><ul class="options"><li>A. Sở thích của user thay đổi nhanh chóng.</li><li class="correct-answer">B. Sở thích của user ổn định, không thay đổi theo thời gian.</li><li>C. Tất cả user đều có sở thích giống nhau.</li><li>D. User chỉ thích các sản phẩm phổ biến.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 9, mục "Giả thiết và ý tưởng", có nêu: "Sở thích của users ổn định, không thay đổi theo thời gian". Đây là một giả thiết mạnh và không phải lúc nào cũng đúng trong thực tế.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 47: Điều gì xảy ra với trọng số của độ tương đồng khi 2 user có ít item tương tác chung, theo slide 16?</p><ul class="options"><li class="correct-answer">A. Trọng số sẽ giảm.</li><li>B. Trọng số sẽ tăng.</li><li>C. Trọng số không đổi.</li><li>D. Trọng số bằng 0.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 16, mục "Số lượng của các items mà 2 users cùng tương tác", nói rằng: "Trọng số sẽ giảm khi 2 users có ít items tương tác chung". Điều này là hợp lý vì độ tương đồng tính trên ít điểm chung sẽ kém tin cậy hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 48: Phương pháp nào sau đây không phải là một cách để lựa chọn láng giềng được đề cập trong slide 16?</p><ul class="options"><li>A. Dựa trên ngưỡng tương đồng.</li><li>B. Dựa trên số cố định các láng giềng.</li><li class="correct-answer">C. Dựa trên thông tin nhân khẩu học của láng giềng.</li><li>D. Cả A và B đều được đề cập.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 16, mục "Lựa chọn láng giềng", chỉ đề cập "Dựa trên ngưỡng tương đồng hoặc số cố định các láng giềng". Thông tin nhân khẩu học là một loại dữ liệu khác, không phải là chiến lược chọn láng giềng trong ngữ cảnh này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 49: Trong hình minh họa trên slide 23, tại sao User2 được xem là láng giềng của User1?</p><ul class="options"><li>A. Vì họ có cùng tên.</li><li class="correct-answer">B. Vì họ cùng mua Item2 và Item4.</li><li>C. Vì User2 đã mua Item3.</li><li>D. Vì họ có độ tuổi tương tự.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 23 giải thích: "...User2 được xem là láng giềng của User1 bởi vì họ cùng mua Item2 và Item4". Điều này được thể hiện bằng các đường nối từ cả User1 và User2 đến Item2 và Item4.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 50: Theo Koren et al. (2009), Matrix Factorization có thể được xem như một mô hình hồi quy tuyến tính loại gì?</p><ul class="options"><li>A. Simple linear regression</li><li class="correct-answer">B. Doubly linear regression</li><li>C. Logistic regression</li><li>D. Polynomial regression</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 26 ghi: "Được xem như mô hình doubly linear regression".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 51: Theo slide 13, user nào có sở thích trái ngược nhất với Alice?</p><ul class="options"><li>A. User1 (sim = 0.85)</li><li>B. User2 (sim = 0.00)</li><li>C. User3 (sim = 0.70)</li><li class="correct-answer">D. User4 (sim = -0.79)</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Độ tương đồng Pearson âm lớn nhất (gần -1 nhất) chỉ ra mối tương quan nghịch mạnh nhất. Trong các giá trị đã cho, -0.79 là giá trị âm lớn nhất, cho thấy User4 có xu hướng thích những gì Alice không thích và ngược lại.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 52: Trong công thức Adjusted Cosine Similarity cho item-based CF, vector của mỗi item được tạo thành từ đâu?</p><ul class="options"><li class="correct-answer">A. Từ các rating mà tất cả user đã đưa ra cho item đó.</li><li>B. Từ các thuộc tính nội dung của item đó.</li><li>C. Từ các item khác tương tự nó.</li><li>D. Từ một vector ngẫu nhiên.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trong item-based CF, mỗi item được xem như một vector, và các chiều của vector này chính là các rating mà các user đã gán cho nó. Độ tương đồng được tính trên các vector này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 53: (Điền đáp án) Trong phương pháp Spreading Activation (slide 25), nếu gợi ý cho User1, Item1 có thể được gợi ý thông qua một đường đi có độ dài là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">5</span></p><p><b>💡 Giải thích:</b> Slide 25 chỉ ra rằng "Độ dài 5: Item1 cũng được gợi ý". Một ví dụ về đường đi này là User1 → Item2 → User2 → Item3 → User3 → Item1.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 54: Trong công thức SGD cho MF, `e_ui` đại diện cho điều gì?</p><ul class="options"><li>A. Vector ẩn của user</li><li class="correct-answer">B. Sai số dự đoán cho cặp (u,i)</li><li>C. Tham số hiệu chỉnh</li><li>D. Tốc độ học</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 32 định nghĩa `e_ui ← v_ui - w_u^T h_i`, đây chính là hiệu số giữa rating thực tế (`v_ui`) và rating dự đoán (`w_u^T h_i`), tức là sai số dự đoán.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 55: (Chọn nhiều đáp án) Theo slide 5, nội dung của chương này bao gồm những gì?</p><ul class="options"><li class="correct-answer">A. Lọc cộng tác dựa trên các mô hình học máy cơ bản.</li><li class="correct-answer">B. Lọc cộng tác dựa trên deep learning để học biểu diễn ẩn.</li><li>C. Lọc cộng tác dựa trên láng giềng gần.</li><li class="correct-answer">D. Dữ liệu implicit và explicit feedback.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 5 ("Nội dung") liệt kê các chủ đề chính của chương, bao gồm học máy cơ bản, deep learning và các loại dữ liệu. Nó không đề cập đến các phương pháp dựa trên láng giềng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 56: Biểu đồ trên slide 14 minh họa điều gì?</p><ul class="options"><li>A. Độ phổ biến của các item.</li><li class="correct-answer">B. Sự tương quan trong xu hướng rating của Alice, User1 và User4.</li><li>C. Rating trung bình của các user.</li><li>D. Số lượng rating cho mỗi item.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Biểu đồ vẽ các đường rating của 3 user qua 4 item. Ta có thể thấy đường của Alice và User1 có xu hướng cùng lên cùng xuống (tương quan dương), trong khi đường của Alice và User4 có xu hướng ngược nhau (tương quan âm). Điều này minh họa cho ý nghĩa của hệ số tương quan Pearson.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 57: Trong công thức dự đoán của item-based CF (slide 19), `r_u,i` là gì?</p><ul class="options"><li>A. Rating trung bình của user u.</li><li>B. Rating trung bình của item i.</li><li class="correct-answer">C. Rating mà user u đã đưa ra cho item i.</li><li>D. Độ tương đồng giữa hai item.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức `pred(u,p) = Σ sim(i,p) * r_u,i / ...` là một dạng trung bình có trọng số. Nó lấy rating mà user u đã cho các item láng giềng `i` (`r_u,i`), nhân với độ tương đồng của `i` với item mục tiêu `p`, rồi tính tổng.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 58: Tại sao trong thực tế, số lượng cặp tương đồng item-item cần lưu trữ lại nhỏ hơn đáng kể so với N²?</p><ul class="options"><li>A. Vì nhiều item không có ai rate.</li><li class="correct-answer">B. Vì độ tương đồng chỉ được tính giữa hai item nếu có ít nhất một người dùng đã rate cả hai, và điều này không phải lúc nào cũng xảy ra do dữ liệu thưa thớt.</li><li>C. Vì N là một số rất nhỏ.</li><li>D. Vì hệ thống chỉ lưu các giá trị tương đồng dương.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 20 giải thích: "Thực tế, số lượng lưu trữ nhỏ hơn đáng kể (items không có rating bởi cùng user)". Do ma trận user-item rất thưa, xác suất để hai item bất kỳ được cùng một user rate là thấp. Do đó, nhiều cặp item sẽ không có "co-rating" và độ tương đồng của chúng không cần tính toán hoặc lưu trữ.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 59: Giải pháp "assign default values to items that only one of the two users to be compared has rated" (Breese et al. 1998) nhằm mục đích gì?</p><ul class="options"><li>A. Tăng tốc độ tính toán.</li><li class="correct-answer">B. Xử lý vấn đề thiếu dữ liệu khi tính độ tương đồng giữa hai user, giúp cho việc so sánh trở nên khả thi hơn.</li><li>C. Giảm số lượng user.</li><li>D. Cải thiện độ chính xác của Matrix Factorization.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Trích dẫn này từ slide 21 là một cách để giải quyết vấn đề cold start/dữ liệu thưa. Khi tính tương đồng giữa user A và B, nếu A đã rate item X nhưng B chưa, việc gán một giá trị mặc định cho rating của B với X sẽ cho phép item X được đưa vào công thức tính tương đồng, thay vì bị bỏ qua.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 60: Theo slide 22, trong Recursive CF, khi rating của User1 cho Item5 chưa có, hệ thống sẽ làm gì?</p><ul class="options"><li>A. Bỏ qua User1.</li><li>B. Gán một giá trị trung bình.</li><li class="correct-answer">C. Dự đoán rating của User1 cho Item5 bằng cách sử dụng các láng giềng của User1 (ví dụ: User2, User3...).</li><li>D. Yêu cầu User1 nhập rating.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sơ đồ trên slide 22 cho thấy một mũi tên từ các user khác chỉ vào ô rating còn thiếu của User1, với chú thích "Dự đoán Rating cho User1". Đây chính là ý tưởng "đệ quy" của thuật toán.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 61: Trong Matrix Factorization, các trục của không gian ẩn (ví dụ: "Serious-Escapist") được xác định như thế nào?</p><ul class="options"><li>A. Do chuyên gia định nghĩa trước.</li><li class="correct-answer">B. Chúng được thuật toán tự động học từ dữ liệu rating, và việc diễn giải ý nghĩa của chúng là do con người thực hiện sau đó.</li><li>C. Dựa trên thể loại phim.</li><li>D. Dựa trên năm sản xuất.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Sức mạnh của MF nằm ở chỗ nó tự động khám phá ra các "đặc trưng ẩn" này. Các trục không được gán nhãn trước; thuật toán chỉ tìm ra các chiều giúp tái tạo lại ma trận rating tốt nhất. Việc gán ý nghĩa "nghiêm túc/giải trí" cho một trục là một nỗ lực diễn giải của con người sau khi mô hình đã được huấn luyện.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 62: (Điền đáp án) Theo slide 34, hàm mục tiêu của BPR sau khi lấy logarit và thêm thành phần hiệu chỉnh có dạng `Σ lnσ(r_ui - r_uj) - ...`. Dấu của thành phần hiệu chỉnh là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">- (âm)</span></p><p><b>💡 Giải thích:</b> Slide 34 cho thấy hàm mục tiêu sau khi biến đổi là `Σ lnσ(...) - λ_Θ||Θ||²`. Dấu trừ phía trước thành phần hiệu chỉnh là vì ta đang tối đa hóa hàm mục tiêu (MAP), tương đương với việc tối thiểu hóa `-ln(...) + λ_Θ||Θ||²` (negative log-likelihood + regularization).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 63: Tại sao Pearson Correlation lại nhạy cảm với số lượng item được đánh giá chung?</p><ul class="options"><li>A. Vì công thức có phép chia.</li><li class="correct-answer">B. Vì nếu chỉ có một hoặc hai item được đánh giá chung, hệ số tương quan tính ra có thể rất cao hoặc rất thấp một cách ngẫu nhiên và không đáng tin cậy.</li><li>C. Vì nó không xử lý được dữ liệu thiếu.</li><li>D. Vì nó chỉ hoạt động với dữ liệu dày đặc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Đây là lý do slide 16 đề xuất việc giảm trọng số khi có ít item tương tác chung. Một hệ số tương quan cao tính trên 2 điểm chung không có ý nghĩa bằng một hệ số tương quan tương tự tính trên 20 điểm chung.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 64: Tiếp cận dựa trên mô hình có ưu thế hơn tiếp cận dựa trên memory ở khía cạnh nào?</p><ul class="options"><li>A. Dễ triển khai hơn.</li><li class="correct-answer">B. Tốc độ dự đoán nhanh hơn và có khả năng xử lý dữ liệu thưa tốt hơn (ví dụ MF).</li><li>C. Không cần training.</li><li>D. Luôn cho kết quả chính xác hơn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Như slide 8 và các slide về MF cho thấy, sau khi mô hình được huấn luyện, việc dự đoán chỉ là một phép tính nhanh (ví dụ: tích vô hướng). Ngoài ra, các phương pháp như MF được thiết kế đặc biệt để xử lý sự thưa thớt của dữ liệu bằng cách học các biểu diễn ẩn dày đặc.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 65: (Điền đáp án) Trong SGD, nếu learning rate `γ` được đặt quá lớn, điều gì có thể xảy ra với hàm lỗi (loss)?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Không hội tụ / Phân kỳ / Dao động mạnh</span></p><p><b>💡 Giải thích:</b> Một learning rate quá lớn sẽ khiến các bước cập nhật trọng số "nhảy" qua lại điểm cực tiểu của hàm lỗi, làm cho hàm lỗi không giảm xuống mà có thể dao động hoặc thậm chí tăng lên (phân kỳ).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 66: "Feedback" trong Lọc cộng tác là gì?</p><ul class="options"><li>A. Lời phàn nàn của khách hàng.</li><li class="correct-answer">B. Dữ liệu về tương tác của người dùng với các item (ví dụ: ratings, clicks, purchases).</li><li>C. Bình luận của chuyên gia.</li><li>D. Mô tả của sản phẩm.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 6 đề cập "Users đưa ra ratings cho các items (implicit hoặc explicit feedback)". Feedback ở đây chính là các tín hiệu về sở thích của người dùng được thu thập từ hành vi của họ.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 67: Trong công thức Adjusted Cosine Similarity (slide 18), vector của item `a` và `b` được tạo từ tập user nào?</p><ul class="options"><li>A. Tất cả các user trong hệ thống.</li><li class="correct-answer">B. Chỉ những user `U` đã đánh giá cho cả item `a` và `b`.</li><li>C. Chỉ những user đã đánh giá item `a`.</li><li>D. Chỉ những user đã đánh giá item `b`.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 18 định nghĩa `U` là "tập users đã đánh giá cho cả a và b". Độ tương đồng chỉ được tính trên những người dùng đã cung cấp thông tin cho cả hai item, để có một cơ sở so sánh chung.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 68: Theo slide 19, hàm dự đoán cho item-based CF là một dạng của phép toán gì?</p><ul class="options"><li>A. Trung bình cộng đơn giản.</li><li class="correct-answer">B. Trung bình cộng có trọng số (weighted average).</li><li>C. Trung vị (median).</li><li>D. Tích vô hướng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức `pred(u,p) = Σ (sim * rating) / Σ sim` là công thức kinh điển của trung bình cộng có trọng số, trong đó trọng số chính là độ tương đồng `sim(i,p)`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 69: Vấn đề chính mà Lọc cộng tác dựa trên mô hình (như MF) cố gắng giải quyết là gì?</p><ul class="options"><li>A. Vấn đề cold-start cho user mới.</li><li class="correct-answer">B. Vấn đề dữ liệu thưa và khả năng mở rộng (scalability).</li><li>C. Vấn đề giải thích được kết quả.</li><li>D. Vấn đề về giao diện người dùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Mặc dù MF cũng gặp khó với cold-start, nhưng mục đích chính của việc học biểu diễn ẩn là để khắc phục sự thưa thớt của ma trận rating. Bằng cách nén thông tin vào không gian ẩn có số chiều thấp, mô hình có thể dự đoán được các rating còn thiếu và có khả năng mở rộng tốt hơn các phương pháp memory-based.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 70: Theo slide 35, các phim "Freddy Got Fingered", "Half Baked" được nhóm vào cụm nào?</p><ul class="options"><li>A. Strong female leads</li><li class="correct-answer">B. Fraternity humor</li><li>C. Quirky independent films</li><li>D. Mainstream action</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Chú thích của Figure 3 trên slide 35 đề cập đến "clusters of movies with strong female leads, fraternity humor, and quirky independent films." Nhìn vào vị trí của các phim này trên biểu đồ (góc trên bên trái), chúng thuộc vào cụm phim hài nhảm kiểu "fraternity humor".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 71: Đâu là một nhược điểm của tiếp cận dựa trên mô hình?</p><ul class="options"><li>A. Tốc độ dự đoán chậm.</li><li class="correct-answer">B. Quá trình xây dựng và huấn luyện mô hình có thể tốn nhiều thời gian và tài nguyên tính toán.</li><li>C. Không thể xử lý dữ liệu lớn.</li><li>D. Không thể cập nhật khi có dữ liệu mới.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 chỉ ra rằng "Xây dựng mô hình và huấn luyện có thể yêu cầu khối lượng tính toán lớn" và "Mô hình phải được cập nhật sau một giai đoạn thời gian".</p></div></div>
        <div class="question-text">Câu 72: (Điền đáp án) Trong công thức Pearson Correlation (slide 12), `P` là gì?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Tập các item được đánh giá bởi cả user a và b</span></p><p><b>💡 Giải thích:</b> Slide 12 định nghĩa: `P : set of items, rated both by a and b`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 73: Trong user-based CF, nếu không có user nào trong "láng giềng" đã rate item mục tiêu, hệ thống sẽ làm gì?</p><ul class="options"><li>A. Đưa ra rating là 5.</li><li class="correct-answer">B. Không thể đưa ra dự đoán bằng công thức láng giềng (cần một chiến lược dự phòng, ví dụ như trả về rating trung bình của user).</li><li>C. Đưa ra rating là 1.</li><li>D. Hỏi người dùng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức dự đoán ở slide 15 yêu cầu có các rating `r_b,p` từ các láng giềng `b`. Nếu không có láng giềng nào rate item `p`, tử số và mẫu số của phần phân số sẽ bằng 0, và không thể tính toán được. Hệ thống cần có một cơ chế dự phòng (fallback strategy).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 74: Tại sao "Spreading activation" được coi là một phương pháp dựa trên đồ thị?</p><ul class="options"><li>A. Vì nó vẽ ra một biểu đồ.</li><li class="correct-answer">B. Vì nó biểu diễn mối quan hệ user-item như một đồ thị hai phía (bipartite graph) và tìm kiếm các đường đi trên đó.</li><li>C. Vì nó sử dụng ma trận kề.</li><li>D. Vì kết quả có dạng đồ thị.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các slide 23-25 minh họa rõ ràng mối quan hệ user-item dưới dạng một đồ thị với hai tập đỉnh (user và item). Các "đường đi" được đề cập chính là các đường đi trên đồ thị này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 75: Tập `Z` trong hàm mục tiêu của MF (slide 30) là gì?</p><ul class="options"><li>A. Tập tất cả các user.</li><li>B. Tập tất cả các item.</li><li class="correct-answer">C. Tập các cặp (user, item) có tương tác (có rating đã biết).</li><li>D. Tập các cặp (user, item) chưa có tương tác.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 30 định nghĩa `Z = {(u,i) : v_ui ≠ 0}` và hàm mục tiêu tính tổng trên `(u,i) ∈ Z`. Điều này có nghĩa là mô hình chỉ học từ các rating đã tồn tại trong ma trận.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 76: (Điền đáp án) Theo slide 13, độ tương đồng giữa Alice và User1 là bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">0.85</span></p><p><b>💡 Giải thích:</b> Bảng ở slide 13 liệt kê giá trị `sim = 0,85` cho cặp Alice và User1.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 77: Tại sao phương pháp CF dựa trên láng giềng được gọi là "lazy learning" (học lười)?</p><ul class="options"><li>A. Vì nó chạy rất chậm.</li><li class="correct-answer">B. Vì nó trì hoãn việc tính toán (xây dựng mô hình) cho đến khi có một yêu cầu dự đoán, thay vì học một mô hình trước.</li><li>C. Vì nó không cần dữ liệu.</li><li>D. Vì nó không hiệu quả.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các thuật toán học lười (như k-NN, nền tảng của CF dựa trên láng giềng) chỉ lưu trữ dữ liệu trong "bước huấn luyện". Toàn bộ công việc tính toán chính (tìm láng giềng, dự đoán) chỉ diễn ra khi có một truy vấn mới. Ngược lại, các thuật toán "eager learning" (như MF) sẽ xây dựng một mô hình tổng quát trước.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 78: Một trong những thách thức của Item-based CF là gì?</p><ul class="options"><li>A. Khó tìm láng giềng cho user.</li><li class="correct-answer">B. Cần tính toán và lưu trữ một ma trận tương đồng item-item có thể rất lớn.</li><li>C. Tốc độ dự đoán chậm.</li><li>D. Không thể xử lý item mới.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 20 đề cập đến vấn đề "Yêu cầu bộ nhớ", có thể lên tới N² cặp tương đồng. Mặc dù có các cách giảm thiểu, đây vẫn là một thách thức về mặt tính toán và lưu trữ đối với các hệ thống rất lớn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 79: Mục đích chính của việc trừ đi rating trung bình trong công thức Pearson và Adjusted Cosine là gì?</p><ul class="options"><li>A. Để làm cho các giá trị nhỏ hơn.</li><li class="correct-answer">B. Để chuẩn hóa dữ liệu, loại bỏ ảnh hưởng của việc mỗi người dùng có một thang điểm (bias) khác nhau.</li><li>C. Để đảm bảo kết quả luôn dương.</li><li>D. Để tăng tốc độ tính toán.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Thao tác này (mean-centering) giúp so sánh "xu hướng" thay đổi của rating thay vì giá trị tuyệt đối. Nó xử lý trường hợp một người dùng "khó tính" cho điểm [2, 3] và một người dùng "dễ tính" cho điểm [4, 5]. Cả hai đều có cùng xu hướng (thích item thứ hai hơn item thứ nhất), và việc trừ đi trung bình sẽ làm nổi bật sự tương đồng này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 80: Trong SGD, tại sao lại gọi là "Stochastic" (ngẫu nhiên)?</p><ul class="options"><li>A. Vì kết quả là ngẫu nhiên.</li><li class="correct-answer">B. Vì ở mỗi bước, gradient được tính trên một mẫu (hoặc một mini-batch) được chọn ngẫu nhiên từ tập dữ liệu, thay vì trên toàn bộ tập dữ liệu.</li><li>C. Vì tốc độ học được chọn ngẫu nhiên.</li><li>D. Vì các vector được khởi tạo ngẫu nhiên.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Thuật toán Gradient Descent truyền thống tính toán gradient trên toàn bộ tập dữ liệu ở mỗi bước, rất tốn kém. "Stochastic" Gradient Descent ước tính gradient chỉ bằng một điểm dữ liệu (hoặc một batch nhỏ) được chọn ngẫu nhiên, giúp quá trình học nhanh hơn và có thể thoát khỏi các điểm cực tiểu cục bộ. Slide 34 cũng mô tả bước "Select a training point (i, j) ∈ Z uniformly at random".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 81: (Chọn nhiều đáp án) Các tiếp cận Lọc cộng tác được đề cập trong slide 5 bao gồm những gì?</p><ul class="options"><li class="correct-answer">A. Lọc cộng tác dựa trên láng giềng gần của user.</li><li class="correct-answer">B. Lọc cộng tác dựa trên láng giềng gần của item.</li><li class="correct-answer">C. Học biểu diễn ẩn của user và item dựa trên phân tích ma trận.</li><li>D. Lọc cộng tác dựa trên nội dung.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 5 ("Nội dung") liệt kê rõ các mục này trong chương Lọc cộng tác. Lọc cộng tác dựa trên nội dung là một phương pháp khác, không phải là một phần của chương này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 82: Trong Matrix Factorization, nếu một phim có vector ẩn nằm gần gốc tọa độ (0,0) trong không gian 2 chiều, điều này có thể có ý nghĩa gì?</p><ul class="options"><li>A. Phim đó rất hay.</li><li class="correct-answer">B. Phim đó có tính chất trung bình, không nổi bật theo các chiều đặc trưng đã học (ví dụ: không quá nghiêm túc cũng không quá hài hước).</li><li>C. Phim đó rất dở.</li><li>D. Phim đó không có ai xem.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Các vector ẩn biểu diễn phim trên các chiều đặc trưng. Một vector gần gốc tọa độ có nghĩa là nó có giá trị thấp trên tất cả các chiều đó, cho thấy nó là một phim "trung tính" hoặc "phổ thông", không có đặc điểm rõ rệt theo các khía cạnh mà mô hình đã học được.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 83: Hàm dự đoán trong Item-based CF (slide 19) KHÔNG phụ thuộc vào yếu tố nào sau đây?</p><ul class="options"><li>A. Rating của user `u` cho các item khác.</li><li>B. Độ tương đồng giữa các item.</li><li class="correct-answer">C. Rating của các user khác cho item `p`.</li><li>D. Các item mà user `u` đã rate.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức dự đoán `pred(u,p)` chỉ sử dụng các rating của chính user `u` (`r_u,i`) và độ tương đồng giữa item mục tiêu `p` với các item `i` đó. Nó không trực tiếp sử dụng rating của các user khác cho item `p`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 84: (Điền đáp án) Trong ma trận ở slide 10, nếu ta coi rating > 3 là tương tác dương (1) và <= 3 là tương tác âm (0), hãy biểu diễn vector tương tác của User3.</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">[0, 0, 0, 1, 1]</span></p><p><b>💡 Giải thích:</b> Rating của User3 là [3, 3, 1, 5, 4]. Áp dụng quy tắc: 3 -> 0, 3 -> 0, 1 -> 0, 5 -> 1, 4 -> 1. Vậy vector nhị phân là [0, 0, 0, 1, 1].</p></div></div>
        <div class="question-block"><p class="question-text">Câu 85: Tại sao việc "đặt trọng số cao hơn trên các items mà có đánh giá là higher variance" lại có thể cải thiện hàm dự đoán (slide 16)?</p><ul class="options"><li>A. Vì các item này phổ biến hơn.</li><li class="correct-answer">B. Vì sự đồng thuận trên các item gây tranh cãi (có cả người thích và ghét) là một tín hiệu mạnh hơn về sự tương đồng sở thích so với sự đồng thuận trên các item mà ai cũng thích.</li><li>C. Vì các item này dễ tính toán hơn.</li><li>D. Vì các item này có rating cao hơn.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Variance cao trong rating của một item có nghĩa là có sự bất đồng ý kiến lớn về nó. Nếu hai người cùng thích hoặc cùng ghét một item gây tranh cãi như vậy, điều đó cho thấy gu của họ thực sự tương đồng hơn là việc họ cùng thích một item "an toàn" mà ai cũng thích.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 86: Nếu hai user có độ tương đồng Pearson là -1, điều này có nghĩa là gì?</p><ul class="options"><li>A. Họ không có điểm chung.</li><li>B. Họ có sở thích giống hệt nhau.</li><li class="correct-answer">C. Họ có sở thích hoàn toàn đối lập nhau một cách tuyến tính.</li><li>D. Ít nhất một người chưa rate item nào.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Giá trị -1 đại diện cho một mối tương quan nghịch tuyến tính hoàn hảo. Điều này có nghĩa là khi một người đánh giá một item cao hơn mức trung bình của họ, người kia sẽ có xu hướng đánh giá item đó thấp hơn mức trung bình của họ một cách tương ứng, và ngược lại.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 87: (Điền đáp án) Ma trận `W` trong công thức `V ≈ WH` (slide 28) biểu diễn điều gì và có kích thước bao nhiêu (nếu có m user và r chiều ẩn)?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">Ma trận đặc trưng ẩn của user, kích thước m x r</span></p><p><b>💡 Giải thích:</b> `W` là ma trận user-feature, mỗi hàng `w_u*` là một vector đặc trưng ẩn cho user `u`. Với `m` user và `r` chiều ẩn, ma trận này sẽ có kích thước `m x r`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 88: Ma trận `H` trong công thức `V ≈ WH` (slide 28) biểu diễn điều gì và có kích thước bao nhiêu (nếu có n item và r chiều ẩn)?</p><ul class="options"><li>A. Ma trận đặc trưng ẩn của user, kích thước m x r</li><li class="correct-answer">B. Ma trận đặc trưng ẩn của item, kích thước r x n</li><li>C. Ma trận rating gốc, kích thước m x n</li><li>D. Ma trận tương đồng, kích thước n x n</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> `H` là ma trận feature-item, mỗi cột `h_*j` là một vector đặc trưng ẩn cho item `j`. Để phép nhân `W * H` (kích thước `m x r` * `r x n`) cho ra ma trận `V` (kích thước `m x n`), `H` phải có kích thước `r x n`.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 89: Thuật toán SGD cho MF cập nhật các vector `w_u` và `h_i` dựa trên điều gì?</p><ul class="options"><li>A. Dựa trên toàn bộ ma trận rating.</li><li class="correct-answer">B. Dựa trên sai số dự đoán của một cặp (user, item) được chọn ngẫu nhiên.</li><li>C. Dựa trên độ phổ biến của item.</li><li>D. Dựa trên số lượng láng giềng.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Công thức cập nhật ở slide 32 cho thấy cả `w_u` và `h_i` đều được cập nhật dựa trên `e_ui`, là sai số của một cặp `(u,i)` cụ thể. Đây là bản chất của "Stochastic" Gradient Descent.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 90: Điều gì xảy ra với công thức dự đoán của item-based CF nếu tất cả các item láng giềng đều có độ tương đồng bằng 0 với item mục tiêu?</p><ul class="options"><li>A. Dự đoán sẽ là 5.</li><li class="correct-answer">B. Công thức sẽ không xác định (chia cho 0), cần một chiến lược dự phòng.</li><li>C. Dự đoán sẽ là 0.</li><li>D. Dự đoán sẽ là rating trung bình của user.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Theo công thức ở slide 19, nếu tất cả `sim(i,p)` đều bằng 0, cả tử số và mẫu số đều sẽ bằng 0, dẫn đến phép chia không xác định. Hệ thống cần một cơ chế fallback, ví dụ như trả về rating trung bình của user hoặc item.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 91: "Implicit feedback" và "explicit feedback" là các khái niệm được đề cập trong nội dung nào của chương (slide 5)?</p><ul class="options"><li>A. Lọc cộng tác dựa trên các mô hình học máy cơ bản.</li><li>B. Lọc cộng tác dựa trên deep learning.</li><li class="correct-answer">C. Dữ liệu implicit và explicit feedback.</li><li>D. Vấn đề với dữ liệu thưa.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 5 có một mục riêng là "Dữ liệu implicit và explicit feedback", cho thấy đây là một chủ đề con quan trọng trong chương.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 92: Trong user-based CF, việc chọn số lượng láng giềng (k) là một ví dụ của việc gì?</p><ul class="options"><li>A. Huấn luyện mô hình.</li><li class="correct-answer">B. Lựa chọn siêu tham số (hyperparameter tuning).</li><li>C. Trích xuất đặc trưng.</li><li>D. Giảm chiều dữ liệu.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Siêu tham số là một tham số mà giá trị của nó được đặt trước khi quá trình học bắt đầu (ví dụ: `k` trong k-NN, tốc độ học `γ` trong SGD). Việc tìm giá trị tốt nhất cho `k` thường được thực hiện thông qua thực nghiệm hoặc các kỹ thuật như cross-validation.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 93: Tại sao "Spreading activation" có thể giúp giảm bớt vấn đề dữ liệu thưa?</p><ul class="options"><li>A. Vì nó xóa dữ liệu thưa.</li><li class="correct-answer">B. Vì nó cho phép tìm thấy mối liên hệ giữa một user và một item ngay cả khi họ không có láng giềng chung trực tiếp, bằng cách xem xét các đường đi dài hơn.</li><li>C. Vì nó làm cho ma trận dày đặc hơn.</li><li>D. Vì nó chỉ hoạt động trên dữ liệu dày đặc.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Vấn đề cốt lõi của dữ liệu thưa trong CF láng giềng là không tìm thấy đủ láng giềng chung. Bằng cách xét các đường đi dài hơn (ví dụ: User A -> Item X -> User B -> Item Y), "Spreading activation" có thể tìm thấy các mối liên hệ gián tiếp và đưa ra gợi ý mà các phương pháp láng giềng gần truyền thống sẽ bỏ lỡ.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 94: (Điền đáp án) Trong Matrix Factorization, nếu ma trận user-item `V` có kích thước 1000x5000 và không gian ẩn có 50 chiều, ma trận `W` sẽ có kích thước bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">1000 x 50</span></p><p><b>💡 Giải thích:</b> Ma trận `W` có số hàng bằng số user (1000) và số cột bằng số chiều ẩn (50).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 95: (Điền đáp án) Với cùng câu hỏi 94, ma trận `H` sẽ có kích thước bao nhiêu?</p><div class="explanation"><p><b>✅ Đáp án:</b> <span class="fill-in-answer">50 x 5000</span></p><p><b>💡 Giải thích:</b> Ma trận `H` có số hàng bằng số chiều ẩn (50) và số cột bằng số item (5000).</p></div></div>
        <div class="question-block"><p class="question-text">Câu 96: Phương pháp nào sau đây thuộc loại tiếp cận memory-based?</p><ul class="options"><li>A. Matrix Factorization</li><li class="correct-answer">B. User-based Collaborative Filtering</li><li>C. Spreading Activation</li><li>D. Recursive CF</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 8 phân loại "Lọc cộng tác dựa trên láng giềng" (bao gồm cả user-based và item-based) là tiếp cận memory-based. Các phương pháp còn lại đều học một mô hình hoặc sử dụng một quy trình suy luận phức tạp hơn.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 97: Phương pháp nào sau đây thuộc loại tiếp cận model-based?</p><ul class="options"><li>A. User-based Collaborative Filtering</li><li>B. Item-based Collaborative Filtering</li><li class="correct-answer">C. Matrix Factorization</li><li>D. Cả A và B</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Slide 26 giới thiệu Matrix Factorization là một "tiếp cận dựa trên mô hình". Nó học một mô hình (hai ma trận `W` và `H`) từ dữ liệu.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 98: Trong biểu đồ của MF (slide 35), hai phim "Lost in Translation" và "Being John Malkovich" nằm gần nhau cho thấy điều gì?</p><ul class="options"><li>A. Chúng được sản xuất cùng năm.</li><li>B. Chúng có cùng diễn viên chính.</li><li class="correct-answer">C. Chúng có các đặc trưng ẩn tương tự nhau theo mô hình đã học (ví dụ: cùng là phim độc lập, kỳ quặc).</li><li>D. Chúng có doanh thu tương đương.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Vị trí trên biểu đồ MF biểu diễn vector đặc trưng ẩn của phim. Hai điểm gần nhau có nghĩa là hai vector của chúng gần nhau trong không gian ẩn, cho thấy chúng có các đặc tính ẩn tương tự nhau. Chú thích của biểu đồ cũng xếp chúng vào nhóm "quirky independent films".</p></div></div>
        <div class="question-block"><p class="question-text">Câu 99: Tại sao công thức dự đoán của user-based CF lại trừ đi rating trung bình của láng giềng (`r_b,p - r_b`)?</p><ul class="options"><li>A. Để làm cho số nhỏ hơn.</li><li class="correct-answer">B. Để chỉ lấy phần "đóng góp thông tin" của láng giềng, tức là mức độ mà họ thích/ghét item `p` so với mức độ trung bình của chính họ.</li><li>C. Để đảm bảo kết quả không âm.</li><li>D. Để giảm độ phức tạp.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Thao tác này giúp loại bỏ bias của láng giềng. Nếu một láng giềng "dễ tính" (r_b = 4.5) rate một phim là 4, đó thực ra là một tín hiệu tiêu cực (4 < 4.5). Nếu một láng giềng "khó tính" (r_b = 2.5) rate phim đó là 4, đó là một tín hiệu tích cực rất mạnh (4 > 2.5). Việc trừ đi trung bình sẽ nắm bắt được sự khác biệt này.</p></div></div>
        <div class="question-block"><p class="question-text">Câu 100: Mục tiêu cuối cùng của việc tối ưu hóa hàm mục tiêu trong Matrix Factorization là gì?</p><ul class="options"><li>A. Tìm ra ma trận `V`.</li><li class="correct-answer">B. Tìm ra các ma trận đặc trưng ẩn `W` và `H` sao cho tích của chúng xấp xỉ tốt nhất ma trận rating `V` đã biết.</li><li>C. Tìm ra số chiều ẩn `r` tối ưu.</li><li>D. Tìm ra tốc độ học `γ` tối ưu.</li></ul><div class="explanation"><p><b>💡 Giải thích:</b> Toàn bộ quá trình tối ưu hóa (ví dụ: bằng SGD) là để tìm các giá trị cho các phần tử trong ma trận `W` và `H`. Đây là các tham số của mô hình. `r` và `γ` là các siêu tham số được chọn trước. `V` là dữ liệu đầu vào. Mục tiêu là tìm `W` và `H` để `W*H` tái tạo lại `V` một cách chính xác nhất có thể.</p></div></div>

    </div>

</body>
</html>